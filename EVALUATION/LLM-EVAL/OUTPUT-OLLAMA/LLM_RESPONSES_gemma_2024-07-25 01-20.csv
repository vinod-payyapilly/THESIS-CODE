,MODEL_NAME,filename,original text,expected_json,llm_full_response,llm_json_extracted,time_taken_seconds
0,gemma:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database""],
    [""Cognitive Search"", ""system"", ""Enterprise search service""],
    [""Computer Vision Read API"", ""component"", ""Optical Character Recognition API""],
    [""Document Classifier Service"", ""component"", ""Document classification service""],
    [""Indexing activity function"", ""component"", ""Function that creates search documents""],
    [""Metadata Store activity function"", ""component"", ""Function that saves document metadata""],
    [""Durable Functions"", ""system"", ""Orchestration framework""],
    [""Web app"", ""person"", ""Application that uploads documents""],
    [""User"", ""person"", ""End user who searches for documents""]
  ],
  ""relationships"": [
    [""Web app"", ""calls"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Computer Vision Read API""],
    [""Computer Vision Read API"", ""returns"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Classify activity function""],
    [""Classify activity function"", ""calls"", ""Document Classifier Service""],
    [""Document Classifier Service"", ""returns"", ""Classify activity function""],
    [""Durable Functions"", ""calls"", ""Metadata Store activity function""],
    [""Durable Functions"", ""calls"", ""Indexing activity function""],
    [""Indexing activity function"", ""returns"", ""Cognitive Search""],
    [""Metadata Store activity function"", ""stores"", ""Azure Cosmos DB""],
    [""Durable Functions"", ""part-of"", ""Web app""],
    [""User"", ""uses"", ""Web app""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database""],
    [""Cognitive Search"", ""system"", ""Enterprise search service""],
    [""Computer Vision Read API"", ""component"", ""Optical Character Recognition API""],
    [""Document Classifier Service"", ""component"", ""Document classification service""],
    [""Indexing activity function"", ""component"", ""Function that creates search documents""],
    [""Metadata Store activity function"", ""component"", ""Function that saves document metadata""],
    [""Durable Functions"", ""system"", ""Orchestration framework""],
    [""Web app"", ""person"", ""Application that uploads documents""],
    [""User"", ""person"", ""End user who searches for documents""]
  ],
  ""relationships"": [
    [""Web app"", ""calls"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Computer Vision Read API""],
    [""Computer Vision Read API"", ""returns"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Classify activity function""],
    [""Classify activity function"", ""calls"", ""Document Classifier Service""],
    [""Document Classifier Service"", ""returns"", ""Classify activity function""],
    [""Durable Functions"", ""calls"", ""Metadata Store activity function""],
    [""Durable Functions"", ""calls"", ""Indexing activity function""],
    [""Indexing activity function"", ""returns"", ""Cognitive Search""],
    [""Metadata Store activity function"", ""stores"", ""Azure Cosmos DB""],
    [""Durable Functions"", ""part-of"", ""Web app""],
    [""User"", ""uses"", ""Web app""]
  ]
}
```",4.904445066000335
1,gemma:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage for storing audio files and transcription results""],
    [""Azure AI Speech"", ""system"", ""Speech recognition service for transcribing audio files""],
    [""Azure AI Language"", ""system"", ""Data analysis service for detecting and redacting personal data""],
    [""Ingestion Client tool"", ""component"", ""Tool for processing transcription data""],
    [""Azure OpenAI"", ""system"", ""Language processing service for extracting entities, summarizing conversations and analyzing sentiments""],
    [""Power BI"", ""component"", ""Business intelligence tool for visualizing data""],
    [""App Service"", ""component"", ""Web hosting service""],
    [""CRM"", ""component"", ""Customer relationship management system""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure Blob Storage"", ""contains"", ""Transcription results""],
    [""Azure AI Speech"", ""calls"", ""Ingestion Client tool""],
    [""Ingestion Client tool"", ""calls"", ""Azure AI Language""],
    [""Azure AI Language"", ""part-of"", ""Azure OpenAI""],
    [""Azure OpenAI"", ""calls"", ""Transcription information""],
    [""Azure OpenAI"", ""contains"", ""Entities""],
    [""Azure OpenAI"", ""contains"", ""Conversation summary""],
    [""Azure OpenAI"", ""contains"", ""Sentiment analysis""],
    [""Azure OpenAI"", ""stores in"", ""Blob Storage""],
    [""Blob Storage"", ""called-from"", ""Power BI""],
    [""Blob Storage"", ""called-from"", ""Web application""],
    [""Power BI"", ""part-of"", ""CRM""],
    [""App Service"", ""part-of"", ""Web application""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage for storing audio files and transcription results""],
    [""Azure AI Speech"", ""system"", ""Speech recognition service for transcribing audio files""],
    [""Azure AI Language"", ""system"", ""Data analysis service for detecting and redacting personal data""],
    [""Ingestion Client tool"", ""component"", ""Tool for processing transcription data""],
    [""Azure OpenAI"", ""system"", ""Language processing service for extracting entities, summarizing conversations and analyzing sentiments""],
    [""Power BI"", ""component"", ""Business intelligence tool for visualizing data""],
    [""App Service"", ""component"", ""Web hosting service""],
    [""CRM"", ""component"", ""Customer relationship management system""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure Blob Storage"", ""contains"", ""Transcription results""],
    [""Azure AI Speech"", ""calls"", ""Ingestion Client tool""],
    [""Ingestion Client tool"", ""calls"", ""Azure AI Language""],
    [""Azure AI Language"", ""part-of"", ""Azure OpenAI""],
    [""Azure OpenAI"", ""calls"", ""Transcription information""],
    [""Azure OpenAI"", ""contains"", ""Entities""],
    [""Azure OpenAI"", ""contains"", ""Conversation summary""],
    [""Azure OpenAI"", ""contains"", ""Sentiment analysis""],
    [""Azure OpenAI"", ""stores in"", ""Blob Storage""],
    [""Blob Storage"", ""called-from"", ""Power BI""],
    [""Blob Storage"", ""called-from"", ""Web application""],
    [""Power BI"", ""part-of"", ""CRM""],
    [""App Service"", ""part-of"", ""Web application""]
  ]
}
```",4.996471612015739
2,gemma:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform that uses connectors to integrate with various services""],
    [""Azure API Management"", ""system"", ""API gateway and developer portal""],
    [""Azure DNS"", ""system"", ""Name resolution service""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service""],
    [""API gateway"", ""component"", ""Accepts HTTP calls and routes them to the backend""],
    [""Developer portal"", ""component"", ""Provides access to documentation and code samples""],
    [""Custom domain name"", ""string"", "" contoso.com""],
    [""Access token"", ""string"", ""Issued by Microsoft Entra ID""],
    [""Standard or Premium tier"", ""string"", ""Tier of API Management""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""calls"", ""API gateway""],
    [""API gateway"", ""part-of"", ""Azure API Management""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure API Management"", ""calls"", ""Microsoft Entra ID""],
    [""Microsoft Entra ID"", ""part-of"", ""Azure API Management""],
    [""Azure DNS"", ""contains"", ""Custom domain name""],
    [""Azure DNS"", ""contains"", ""Access token""],
    [""Azure API Management"", ""contains"", ""Standard or Premium tier""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform that uses connectors to integrate with various services""],
    [""Azure API Management"", ""system"", ""API gateway and developer portal""],
    [""Azure DNS"", ""system"", ""Name resolution service""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service""],
    [""API gateway"", ""component"", ""Accepts HTTP calls and routes them to the backend""],
    [""Developer portal"", ""component"", ""Provides access to documentation and code samples""],
    [""Custom domain name"", ""string"", "" contoso.com""],
    [""Access token"", ""string"", ""Issued by Microsoft Entra ID""],
    [""Standard or Premium tier"", ""string"", ""Tier of API Management""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""calls"", ""API gateway""],
    [""API gateway"", ""part-of"", ""Azure API Management""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure API Management"", ""calls"", ""Microsoft Entra ID""],
    [""Microsoft Entra ID"", ""part-of"", ""Azure API Management""],
    [""Azure DNS"", ""contains"", ""Custom domain name""],
    [""Azure DNS"", ""contains"", ""Access token""],
    [""Azure API Management"", ""contains"", ""Standard or Premium tier""]
  ]
}
```",4.2762946219882
3,gemma:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system for suppliers""],
    [""Data Factory"", ""component"", ""Data extraction pipeline""],
    [""ERP system"", ""system"", ""Purchase order management system""],
    [""Azure SQL Database"", ""system"", ""Data storage for purchase orders""],
    [""Azure Functions"", ""component"", ""Data extraction API""],
    [""Power Apps"", ""component"", ""Data review and update application""],
    [""Suppliers"", ""system"", ""External data source""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""ERP system"", ""part-of"", ""Azure SQL Database""],
    [""Azure Functions"", ""calls"", ""API""],
    [""Azure SQL Database"", ""part-of"", ""Power Apps""],
    [""Power Apps"", ""calls"", ""Suppliers""],
    [""Suppliers"", ""part-of"", ""Power BI""]
  ]
}
```","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system for suppliers""],
    [""Data Factory"", ""component"", ""Data extraction pipeline""],
    [""ERP system"", ""system"", ""Purchase order management system""],
    [""Azure SQL Database"", ""system"", ""Data storage for purchase orders""],
    [""Azure Functions"", ""component"", ""Data extraction API""],
    [""Power Apps"", ""component"", ""Data review and update application""],
    [""Suppliers"", ""system"", ""External data source""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""ERP system"", ""part-of"", ""Azure SQL Database""],
    [""Azure Functions"", ""calls"", ""API""],
    [""Azure SQL Database"", ""part-of"", ""Power Apps""],
    [""Power Apps"", ""calls"", ""Suppliers""],
    [""Suppliers"", ""part-of"", ""Power BI""]
  ]
}
```",2.971023183024954
4,gemma:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}","```json
{
  ""entities"": [
    [""Outlook"", ""system"", ""Email receiving system""],
    [""Logic App"", ""component"", ""Processing logic app""],
    [""Azure Data Lake Storage"", ""system"", ""Storage container""],
    [""Function App"", ""component"", ""Processing function app""],
    [""Azure AI Document Intelligence"", ""system"", ""AI service""],
    [""Cosmos DB"", ""system"", ""Database""],
    [""Power BI"", ""person"", ""Data analytics tool""]
  ],
  ""relationships"": [
    [""Outlook"", ""calls"", ""Logic App""],
    [""Logic App"", ""contains"", ""PDF container""],
    [""PDF container"", ""calls"", ""Logic App""],
    [""Logic App"", ""calls"", ""Function App""],
    [""Function App"", ""calls"", ""Azure AI Document Intelligence""],
    [""Azure AI Document Intelligence"", ""called-from"", ""Function App""],
    [""Function App"", ""contains"", ""Processed PDF data""],
    [""Function App"", ""calls"", ""Cosmos DB""],
    [""Cosmos DB"", ""called-from"", ""Function App""],
    [""Cosmos DB"", ""contains"", ""Processed data""],
    [""Power BI"", ""calls"", ""Cosmos DB""]
  ]
}
```","```json
{
  ""entities"": [
    [""Outlook"", ""system"", ""Email receiving system""],
    [""Logic App"", ""component"", ""Processing logic app""],
    [""Azure Data Lake Storage"", ""system"", ""Storage container""],
    [""Function App"", ""component"", ""Processing function app""],
    [""Azure AI Document Intelligence"", ""system"", ""AI service""],
    [""Cosmos DB"", ""system"", ""Database""],
    [""Power BI"", ""person"", ""Data analytics tool""]
  ],
  ""relationships"": [
    [""Outlook"", ""calls"", ""Logic App""],
    [""Logic App"", ""contains"", ""PDF container""],
    [""PDF container"", ""calls"", ""Logic App""],
    [""Logic App"", ""calls"", ""Function App""],
    [""Function App"", ""calls"", ""Azure AI Document Intelligence""],
    [""Azure AI Document Intelligence"", ""called-from"", ""Function App""],
    [""Function App"", ""contains"", ""Processed PDF data""],
    [""Function App"", ""calls"", ""Cosmos DB""],
    [""Cosmos DB"", ""called-from"", ""Function App""],
    [""Cosmos DB"", ""contains"", ""Processed data""],
    [""Power BI"", ""calls"", ""Cosmos DB""]
  ]
}
```",3.853694281016942
5,gemma:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity Provider""],
    [""Azure Front Door"", ""component"", ""Gateway""],
    [""API Management"", ""system"", ""API Gateway""],
    [""App Service Environment"", ""component"", ""Environment""],
    [""Function Apps"", ""component"", ""Environment""],
    [""AKS"", ""component"", ""Environment""],
    [""User"", ""person"", ""Client""],
    [""JWT"", ""component"", ""Authentication Token""]
  ],
  ""relationships"": [
    [""Azure Front Door"", ""contains"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""calls"", ""User""],
    [""User"", ""calls"", ""API Management""],
    [""API Management"", ""contains"", ""JWT""],
    [""API Management"", ""contains"", ""Function Apps""],
    [""API Management"", ""contains"", ""AKS""],
    [""API Management"", ""part-of"", ""Azure Front Door""],
    [""API Management"", ""calls"", ""Backend API""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity Provider""],
    [""Azure Front Door"", ""component"", ""Gateway""],
    [""API Management"", ""system"", ""API Gateway""],
    [""App Service Environment"", ""component"", ""Environment""],
    [""Function Apps"", ""component"", ""Environment""],
    [""AKS"", ""component"", ""Environment""],
    [""User"", ""person"", ""Client""],
    [""JWT"", ""component"", ""Authentication Token""]
  ],
  ""relationships"": [
    [""Azure Front Door"", ""contains"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""calls"", ""User""],
    [""User"", ""calls"", ""API Management""],
    [""API Management"", ""contains"", ""JWT""],
    [""API Management"", ""contains"", ""Function Apps""],
    [""API Management"", ""contains"", ""AKS""],
    [""API Management"", ""part-of"", ""Azure Front Door""],
    [""API Management"", ""calls"", ""Backend API""]
  ]
}
```",3.3590728620183654
6,gemma:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Mainframe"", ""system""],
    [""Midrange systems"", ""system""],
    [""Publisher"", ""component""],
    [""Listener"", ""component""],
    [""Enhanced-security connection"", ""component""],
    [""Replicator Engine"", ""component""],
    [""Target"", ""system""],
    [""Azure Databricks"", ""system""],
    [""Azure data platform services"", ""system""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""contains"", ""Midrange systems"", ""Listener""],
    [""part-of"", ""Mainframe"", ""Publisher""],
    [""part-of"", ""Midrange systems"", ""Listener""],
    [""calls"", ""Publisher/Listener"", ""Replicator Engine""],
    [""called-from"", ""Replicator Engine"", ""Publisher/Listener""],
    [""contains"", ""Target"", ""Azure Data Hubs""],
    [""contains"", ""Target"", ""ODBC""],
    [""calls"", ""Azure Data Hubs"", ""Azure Databricks""],
    [""called-from"", ""Azure Databricks"", ""Azure Data Hubs""]
  ]
}
```","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Mainframe"", ""system""],
    [""Midrange systems"", ""system""],
    [""Publisher"", ""component""],
    [""Listener"", ""component""],
    [""Enhanced-security connection"", ""component""],
    [""Replicator Engine"", ""component""],
    [""Target"", ""system""],
    [""Azure Databricks"", ""system""],
    [""Azure data platform services"", ""system""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""contains"", ""Midrange systems"", ""Listener""],
    [""part-of"", ""Mainframe"", ""Publisher""],
    [""part-of"", ""Midrange systems"", ""Listener""],
    [""calls"", ""Publisher/Listener"", ""Replicator Engine""],
    [""called-from"", ""Replicator Engine"", ""Publisher/Listener""],
    [""contains"", ""Target"", ""Azure Data Hubs""],
    [""contains"", ""Target"", ""ODBC""],
    [""calls"", ""Azure Data Hubs"", ""Azure Databricks""],
    [""called-from"", ""Azure Databricks"", ""Azure Data Hubs""]
  ]
}
```",3.755214678007178
7,gemma:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API Management securely routes HTTP requests to Logic Apps.""],
    [""Logic Apps"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""Azure Key Vault"", ""system"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""Data Gateway"", ""component"", ""Logic Apps uses TLS to send the database credentials and a database statement to the on-premises data gateway.""],
    [""SQL Server"", ""system"", ""SQL Server stores the data and makes it available to apps that users access""]
  ],
  ""relationships"": [
    [""API Management"", ""contains"", ""Logic Apps""],
    [""Logic Apps"", ""part-of"", ""API Management""],
    [""Logic Apps"", ""calls"", ""Azure Key Vault""],
    [""Logic Apps"", ""part-of"", ""Data Gateway""],
    [""Data Gateway"", ""part-of"", ""Logic Apps""],
    [""Logic Apps"", ""calls"", ""SQL Server""],
    [""SQL Server"", ""part-of"", ""Logic Apps""]
  ]
}
```","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API Management securely routes HTTP requests to Logic Apps.""],
    [""Logic Apps"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""Azure Key Vault"", ""system"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""Data Gateway"", ""component"", ""Logic Apps uses TLS to send the database credentials and a database statement to the on-premises data gateway.""],
    [""SQL Server"", ""system"", ""SQL Server stores the data and makes it available to apps that users access""]
  ],
  ""relationships"": [
    [""API Management"", ""contains"", ""Logic Apps""],
    [""Logic Apps"", ""part-of"", ""API Management""],
    [""Logic Apps"", ""calls"", ""Azure Key Vault""],
    [""Logic Apps"", ""part-of"", ""Data Gateway""],
    [""Data Gateway"", ""part-of"", ""Logic Apps""],
    [""Logic Apps"", ""calls"", ""SQL Server""],
    [""SQL Server"", ""part-of"", ""Logic Apps""]
  ]
}
```",3.2682585470029153
8,gemma:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Custom API"", ""component"", ""Custom API hosted in Azure API Management""],
    [""Custom API Coordinator"", ""component"", ""Custom API Coordinator""],
    [""Power Apps"", ""system""],
    [""Microsoft Teams"", ""system""],
    [""Graph API"", ""component""],
    [""Azure Storage Account"", ""component""],
    [""Azure Functions"", ""component""],
    [""Azure Bot Service"", ""system""],
    [""Custom Bots"", ""component""],
    [""Azure Data Lake"", ""system""],
    [""Power BI"", ""system""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API""],
    [""Custom API Coordinator"", ""contains"", ""Custom API""],
    [""Custom API"", ""calls"", ""Power Apps""],
    [""Custom API"", ""contains"", ""Graph API""],
    [""Graph API"", ""calls"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Custom Bot Service"", ""contains"", ""Custom Bots""],
    [""Custom Bots"", ""calls"", ""Microsoft Teams""],
    [""Azure Event Hubs"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""contains"", ""Power BI""]
  ]
}
```","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Custom API"", ""component"", ""Custom API hosted in Azure API Management""],
    [""Custom API Coordinator"", ""component"", ""Custom API Coordinator""],
    [""Power Apps"", ""system""],
    [""Microsoft Teams"", ""system""],
    [""Graph API"", ""component""],
    [""Azure Storage Account"", ""component""],
    [""Azure Functions"", ""component""],
    [""Azure Bot Service"", ""system""],
    [""Custom Bots"", ""component""],
    [""Azure Data Lake"", ""system""],
    [""Power BI"", ""system""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API""],
    [""Custom API Coordinator"", ""contains"", ""Custom API""],
    [""Custom API"", ""calls"", ""Power Apps""],
    [""Custom API"", ""contains"", ""Graph API""],
    [""Graph API"", ""calls"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Custom Bot Service"", ""contains"", ""Custom Bots""],
    [""Custom Bots"", ""calls"", ""Microsoft Teams""],
    [""Azure Event Hubs"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""contains"", ""Power BI""]
  ]
}
```",3.9556223349645734
9,gemma:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API Management Platform""],
    [""Patient API"", ""component"", ""API for managing patients""],
    [""Audit API"", ""component"", ""API for creating audit records""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing patient data""],
    [""CreatePatient"", ""function"", ""Function for creating a patient""],
    [""CreateAuditRecord"", ""function"", ""Function for creating an audit record""]
  ],
  ""relationships"": [
    [""calls"", ""APIM"", ""Patient API""],
    [""calls"", ""Patient API"", ""CreatePatient""],
    [""calls"", ""CreatePatient"", ""Audit API CreateAuditRecord""],
    [""contains"", ""Patient API"", ""CreatePatient""],
    [""contains"", ""Audit API"", ""CreateAuditRecord""]
  ]
}
```","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API Management Platform""],
    [""Patient API"", ""component"", ""API for managing patients""],
    [""Audit API"", ""component"", ""API for creating audit records""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing patient data""],
    [""CreatePatient"", ""function"", ""Function for creating a patient""],
    [""CreateAuditRecord"", ""function"", ""Function for creating an audit record""]
  ],
  ""relationships"": [
    [""calls"", ""APIM"", ""Patient API""],
    [""calls"", ""Patient API"", ""CreatePatient""],
    [""calls"", ""CreatePatient"", ""Audit API CreateAuditRecord""],
    [""contains"", ""Patient API"", ""CreatePatient""],
    [""contains"", ""Audit API"", ""CreateAuditRecord""]
  ]
}
```",2.701157625997439
10,gemma:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""Version control system""],
    [""Azure VM"", ""component"", ""Virtual machine""],
    [""API application"", ""component"", ""Software application""],
    [""Azure API Management"", ""system"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Reverse proxy""],
    [""ASE"", ""system"", ""Application service environment""],
    [""External users"", ""person"", ""Users""],
    [""Private DNS"", ""component"", ""Domain name service""]
  ],
  ""relationships"": [
    [""GitHub"", ""contains"", ""Azure VM""],
    [""Azure VM"", ""part-of"", ""API application""],
    [""API application"", ""calls"", ""Azure API Management""],
    [""Azure API Management"", ""part-of"", ""ASE""],
    [""Application Gateway"", ""contains"", ""Azure API Management""],
    [""ASE"", ""contains"", ""API Management""],
    [""Azure API Management"", ""uses"", ""Private DNS""],
    [""Application Gateway"", ""exposes"", ""Azure API Management""]
  ]
}
```","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""Version control system""],
    [""Azure VM"", ""component"", ""Virtual machine""],
    [""API application"", ""component"", ""Software application""],
    [""Azure API Management"", ""system"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Reverse proxy""],
    [""ASE"", ""system"", ""Application service environment""],
    [""External users"", ""person"", ""Users""],
    [""Private DNS"", ""component"", ""Domain name service""]
  ],
  ""relationships"": [
    [""GitHub"", ""contains"", ""Azure VM""],
    [""Azure VM"", ""part-of"", ""API application""],
    [""API application"", ""calls"", ""Azure API Management""],
    [""Azure API Management"", ""part-of"", ""ASE""],
    [""Application Gateway"", ""contains"", ""Azure API Management""],
    [""ASE"", ""contains"", ""API Management""],
    [""Azure API Management"", ""uses"", ""Private DNS""],
    [""Application Gateway"", ""exposes"", ""Azure API Management""]
  ]
}
```",3.2744222990004346
11,gemma:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Data processing platform""],
    [""Dataset"", ""component"", ""Data storage""],
    [""Microsoft-managed keys"", ""component"", ""Encryption""],
    [""Secure storage account"", ""component"", ""Data storage""],
    [""Streaming application"", ""component"", ""Access management""],
    [""Data science VMs"", ""component"", ""Compute environment""],
    [""Azure Machine Learning"", ""system"", ""Machine learning platform""],
    [""Logic app"", ""component"", ""Notification""],
    [""Reviewers"", ""person"", ""Data security""],
    [""Public storage account"", ""component"", ""Data access""]
  ],
  ""relationships"": [
    [""Data Factory"", ""contains"", ""Dataset""],
    [""Dataset"", ""part-of"", ""Secure storage account""],
    [""Secure storage account"", ""part-of"", ""Azure Machine Learning""],
    [""Secure storage account"", ""contains"", ""Dataset""],
    [""Streaming application"", ""calls"", ""Data science VMs""],
    [""Data science VMs"", ""contains"", ""Dataset""],
    [""Azure Machine Learning"", ""contains"", ""Dataset""],
    [""Secure storage account"", ""contains"", ""Models""],
    [""Logic app"", ""calls"", ""Reviewers""],
    [""Reviewers"", ""calls"", ""Data Factory""],
    [""Data Factory"", ""contains"", ""Public storage account""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Data processing platform""],
    [""Dataset"", ""component"", ""Data storage""],
    [""Microsoft-managed keys"", ""component"", ""Encryption""],
    [""Secure storage account"", ""component"", ""Data storage""],
    [""Streaming application"", ""component"", ""Access management""],
    [""Data science VMs"", ""component"", ""Compute environment""],
    [""Azure Machine Learning"", ""system"", ""Machine learning platform""],
    [""Logic app"", ""component"", ""Notification""],
    [""Reviewers"", ""person"", ""Data security""],
    [""Public storage account"", ""component"", ""Data access""]
  ],
  ""relationships"": [
    [""Data Factory"", ""contains"", ""Dataset""],
    [""Dataset"", ""part-of"", ""Secure storage account""],
    [""Secure storage account"", ""part-of"", ""Azure Machine Learning""],
    [""Secure storage account"", ""contains"", ""Dataset""],
    [""Streaming application"", ""calls"", ""Data science VMs""],
    [""Data science VMs"", ""contains"", ""Dataset""],
    [""Azure Machine Learning"", ""contains"", ""Dataset""],
    [""Secure storage account"", ""contains"", ""Models""],
    [""Logic app"", ""calls"", ""Reviewers""],
    [""Reviewers"", ""calls"", ""Data Factory""],
    [""Data Factory"", ""contains"", ""Public storage account""]
  ]
}
```",4.3010441599763
12,gemma:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity provider configured with a custom domain for sign-in""],
    [""Azure Front Door"", ""component"", ""Proxy service""],
    [""API Management"", ""system"", ""API gateway""],
    [""JWT"", ""component"", ""JSON Web Token""],
    [""App Service Environment"", ""component"", ""Cloud-based app hosting""],
    [""Function Apps"", ""component"", ""Serverless compute platform""],
    [""AKS"", ""component"", ""Kubernetes cluster""],
    [""User"", ""person"", ""Application user""]
  ],
  ""relationships"": [
    [""User"", ""calls"", ""Azure Front Door""],
    [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure AD B2C"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""App Service Environment""],
    [""API Management"", ""contains"", ""Function Apps""],
    [""API Management"", ""contains"", ""AKS""],
    [""API Management"", ""calls"", ""Backend API""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity provider configured with a custom domain for sign-in""],
    [""Azure Front Door"", ""component"", ""Proxy service""],
    [""API Management"", ""system"", ""API gateway""],
    [""JWT"", ""component"", ""JSON Web Token""],
    [""App Service Environment"", ""component"", ""Cloud-based app hosting""],
    [""Function Apps"", ""component"", ""Serverless compute platform""],
    [""AKS"", ""component"", ""Kubernetes cluster""],
    [""User"", ""person"", ""Application user""]
  ],
  ""relationships"": [
    [""User"", ""calls"", ""Azure Front Door""],
    [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure AD B2C"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""App Service Environment""],
    [""API Management"", ""contains"", ""Function Apps""],
    [""API Management"", ""contains"", ""AKS""],
    [""API Management"", ""calls"", ""Backend API""]
  ]
}
```",3.96444385900395
13,gemma:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech recognition service for transcribing audio""],
    [""Azure AI Language"", ""system"", ""Natural language processing service for detecting personal data""],
    [""Azure OpenAI"", ""system"", ""AI-powered platform for processing text""],
    [""Power BI"", ""component"", ""Business intelligence tool for visualizing data""],
    [""App Service"", ""component"", ""Cloud-based platform for hosting web applications""],
    [""CRM"", ""component"", ""Customer relationship management system""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
    [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""contains"", ""Entities""],
    [""Azure OpenAI"", ""contains"", ""Conversation summary""],
    [""Azure OpenAI"", ""contains"", ""Sentiment analysis""],
    [""Azure OpenAI"", ""calls"", ""Power BI""],
    [""Azure OpenAI"", ""calls"", ""App Service""],
    [""App Service"", ""calls"", ""CRM""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech recognition service for transcribing audio""],
    [""Azure AI Language"", ""system"", ""Natural language processing service for detecting personal data""],
    [""Azure OpenAI"", ""system"", ""AI-powered platform for processing text""],
    [""Power BI"", ""component"", ""Business intelligence tool for visualizing data""],
    [""App Service"", ""component"", ""Cloud-based platform for hosting web applications""],
    [""CRM"", ""component"", ""Customer relationship management system""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
    [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""contains"", ""Entities""],
    [""Azure OpenAI"", ""contains"", ""Conversation summary""],
    [""Azure OpenAI"", ""contains"", ""Sentiment analysis""],
    [""Azure OpenAI"", ""calls"", ""Power BI""],
    [""Azure OpenAI"", ""calls"", ""App Service""],
    [""App Service"", ""calls"", ""CRM""]
  ]
}
```",3.9916042370023206
14,gemma:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""Version control platform""],
    [""CI/CD pipeline agent"", ""component"", ""Continuous integration and continuous delivery pipeline agent""],
    [""Azure VM"", ""component"", ""Virtual machine""],
    [""API application"", ""component"", ""Application""],
    [""ILB ASE"", ""system"", ""Azure Service Environment""],
    [""API Management"", ""system"", ""Azure API Management""],
    [""Application Gateway"", ""component"", ""Ingress controller""],
    [""Azure Private DNS"", ""component"", ""Domain name system""],
    [""External users"", ""person"", ""Users outside the organization""]
  ],
  ""relationships"": [
    [""developers"", ""calls"", ""GitHub""],
    [""GitHub"", ""contains"", ""CI/CD pipeline agent""],
    [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
    [""CI/CD pipeline agent"", ""calls"", ""API application""],
    [""API application"", ""part-of"", ""ILB ASE""],
    [""API Management"", ""calls"", ""API application""],
    [""API Management"", ""contains"", ""HOST header""],
    [""API Management"", ""part-of"", ""ILB ASE""],
    [""Application Gateway"", ""part-of"", ""ILB ASE""],
    [""Azure Private DNS"", ""part-of"", ""ILB ASE""],
    [""Application Gateway"", ""calls"", ""API Management""],
    [""API Management"", ""called-from"", ""Application Gateway""]
  ]
}
```","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""Version control platform""],
    [""CI/CD pipeline agent"", ""component"", ""Continuous integration and continuous delivery pipeline agent""],
    [""Azure VM"", ""component"", ""Virtual machine""],
    [""API application"", ""component"", ""Application""],
    [""ILB ASE"", ""system"", ""Azure Service Environment""],
    [""API Management"", ""system"", ""Azure API Management""],
    [""Application Gateway"", ""component"", ""Ingress controller""],
    [""Azure Private DNS"", ""component"", ""Domain name system""],
    [""External users"", ""person"", ""Users outside the organization""]
  ],
  ""relationships"": [
    [""developers"", ""calls"", ""GitHub""],
    [""GitHub"", ""contains"", ""CI/CD pipeline agent""],
    [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
    [""CI/CD pipeline agent"", ""calls"", ""API application""],
    [""API application"", ""part-of"", ""ILB ASE""],
    [""API Management"", ""calls"", ""API application""],
    [""API Management"", ""contains"", ""HOST header""],
    [""API Management"", ""part-of"", ""ILB ASE""],
    [""Application Gateway"", ""part-of"", ""ILB ASE""],
    [""Azure Private DNS"", ""part-of"", ""ILB ASE""],
    [""Application Gateway"", ""calls"", ""API Management""],
    [""API Management"", ""called-from"", ""Application Gateway""]
  ]
}
```",4.3833224209956825
15,gemma:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
    [""Command message"", ""system"", ""Message containing metadata""],
    [""Durable Functions"", ""system"", ""Orchestration framework""],
    [""Computer Vision Read API"", ""component"", ""Optical character recognition API""],
    [""Document classifier service"", ""component"", ""Service for classifying documents""],
    [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
    [""Azure Cosmos DB"", ""system"", ""Document database""],
    [""Cognitive Search"", ""system"", ""Search engine""],
    [""Azure AI Search libraries"", ""component"", ""Libraries for searching data""]
  ],
  ""relationships"": [
    [""Contains"", ""Command message"", ""Durable Functions""],
    [""Calls"", ""Durable Functions"", ""Computer Vision Read API""],
    [""Called-from"", ""Computer Vision Read API"", ""Durable Functions""],
    [""Calls"", ""Durable Functions"", ""Document classifier service""],
    [""Contains"", ""Document classifier service"", ""Azure Kubernetes Service""],
    [""Contains"", ""Document file"", ""Azure Blob Storage""],
    [""Contains"", ""Document file"", ""Multiple document types""],
    [""Calls"", ""Classify activity function"", ""Document classifier service""],
    [""Called-from"", ""Document classifier service"", ""Classify activity function""],
    [""Contains"", ""Document type"", ""Azure Cosmos DB""],
    [""Contains"", ""OCR results"", ""Search document""],
    [""Contains"", ""Correlation ID"", ""Search document""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
    [""Command message"", ""system"", ""Message containing metadata""],
    [""Durable Functions"", ""system"", ""Orchestration framework""],
    [""Computer Vision Read API"", ""component"", ""Optical character recognition API""],
    [""Document classifier service"", ""component"", ""Service for classifying documents""],
    [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
    [""Azure Cosmos DB"", ""system"", ""Document database""],
    [""Cognitive Search"", ""system"", ""Search engine""],
    [""Azure AI Search libraries"", ""component"", ""Libraries for searching data""]
  ],
  ""relationships"": [
    [""Contains"", ""Command message"", ""Durable Functions""],
    [""Calls"", ""Durable Functions"", ""Computer Vision Read API""],
    [""Called-from"", ""Computer Vision Read API"", ""Durable Functions""],
    [""Calls"", ""Durable Functions"", ""Document classifier service""],
    [""Contains"", ""Document classifier service"", ""Azure Kubernetes Service""],
    [""Contains"", ""Document file"", ""Azure Blob Storage""],
    [""Contains"", ""Document file"", ""Multiple document types""],
    [""Calls"", ""Classify activity function"", ""Document classifier service""],
    [""Called-from"", ""Document classifier service"", ""Classify activity function""],
    [""Contains"", ""Document type"", ""Azure Cosmos DB""],
    [""Contains"", ""OCR results"", ""Search document""],
    [""Contains"", ""Correlation ID"", ""Search document""]
  ]
}
```",4.632985588978045
16,gemma:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system containing supplier data""],
    [""Data Factory"", ""component"", ""Data extraction pipeline""],
    [""ERP system"", ""system"", ""System containing purchase order data""],
    [""Azure SQL Database"", ""system"", ""Database storing purchase order data""],
    [""Azure Functions"", ""component"", ""Automated process for surfacing purchase order data""],
    [""API"", ""component"", ""Interface for accessing purchase order data""],
    [""Power Apps"", ""component"", ""Application for reviewing and updating purchase orders""],
    [""Suppliers"", ""external"", ""Organizations that provide goods or services""],
    [""CSV"", ""file_type"", ""Comma-separated values file""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""Data Factory"", ""calls"", ""ERP system""],
    [""ERP system"", ""part-of"", ""Azure SQL Database""],
    [""Azure Functions"", ""calls"", ""API""],
    [""API"", ""part-of"", ""Azure SQL Database""],
    [""Power Apps"", ""calls"", ""API""],
    [""Power Apps"", ""part-of"", ""Azure SQL Database""],
    [""Power Apps"", ""calls"", ""Suppliers""],
    [""Suppliers"", ""part-of"", ""CSV""]
  ]
}
```","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system containing supplier data""],
    [""Data Factory"", ""component"", ""Data extraction pipeline""],
    [""ERP system"", ""system"", ""System containing purchase order data""],
    [""Azure SQL Database"", ""system"", ""Database storing purchase order data""],
    [""Azure Functions"", ""component"", ""Automated process for surfacing purchase order data""],
    [""API"", ""component"", ""Interface for accessing purchase order data""],
    [""Power Apps"", ""component"", ""Application for reviewing and updating purchase orders""],
    [""Suppliers"", ""external"", ""Organizations that provide goods or services""],
    [""CSV"", ""file_type"", ""Comma-separated values file""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""Data Factory"", ""calls"", ""ERP system""],
    [""ERP system"", ""part-of"", ""Azure SQL Database""],
    [""Azure Functions"", ""calls"", ""API""],
    [""API"", ""part-of"", ""Azure SQL Database""],
    [""Power Apps"", ""calls"", ""API""],
    [""Power Apps"", ""part-of"", ""Azure SQL Database""],
    [""Power Apps"", ""calls"", ""Suppliers""],
    [""Suppliers"", ""part-of"", ""CSV""]
  ]
}
```",3.8551811419893056
17,gemma:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API gateway that receives client requests""],
    [""Patient API"", ""component"", ""API for managing patients""],
    [""Audit API"", ""component"", ""API for creating audit records""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing patient and audit data""],
    [""CreatePatient"", ""function"", ""Function for creating a patient""],
    [""CreateAuditRecord"", ""function"", ""Function for creating an audit record""]
  ],
  ""relationships"": [
    [""calls"", ""APIM"", ""Patient API""],
    [""calls"", ""Patient API"", ""CreatePatient""],
    [""calls"", ""CreatePatient"", ""Audit API""],
    [""contains"", ""CreatePatient"", ""Azure Cosmos DB""],
    [""contains"", ""Audit API"", ""Azure Cosmos DB""]
  ]
}
```","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API gateway that receives client requests""],
    [""Patient API"", ""component"", ""API for managing patients""],
    [""Audit API"", ""component"", ""API for creating audit records""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing patient and audit data""],
    [""CreatePatient"", ""function"", ""Function for creating a patient""],
    [""CreateAuditRecord"", ""function"", ""Function for creating an audit record""]
  ],
  ""relationships"": [
    [""calls"", ""APIM"", ""Patient API""],
    [""calls"", ""Patient API"", ""CreatePatient""],
    [""calls"", ""CreatePatient"", ""Audit API""],
    [""contains"", ""CreatePatient"", ""Azure Cosmos DB""],
    [""contains"", ""Audit API"", ""Azure Cosmos DB""]
  ]
}
```",2.747996825957671
18,gemma:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Azure API Management"", ""system"", ""Custom API host""],
    [""Custom API Coordinator"", ""component"", ""API notification handler""],
    [""Power Apps"", ""system"", ""Flight assignment platform""],
    [""Microsoft Teams"", ""system"", ""Collaboration platform""],
    [""Graph API"", ""component"", ""Communication protocol""],
    [""Azure Storage Account"", ""system"", ""Message queue""],
    [""Azure Functions"", ""system"", ""API call processor""],
    [""Azure Event Hubs"", ""system"", ""Event streaming platform""],
    [""Azure Data Lake"", ""system"", ""Data storage""],
    [""Power BI"", ""system"", ""Reporting tool""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API Coordinator""],
    [""Custom API Coordinator"", ""calls"", ""Power Apps""],
    [""Custom API Coordinator"", ""calls"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Custom API Coordinator"", ""part-of"", ""Airline System""],
    [""Azure Storage Account"", ""part-of"", ""Azure Functions""],
    [""Azure Functions"", ""part-of"", ""Airline System""],
    [""Azure Event Hubs"", ""part-of"", ""Airline System""],
    [""Azure Data Lake"", ""part-of"", ""Airline System""],
    [""Power BI"", ""part-of"", ""Airline System""]
  ]
}
```","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Azure API Management"", ""system"", ""Custom API host""],
    [""Custom API Coordinator"", ""component"", ""API notification handler""],
    [""Power Apps"", ""system"", ""Flight assignment platform""],
    [""Microsoft Teams"", ""system"", ""Collaboration platform""],
    [""Graph API"", ""component"", ""Communication protocol""],
    [""Azure Storage Account"", ""system"", ""Message queue""],
    [""Azure Functions"", ""system"", ""API call processor""],
    [""Azure Event Hubs"", ""system"", ""Event streaming platform""],
    [""Azure Data Lake"", ""system"", ""Data storage""],
    [""Power BI"", ""system"", ""Reporting tool""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API Coordinator""],
    [""Custom API Coordinator"", ""calls"", ""Power Apps""],
    [""Custom API Coordinator"", ""calls"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Custom API Coordinator"", ""part-of"", ""Airline System""],
    [""Azure Storage Account"", ""part-of"", ""Azure Functions""],
    [""Azure Functions"", ""part-of"", ""Airline System""],
    [""Azure Event Hubs"", ""part-of"", ""Airline System""],
    [""Azure Data Lake"", ""part-of"", ""Airline System""],
    [""Power BI"", ""part-of"", ""Airline System""]
  ]
}
```",4.390491297002882
19,gemma:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}","```json
{
  ""entities"": [
    [""Connect agent"", ""component"", ""Capture change logs from mainframe/midrange systems""],
    [""Publisher"", ""component"", ""Manage data migration for mainframe systems""],
    [""Listener"", ""component"", ""Manage data migration for midrange systems""],
    [""Enhanced-security connection"", ""connection"", ""Secure data transfer from on-premises to Azure""],
    [""Replicator Engine"", ""engine"", ""Capture data from publisher/listener and apply to target""],
    [""Target"", ""database"", ""Receive changes via ODBC or Azure Event Hubs""],
    [""Azure Databricks"", ""service"", ""Consume changed data""],
    [""Azure data platform services"", ""service"", ""Apply changes from changed data""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent"", ""Publisher""],
    [""part-of"", ""Publisher"", ""Enhanced-security connection""],
    [""contains"", ""Connect agent"", ""Listener""],
    [""part-of"", ""Listener"", ""Enhanced-security connection""],
    [""calls"", ""Publisher/Listener"", ""Replicator Engine""],
    [""called-from"", ""Replicator Engine"", ""Publisher/Listener""],
    [""contains"", ""Replicator Engine"", ""Target""],
    [""part-of"", ""Target"", ""Azure Databricks""],
    [""part-of"", ""Target"", ""Azure data platform services""]
  ]
}
```","```json
{
  ""entities"": [
    [""Connect agent"", ""component"", ""Capture change logs from mainframe/midrange systems""],
    [""Publisher"", ""component"", ""Manage data migration for mainframe systems""],
    [""Listener"", ""component"", ""Manage data migration for midrange systems""],
    [""Enhanced-security connection"", ""connection"", ""Secure data transfer from on-premises to Azure""],
    [""Replicator Engine"", ""engine"", ""Capture data from publisher/listener and apply to target""],
    [""Target"", ""database"", ""Receive changes via ODBC or Azure Event Hubs""],
    [""Azure Databricks"", ""service"", ""Consume changed data""],
    [""Azure data platform services"", ""service"", ""Apply changes from changed data""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent"", ""Publisher""],
    [""part-of"", ""Publisher"", ""Enhanced-security connection""],
    [""contains"", ""Connect agent"", ""Listener""],
    [""part-of"", ""Listener"", ""Enhanced-security connection""],
    [""calls"", ""Publisher/Listener"", ""Replicator Engine""],
    [""called-from"", ""Replicator Engine"", ""Publisher/Listener""],
    [""contains"", ""Replicator Engine"", ""Target""],
    [""part-of"", ""Target"", ""Azure Databricks""],
    [""part-of"", ""Target"", ""Azure data platform services""]
  ]
}
```",4.0440107570029795
20,gemma:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Cloud data pipeline tool""],
    [""Blob storage"", ""component"", ""Data storage""],
    [""Microsoft-managed keys"", ""component"", ""Encryption key management""],
    [""Azure Virtual Desktop"", ""component"", ""Remote access""],
    [""Data science VMs"", ""component"", ""Virtual machines""],
    [""Azure Machine Learning"", ""system"", ""Machine learning platform""],
    [""Logic app"", ""component"", ""Workflow automation""],
    [""Reviewers"", ""person""],
    [""Public storage account"", ""component"", ""Data storage""],
    [""Data"", ""component"", ""Dataset""]
  ],
  ""relationships"": [
    [""Azure Data Factory"", ""calls"", ""Trigger""],
    [""Trigger"", ""calls"", ""Copy data""],
    [""Data"", ""contains"", ""Microsoft-managed keys""],
    [""Azure Virtual Desktop"", ""part-of"", ""Secure environment""],
    [""Secure environment"", ""contains"", ""Data science VMs""],
    [""Data science VMs"", ""part-of"", ""Secure environment""],
    [""Azure Machine Learning"", ""part-of"", ""Secure environment""],
    [""Data"", ""part-of"", ""Secure storage account""],
    [""Logic app"", ""calls"", ""Approval process""],
    [""Approval process"", ""calls"", ""Data""],
    [""Data"", ""part-of"", ""Public storage account""],
    [""Public storage account"", ""part-of"", ""Data exfiltration process""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Cloud data pipeline tool""],
    [""Blob storage"", ""component"", ""Data storage""],
    [""Microsoft-managed keys"", ""component"", ""Encryption key management""],
    [""Azure Virtual Desktop"", ""component"", ""Remote access""],
    [""Data science VMs"", ""component"", ""Virtual machines""],
    [""Azure Machine Learning"", ""system"", ""Machine learning platform""],
    [""Logic app"", ""component"", ""Workflow automation""],
    [""Reviewers"", ""person""],
    [""Public storage account"", ""component"", ""Data storage""],
    [""Data"", ""component"", ""Dataset""]
  ],
  ""relationships"": [
    [""Azure Data Factory"", ""calls"", ""Trigger""],
    [""Trigger"", ""calls"", ""Copy data""],
    [""Data"", ""contains"", ""Microsoft-managed keys""],
    [""Azure Virtual Desktop"", ""part-of"", ""Secure environment""],
    [""Secure environment"", ""contains"", ""Data science VMs""],
    [""Data science VMs"", ""part-of"", ""Secure environment""],
    [""Azure Machine Learning"", ""part-of"", ""Secure environment""],
    [""Data"", ""part-of"", ""Secure storage account""],
    [""Logic app"", ""calls"", ""Approval process""],
    [""Approval process"", ""calls"", ""Data""],
    [""Data"", ""part-of"", ""Public storage account""],
    [""Public storage account"", ""part-of"", ""Data exfiltration process""]
  ]
}
```",4.59309633198427
21,gemma:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API Management securely routes HTTP requests to Logic Apps.""],
    [""Logic Apps"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""Azure Key Vault"", ""system"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""Transport Layer Security"", ""system"", ""Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
    [""Data Gateway"", ""component"", ""The on-premises data gateway connects to a SQL Server database to run the statement.""],
    [""SQL Server"", ""component"", ""SQL Server stores the data and makes it available to apps that users access""]
  ],
  ""relationships"": [
    [""API Management"", ""contains"", ""Logic Apps""],
    [""Logic Apps"", ""calls"", ""Azure Key Vault""],
    [""Logic Apps"", ""contains"", ""Transport Layer Security""],
    [""Transport Layer Security"", ""part-of"", ""Logic Apps""],
    [""Logic Apps"", ""contains"", ""Data Gateway""],
    [""Data Gateway"", ""part-of"", ""Logic Apps""],
    [""Logic Apps"", ""contains"", ""SQL Server""],
    [""SQL Server"", ""part-of"", ""Logic Apps""]
  ]
}
```","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API Management securely routes HTTP requests to Logic Apps.""],
    [""Logic Apps"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""Azure Key Vault"", ""system"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""Transport Layer Security"", ""system"", ""Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
    [""Data Gateway"", ""component"", ""The on-premises data gateway connects to a SQL Server database to run the statement.""],
    [""SQL Server"", ""component"", ""SQL Server stores the data and makes it available to apps that users access""]
  ],
  ""relationships"": [
    [""API Management"", ""contains"", ""Logic Apps""],
    [""Logic Apps"", ""calls"", ""Azure Key Vault""],
    [""Logic Apps"", ""contains"", ""Transport Layer Security""],
    [""Transport Layer Security"", ""part-of"", ""Logic Apps""],
    [""Logic Apps"", ""contains"", ""Data Gateway""],
    [""Data Gateway"", ""part-of"", ""Logic Apps""],
    [""Logic Apps"", ""contains"", ""SQL Server""],
    [""SQL Server"", ""part-of"", ""Logic Apps""]
  ]
}
```",3.779127792979125
22,gemma:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF attachments""],
    [""Logic App"", ""component"", ""Logic app built using Azure Logic Apps""],
    [""PDF container"", ""component"", ""Container in Azure Data Lake Storage for PDF files""],
    [""Function App"", ""component"", ""Function app built using Azure Functions""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud-based storage service for data""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database""],
    [""Power BI"", ""person"", ""Business intelligence and data analytics platform""]
  ],
  ""relationships"": [
    [""contains"", ""Logic App"", ""Outlook email account""],
    [""part-of"", ""PDF container"", ""Azure Data Lake Storage""],
    [""calls"", ""PDF container"", ""Logic App""],
    [""called-from"", ""Logic App"", ""Function App""],
    [""contains"", ""Function App"", ""Azure Functions""],
    [""contains"", ""Function App"", ""PDF""],
    [""part-of"", ""PDF"", ""Data Lake Storage""],
    [""calls"", ""Function App"", ""AI Document Intelligence""],
    [""called-from"", ""AI Document Intelligence"", ""Function App""],
    [""contains"", ""Function App"", ""Response data""],
    [""part-of"", ""Response data"", ""Data Lake Storage""],
    [""contains"", ""Forms processing logic App"", ""Processed data""],
    [""part-of"", ""Processed data"", ""Azure Cosmos DB""],
    [""calls"", ""Azure Cosmos DB"", ""Power BI""],
    [""called-from"", ""Power BI"", ""Azure Cosmos DB""]
  ]
}
```","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF attachments""],
    [""Logic App"", ""component"", ""Logic app built using Azure Logic Apps""],
    [""PDF container"", ""component"", ""Container in Azure Data Lake Storage for PDF files""],
    [""Function App"", ""component"", ""Function app built using Azure Functions""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud-based storage service for data""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database""],
    [""Power BI"", ""person"", ""Business intelligence and data analytics platform""]
  ],
  ""relationships"": [
    [""contains"", ""Logic App"", ""Outlook email account""],
    [""part-of"", ""PDF container"", ""Azure Data Lake Storage""],
    [""calls"", ""PDF container"", ""Logic App""],
    [""called-from"", ""Logic App"", ""Function App""],
    [""contains"", ""Function App"", ""Azure Functions""],
    [""contains"", ""Function App"", ""PDF""],
    [""part-of"", ""PDF"", ""Data Lake Storage""],
    [""calls"", ""Function App"", ""AI Document Intelligence""],
    [""called-from"", ""AI Document Intelligence"", ""Function App""],
    [""contains"", ""Function App"", ""Response data""],
    [""part-of"", ""Response data"", ""Data Lake Storage""],
    [""contains"", ""Forms processing logic App"", ""Processed data""],
    [""part-of"", ""Processed data"", ""Azure Cosmos DB""],
    [""calls"", ""Azure Cosmos DB"", ""Power BI""],
    [""called-from"", ""Power BI"", ""Azure Cosmos DB""]
  ]
}
```",5.014214746013749
23,gemma:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform that triggers on HTTP requests""],
    [""Azure API Management"", ""system"", ""API gateway and developer portal""],
    [""API Gateway"", ""component"", ""Accepts HTTP calls and routes them to backend""],
    [""Developer Portal"", ""component"", ""Provides access to API documentation and code samples""],
    [""Azure DNS"", ""system"", ""Name resolution service""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service for API Gateway""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""contains"", ""Workflow""],
    [""Azure Logic Apps"", ""calls"", ""Connectors""],
    [""Azure API Management"", ""contains"", ""API Gateway""],
    [""Azure API Management"", ""contains"", ""Developer Portal""],
    [""Azure API Management"", ""part-of"", ""Azure DNS""],
    [""Azure DNS"", ""calls"", ""Microsoft Entra ID""],
    [""Microsoft Entra ID"", ""calls"", ""API Gateway""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform that triggers on HTTP requests""],
    [""Azure API Management"", ""system"", ""API gateway and developer portal""],
    [""API Gateway"", ""component"", ""Accepts HTTP calls and routes them to backend""],
    [""Developer Portal"", ""component"", ""Provides access to API documentation and code samples""],
    [""Azure DNS"", ""system"", ""Name resolution service""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service for API Gateway""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""contains"", ""Workflow""],
    [""Azure Logic Apps"", ""calls"", ""Connectors""],
    [""Azure API Management"", ""contains"", ""API Gateway""],
    [""Azure API Management"", ""contains"", ""Developer Portal""],
    [""Azure API Management"", ""part-of"", ""Azure DNS""],
    [""Azure DNS"", ""calls"", ""Microsoft Entra ID""],
    [""Microsoft Entra ID"", ""calls"", ""API Gateway""]
  ]
}
```",3.3432555409963243
24,gemma:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}","```json
{
  ""entities"": [
    [ ""CDS"", ""system"", ""Data storage system containing supplier data"" ],
    [ ""SQL"", ""system"", ""Database containing purchase order data"" ],
    [ ""Azure Functions"", ""component"", ""Function app that extracts purchase order data"" ],
    [ ""API"", ""component"", ""Application programming interface for accessing purchase order data"" ],
    [ ""Power Apps"", ""component"", ""Application for reviewing and updating purchase orders"" ],
    [ ""Azure SQL Database"", ""system"", ""Database containing purchase order data accessed by Power Apps"" ]
  ],
  ""relationships"": [
    [ ""contains"", ""CDS"", ""SQL"" ],
    [ ""part-of"", ""Azure Functions"", ""API"" ],
    [ ""calls"", ""Azure Functions"", ""API"" ],
    [ ""calls"", ""Power Apps"", ""Azure SQL Database"" ]
  ]
}
```","```json
{
  ""entities"": [
    [ ""CDS"", ""system"", ""Data storage system containing supplier data"" ],
    [ ""SQL"", ""system"", ""Database containing purchase order data"" ],
    [ ""Azure Functions"", ""component"", ""Function app that extracts purchase order data"" ],
    [ ""API"", ""component"", ""Application programming interface for accessing purchase order data"" ],
    [ ""Power Apps"", ""component"", ""Application for reviewing and updating purchase orders"" ],
    [ ""Azure SQL Database"", ""system"", ""Database containing purchase order data accessed by Power Apps"" ]
  ],
  ""relationships"": [
    [ ""contains"", ""CDS"", ""SQL"" ],
    [ ""part-of"", ""Azure Functions"", ""API"" ],
    [ ""calls"", ""Azure Functions"", ""API"" ],
    [ ""calls"", ""Power Apps"", ""Azure SQL Database"" ]
  ]
}
```",2.8331492220168
25,gemma:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF attachments""],
    [""Logic app"", ""component"", ""Logic app for processing email attachments""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud storage for PDF files""],
    [""Function app"", ""component"", ""Function app for processing PDF forms""],
    [""AI Document Intelligence"", ""system"", ""Microsoft AI service for document processing""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database for storing processed data""],
    [""Power BI"", ""component"", ""Business intelligence tool for data analysis""]
  ],
  ""relationships"": [
    [""contains"", ""Logic app"", ""Outlook email account""],
    [""part-of"", ""PDF container"", ""Data Lake Storage""],
    [""calls"", ""PDF container logic app"", ""Function app""],
    [""called-from"", ""Function app"", ""AI Document Intelligence""],
    [""contains"", ""Function app"", ""Single-page PDF files""],
    [""part-of"", ""Single-page PDF files"", ""Data Lake Storage""],
    [""calls"", ""Function app"", ""Cosmos DB""],
    [""called-from"", ""Cosmos DB"", ""Power BI""]
  ]
}
```","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF attachments""],
    [""Logic app"", ""component"", ""Logic app for processing email attachments""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud storage for PDF files""],
    [""Function app"", ""component"", ""Function app for processing PDF forms""],
    [""AI Document Intelligence"", ""system"", ""Microsoft AI service for document processing""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database for storing processed data""],
    [""Power BI"", ""component"", ""Business intelligence tool for data analysis""]
  ],
  ""relationships"": [
    [""contains"", ""Logic app"", ""Outlook email account""],
    [""part-of"", ""PDF container"", ""Data Lake Storage""],
    [""calls"", ""PDF container logic app"", ""Function app""],
    [""called-from"", ""Function app"", ""AI Document Intelligence""],
    [""contains"", ""Function app"", ""Single-page PDF files""],
    [""part-of"", ""Single-page PDF files"", ""Data Lake Storage""],
    [""calls"", ""Function app"", ""Cosmos DB""],
    [""called-from"", ""Cosmos DB"", ""Power BI""]
  ]
}
```",3.881194156012498
26,gemma:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing document metadata""],
    [""Cognitive Search"", ""system"", ""Search engine for searching documents""],
    [""Computer Vision Read API"", ""component"", ""Optical character recognition API""],
    [""Document Classifier Service"", ""component"", ""Service for classifying document types""],
    [""Indexing activity function"", ""component"", ""Creates search documents""],
    [""Metadata Store activity function"", ""component"", ""Saves document type information""],
    [""Durable Functions"", ""system"", ""Orchestrator for pipeline processing""],
    [""User"", ""person"", ""End user of the web app""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Document file""],
    [""Durable Functions"", ""calls"", ""Command message""],
    [""Command message"", ""called-from"", ""Durable Functions""],
    [""Scan activity function"", ""calls"", ""Computer Vision Read API""],
    [""Computer Vision Read API"", ""calls"", ""Document Classifier Service""],
    [""Document Classifier Service"", ""calls"", ""Azure Kubernetes Service""],
    [""Document Classifier Service"", ""returns"", ""Document type and page range""],
    [""Metadata Store activity function"", ""writes"", ""Document type and page range""],
    [""Indexing activity function"", ""writes"", ""Search document""],
    [""User"", ""searches"", ""Cognitive Search""],
    [""Search document"", ""has"", ""Correlation ID""],
    [""Correlation ID"", ""reads"", ""Azure Cosmos DB""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing document metadata""],
    [""Cognitive Search"", ""system"", ""Search engine for searching documents""],
    [""Computer Vision Read API"", ""component"", ""Optical character recognition API""],
    [""Document Classifier Service"", ""component"", ""Service for classifying document types""],
    [""Indexing activity function"", ""component"", ""Creates search documents""],
    [""Metadata Store activity function"", ""component"", ""Saves document type information""],
    [""Durable Functions"", ""system"", ""Orchestrator for pipeline processing""],
    [""User"", ""person"", ""End user of the web app""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Document file""],
    [""Durable Functions"", ""calls"", ""Command message""],
    [""Command message"", ""called-from"", ""Durable Functions""],
    [""Scan activity function"", ""calls"", ""Computer Vision Read API""],
    [""Computer Vision Read API"", ""calls"", ""Document Classifier Service""],
    [""Document Classifier Service"", ""calls"", ""Azure Kubernetes Service""],
    [""Document Classifier Service"", ""returns"", ""Document type and page range""],
    [""Metadata Store activity function"", ""writes"", ""Document type and page range""],
    [""Indexing activity function"", ""writes"", ""Search document""],
    [""User"", ""searches"", ""Cognitive Search""],
    [""Search document"", ""has"", ""Correlation ID""],
    [""Correlation ID"", ""reads"", ""Azure Cosmos DB""]
  ]
}
```",4.82218452898087
27,gemma:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}","```json
{
""entities"": [
  [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
  [""Azure AI Speech"", ""system"", ""Speech recognition service""],
  [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
  [""Azure OpenAI"", ""system"", ""Transcript processing and analysis service""],
  [""Power BI"", ""component"", ""Data visualization tool""],
  [""App Service"", ""component"", ""Web application hosting service""],
  [""Customer"", ""person""],
  [""Agent"", ""person""]
],
""relationships"": [
  [""Azure Blob Storage"", ""contains"", ""Audio files""],
  [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
  [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
  [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
  [""Azure AI Speech"", ""part-of"", ""Azure OpenAI""],
  [""Azure AI Language"", ""part-of"", ""Azure OpenAI""],
  [""Azure OpenAI"", ""contains"", ""Transcript""],
  [""Azure OpenAI"", ""part-of"", ""Customer call analysis""],
  [""Azure OpenAI"", ""calls"", ""Power BI""],
  [""Power BI"", ""part-of"", ""Customer call analysis""],
  [""Customer call analysis"", ""part-of"", ""Customer relationship management""]
]
}
```","```json
{
""entities"": [
  [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
  [""Azure AI Speech"", ""system"", ""Speech recognition service""],
  [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
  [""Azure OpenAI"", ""system"", ""Transcript processing and analysis service""],
  [""Power BI"", ""component"", ""Data visualization tool""],
  [""App Service"", ""component"", ""Web application hosting service""],
  [""Customer"", ""person""],
  [""Agent"", ""person""]
],
""relationships"": [
  [""Azure Blob Storage"", ""contains"", ""Audio files""],
  [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
  [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
  [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
  [""Azure AI Speech"", ""part-of"", ""Azure OpenAI""],
  [""Azure AI Language"", ""part-of"", ""Azure OpenAI""],
  [""Azure OpenAI"", ""contains"", ""Transcript""],
  [""Azure OpenAI"", ""part-of"", ""Customer call analysis""],
  [""Azure OpenAI"", ""calls"", ""Power BI""],
  [""Power BI"", ""part-of"", ""Customer call analysis""],
  [""Customer call analysis"", ""part-of"", ""Customer relationship management""]
]
}
```",4.177202875027433
28,gemma:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Triggered by HTTP requests. Uses connectors to integrate with commonly used services.""],
    [""API Management"", ""system"", ""Consists of API gateway and Developer portal.""],
    [""API gateway"", ""component"", ""Accepts HTTP calls and routes them to the backend.""],
    [""Developer portal"", ""component"", ""Provides access to documentation and code samples for calling APIs.""],
    [""Azure DNS"", ""system"", ""Provides name resolution by using the Azure infrastructure.""],
    [""Microsoft Entra ID"", ""system"", ""Authenticates clients that call the API gateway""],
    [""Azure"", ""system"", ""Infrastructure for hosting domains and managing DNS records""],
    [""Contoso.com"", ""string"", ""Custom domain name""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""contains"", ""Workflows""],
    [""Azure Logic Apps"", ""calls"", ""Connectors""],
    [""API Management"", ""contains"", ""API gateway""],
    [""API Management"", ""contains"", ""Developer portal""],
    [""Azure DNS"", ""part-of"", ""Azure""],
    [""Microsoft Entra ID"", ""part-of"", ""API Management""],
    [""API Management"", ""calls"", ""Azure DNS""],
    [""API Management"", ""called-from"", ""Microsoft Entra ID""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Triggered by HTTP requests. Uses connectors to integrate with commonly used services.""],
    [""API Management"", ""system"", ""Consists of API gateway and Developer portal.""],
    [""API gateway"", ""component"", ""Accepts HTTP calls and routes them to the backend.""],
    [""Developer portal"", ""component"", ""Provides access to documentation and code samples for calling APIs.""],
    [""Azure DNS"", ""system"", ""Provides name resolution by using the Azure infrastructure.""],
    [""Microsoft Entra ID"", ""system"", ""Authenticates clients that call the API gateway""],
    [""Azure"", ""system"", ""Infrastructure for hosting domains and managing DNS records""],
    [""Contoso.com"", ""string"", ""Custom domain name""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""contains"", ""Workflows""],
    [""Azure Logic Apps"", ""calls"", ""Connectors""],
    [""API Management"", ""contains"", ""API gateway""],
    [""API Management"", ""contains"", ""Developer portal""],
    [""Azure DNS"", ""part-of"", ""Azure""],
    [""Microsoft Entra ID"", ""part-of"", ""API Management""],
    [""API Management"", ""calls"", ""Azure DNS""],
    [""API Management"", ""called-from"", ""Microsoft Entra ID""]
  ]
}
```",4.033761510043405
29,gemma:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM...""],
    [""Patient API"", ""component"", ""APIM calls the CreatePatient function in the Patient API...""],
    [""CreatePatient"", ""function"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function...""],
    [""Audit API"", ""component"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app...""],
    [""CreateAuditRecord"", ""function"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB...""],
    [""Azure Cosmos DB"", ""storage"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB...""],
    [""Patient document"", ""document"", ""The CreatePatient function creates the patient document in Azure Cosmos DB...""]
  ],
  ""relationships"": [
    [""APIM"", ""calls"", ""Patient API"", ""APIM calls the CreatePatient function in the Patient API...""],
    [""Patient API"", ""calls"", ""Audit API"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app...""],
    [""Audit API"", ""contains"", ""CreateAuditRecord"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB...""],
    [""CreateAuditRecord"", ""stores"", ""Azure Cosmos DB"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB...""],
    [""Patient API"", ""stores"", ""Patient document"", ""The CreatePatient function creates the patient document in Azure Cosmos DB...""],
    [""Patient API"", ""called-from"", ""APIM"", ""APIM calls the CreatePatient function in the Patient API...""]
  ]
}
```","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM...""],
    [""Patient API"", ""component"", ""APIM calls the CreatePatient function in the Patient API...""],
    [""CreatePatient"", ""function"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function...""],
    [""Audit API"", ""component"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app...""],
    [""CreateAuditRecord"", ""function"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB...""],
    [""Azure Cosmos DB"", ""storage"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB...""],
    [""Patient document"", ""document"", ""The CreatePatient function creates the patient document in Azure Cosmos DB...""]
  ],
  ""relationships"": [
    [""APIM"", ""calls"", ""Patient API"", ""APIM calls the CreatePatient function in the Patient API...""],
    [""Patient API"", ""calls"", ""Audit API"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app...""],
    [""Audit API"", ""contains"", ""CreateAuditRecord"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB...""],
    [""CreateAuditRecord"", ""stores"", ""Azure Cosmos DB"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB...""],
    [""Patient API"", ""stores"", ""Patient document"", ""The CreatePatient function creates the patient document in Azure Cosmos DB...""],
    [""Patient API"", ""called-from"", ""APIM"", ""APIM calls the CreatePatient function in the Patient API...""]
  ]
}
```",5.055708253988996
30,gemma:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline system that communicates with the API""],
    [""Custom API"", ""component"", ""Custom API hosted in Azure API Management""],
    [""API Coordinator"", ""person"", ""Custom API coordinator who receives notifications""],
    [""Power Apps"", ""system"", ""Platform for assigning flights to Teams channels""],
    [""Microsoft Teams"", ""system"", ""Platform for receiving flight updates""],
    [""Azure Storage Account"", ""component"", ""Storage for Graph API calls""],
    [""Azure Functions"", ""system"", ""Function that runs Graph API calls""],
    [""Azure Event Hubs"", ""system"", ""Event processing platform""],
    [""Azure Data Lake"", ""system"", ""Storage for long-term retention""],
    [""Power BI"", ""system"", ""Reporting platform""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API""],
    [""Custom API"", ""part-of"", ""Custom API Coordinator""],
    [""Custom API Coordinator"", ""calls"", ""Power Apps""],
    [""Custom API"", ""contains"", ""Azure Storage Account""],
    [""Azure Storage Account"", ""part-of"", ""Azure Functions""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Graph API"", ""part-of"", ""Azure Event Hubs""],
    [""Azure Event Hubs"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""contains"", ""Power BI""]
  ]
}
```","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline system that communicates with the API""],
    [""Custom API"", ""component"", ""Custom API hosted in Azure API Management""],
    [""API Coordinator"", ""person"", ""Custom API coordinator who receives notifications""],
    [""Power Apps"", ""system"", ""Platform for assigning flights to Teams channels""],
    [""Microsoft Teams"", ""system"", ""Platform for receiving flight updates""],
    [""Azure Storage Account"", ""component"", ""Storage for Graph API calls""],
    [""Azure Functions"", ""system"", ""Function that runs Graph API calls""],
    [""Azure Event Hubs"", ""system"", ""Event processing platform""],
    [""Azure Data Lake"", ""system"", ""Storage for long-term retention""],
    [""Power BI"", ""system"", ""Reporting platform""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API""],
    [""Custom API"", ""part-of"", ""Custom API Coordinator""],
    [""Custom API Coordinator"", ""calls"", ""Power Apps""],
    [""Custom API"", ""contains"", ""Azure Storage Account""],
    [""Azure Storage Account"", ""part-of"", ""Azure Functions""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Graph API"", ""part-of"", ""Azure Event Hubs""],
    [""Azure Event Hubs"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""contains"", ""Power BI""]
  ]
}
```",4.3859944739961065
31,gemma:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Data processing platform""],
    [""Microsoft-managed keys"", ""component"", ""Encryption key management""],
    [""Secure storage account"", ""component"", ""Storage account with security controls""],
    [""Streaming application"", ""component"", ""Application for secure access""],
    [""Data science VMs"", ""component"", ""Virtual machines for data preparation""],
    [""Azure Machine Learning compute"", ""component"", ""Machine learning processing environment""],
    [""Logic app"", ""component"", ""Automated notification system""],
    [""Data"", ""system"", ""Dataset being processed""],
    [""Models"", ""system"", ""Trained machine learning models""]
  ],
  ""relationships"": [
    [""contains"", ""Azure Data Factory"", ""Logic app""],
    [""part-of"", ""Secure storage account"", ""Azure Machine Learning compute""],
    [""calls"", ""Logic app"", ""Streaming application""],
    [""called-from"", ""Streaming application"", ""Logic app""],
    [""contains"", ""Secure storage account"", ""Data""],
    [""contains"", ""Secure storage account"", ""Models""],
    [""part-of"", ""Data science VMs"", ""Secure storage account""],
    [""part-of"", ""Azure Machine Learning compute"", ""Secure storage account""],
    [""calls"", ""Data Factory"", ""Azure Data Factory""],
    [""called-from"", ""Data Factory"", ""Azure Data Factory""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Data processing platform""],
    [""Microsoft-managed keys"", ""component"", ""Encryption key management""],
    [""Secure storage account"", ""component"", ""Storage account with security controls""],
    [""Streaming application"", ""component"", ""Application for secure access""],
    [""Data science VMs"", ""component"", ""Virtual machines for data preparation""],
    [""Azure Machine Learning compute"", ""component"", ""Machine learning processing environment""],
    [""Logic app"", ""component"", ""Automated notification system""],
    [""Data"", ""system"", ""Dataset being processed""],
    [""Models"", ""system"", ""Trained machine learning models""]
  ],
  ""relationships"": [
    [""contains"", ""Azure Data Factory"", ""Logic app""],
    [""part-of"", ""Secure storage account"", ""Azure Machine Learning compute""],
    [""calls"", ""Logic app"", ""Streaming application""],
    [""called-from"", ""Streaming application"", ""Logic app""],
    [""contains"", ""Secure storage account"", ""Data""],
    [""contains"", ""Secure storage account"", ""Models""],
    [""part-of"", ""Data science VMs"", ""Secure storage account""],
    [""part-of"", ""Azure Machine Learning compute"", ""Secure storage account""],
    [""calls"", ""Data Factory"", ""Azure Data Factory""],
    [""called-from"", ""Data Factory"", ""Azure Data Factory""]
  ]
}
```",4.414900225005113
32,gemma:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs from mainframe/midrange systems""],
    [""Publisher"", ""component"", ""Manage data migration for mainframe systems""],
    [""Listener"", ""component"", ""Manage data migration for midrange systems""],
    [""Enhanced-security connection"", ""system"", ""Provides secure connection""],
    [""Replicator Engine"", ""component"", ""Captures data from publisher/listener and applies to target""],
    [""Target"", ""system"", ""Database receiving changes""],
    [""Azure Databricks"", ""system"", ""Consume changed data""],
    [""Azure data platform services"", ""system"", ""Receive changes""],
    [""Connect Controller Daemon"", ""component"", ""Authenticate request and establish socket connection""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""part-of"", ""Publisher"", ""Enhanced-security connection""],
    [""part-of"", ""Listener"", ""Enhanced-security connection""],
    [""contains"", ""Enhanced-security connection"", ""Publisher""],
    [""contains"", ""Enhanced-security connection"", ""Listener""],
    [""calls"", ""Publisher/Listener"", ""Replicator Engine""],
    [""called-from"", ""Replicator Engine"", ""Publisher/Listener""],
    [""contains"", ""Replicator Engine"", ""Target""],
    [""contains"", ""Target"", ""Azure Databricks""],
    [""contains"", ""Target"", ""Azure data platform services""],
    [""calls"", ""Connect Controller Daemon"", ""Publisher/Listener""]
  ]
}
```","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs from mainframe/midrange systems""],
    [""Publisher"", ""component"", ""Manage data migration for mainframe systems""],
    [""Listener"", ""component"", ""Manage data migration for midrange systems""],
    [""Enhanced-security connection"", ""system"", ""Provides secure connection""],
    [""Replicator Engine"", ""component"", ""Captures data from publisher/listener and applies to target""],
    [""Target"", ""system"", ""Database receiving changes""],
    [""Azure Databricks"", ""system"", ""Consume changed data""],
    [""Azure data platform services"", ""system"", ""Receive changes""],
    [""Connect Controller Daemon"", ""component"", ""Authenticate request and establish socket connection""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""part-of"", ""Publisher"", ""Enhanced-security connection""],
    [""part-of"", ""Listener"", ""Enhanced-security connection""],
    [""contains"", ""Enhanced-security connection"", ""Publisher""],
    [""contains"", ""Enhanced-security connection"", ""Listener""],
    [""calls"", ""Publisher/Listener"", ""Replicator Engine""],
    [""called-from"", ""Replicator Engine"", ""Publisher/Listener""],
    [""contains"", ""Replicator Engine"", ""Target""],
    [""contains"", ""Target"", ""Azure Databricks""],
    [""contains"", ""Target"", ""Azure data platform services""],
    [""calls"", ""Connect Controller Daemon"", ""Publisher/Listener""]
  ]
}
```",4.606363369966857
33,gemma:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""GitHub repository""],
    [""Agent"", ""component"", ""CI/CD pipeline agent""],
    [""Azure VM"", ""component"", ""Azure virtual machine""],
    [""API application"", ""component"", ""API application""],
    [""ILB ASE"", ""system"", ""ILB Application Service Environment""],
    [""API Management"", ""system"", ""Azure API Management""],
    [""Application Gateway"", ""component"", ""Application Gateway""],
    [""Azure Private DNS"", ""component"", ""Azure Private DNS""],
    [""Users"", ""person"", ""External users""]
  ],
  ""relationships"": [
    [""GitHub"", ""contains"", ""Agent""],
    [""Agent"", ""part-of"", ""Azure VM""],
    [""Azure VM"", ""contains"", ""API application""],
    [""API application"", ""part-of"", ""ILB ASE""],
    [""API Management"", ""calls"", ""API application""],
    [""API Management"", ""part-of"", ""ILB ASE""],
    [""Application Gateway"", ""part-of"", ""API Management""],
    [""Azure Private DNS"", ""contains"", ""ILB ASE""],
    [""Application Gateway"", ""calls"", ""API Management""],
    [""Users"", ""calls"", ""Application Gateway""]
  ]
}
```","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""GitHub repository""],
    [""Agent"", ""component"", ""CI/CD pipeline agent""],
    [""Azure VM"", ""component"", ""Azure virtual machine""],
    [""API application"", ""component"", ""API application""],
    [""ILB ASE"", ""system"", ""ILB Application Service Environment""],
    [""API Management"", ""system"", ""Azure API Management""],
    [""Application Gateway"", ""component"", ""Application Gateway""],
    [""Azure Private DNS"", ""component"", ""Azure Private DNS""],
    [""Users"", ""person"", ""External users""]
  ],
  ""relationships"": [
    [""GitHub"", ""contains"", ""Agent""],
    [""Agent"", ""part-of"", ""Azure VM""],
    [""Azure VM"", ""contains"", ""API application""],
    [""API application"", ""part-of"", ""ILB ASE""],
    [""API Management"", ""calls"", ""API application""],
    [""API Management"", ""part-of"", ""ILB ASE""],
    [""Application Gateway"", ""part-of"", ""API Management""],
    [""Azure Private DNS"", ""contains"", ""ILB ASE""],
    [""Application Gateway"", ""calls"", ""API Management""],
    [""Users"", ""calls"", ""Application Gateway""]
  ]
}
```",3.915811365994159
34,gemma:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity Provider""],
    [""Azure Front Door"", ""component"", ""Gateway""],
    [""API Management"", ""component"", ""API Gateway""],
    [""App Service Environment"", ""component"", ""Environment""],
    [""Function Apps"", ""component"", ""Environment""],
    [""AKS"", ""component"", ""Environment""]
  ],
  ""relationships"": [
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""calls"", ""API Management""],
    [""API Management"", ""contains"", ""Validate JWT Policy""],
    [""API Management"", ""contains"", ""Backend API""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""Backend API"", ""part-of"", ""App Service Environment""],
    [""Backend API"", ""part-of"", ""Function Apps""],
    [""Backend API"", ""part-of"", ""AKS""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity Provider""],
    [""Azure Front Door"", ""component"", ""Gateway""],
    [""API Management"", ""component"", ""API Gateway""],
    [""App Service Environment"", ""component"", ""Environment""],
    [""Function Apps"", ""component"", ""Environment""],
    [""AKS"", ""component"", ""Environment""]
  ],
  ""relationships"": [
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""calls"", ""API Management""],
    [""API Management"", ""contains"", ""Validate JWT Policy""],
    [""API Management"", ""contains"", ""Backend API""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""Backend API"", ""part-of"", ""App Service Environment""],
    [""Backend API"", ""part-of"", ""Function Apps""],
    [""Backend API"", ""part-of"", ""AKS""]
  ]
}
```",3.407396814029198
35,gemma:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API Management accepts API calls in the form of HTTP requests.""],
    [""Logic Apps"", ""component"", ""API Management securely routes the HTTP requests to Logic Apps.""],
    [""Azure Key Vault"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""On-premises data gateway"", ""component"", ""Logic Apps uses TLS to send the database credentials and a database statement to the on-premises data gateway.""],
    [""SQL Server"", ""component"", ""The on-premises data gateway connects to a SQL Server database to run the statement.""]
  ],
  ""relationships"": [
    [""API Management"", ""contains"", ""Logic Apps""],
    [""Logic Apps"", ""contains"", ""Azure Key Vault""],
    [""Logic Apps"", ""contains"", ""On-premises data gateway""],
    [""Logic Apps"", ""calls"", ""SQL Server""],
    [""SQL Server"", ""part-of"", ""Logic Apps""]
  ]
}
```","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API Management accepts API calls in the form of HTTP requests.""],
    [""Logic Apps"", ""component"", ""API Management securely routes the HTTP requests to Logic Apps.""],
    [""Azure Key Vault"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""On-premises data gateway"", ""component"", ""Logic Apps uses TLS to send the database credentials and a database statement to the on-premises data gateway.""],
    [""SQL Server"", ""component"", ""The on-premises data gateway connects to a SQL Server database to run the statement.""]
  ],
  ""relationships"": [
    [""API Management"", ""contains"", ""Logic Apps""],
    [""Logic Apps"", ""contains"", ""Azure Key Vault""],
    [""Logic Apps"", ""contains"", ""On-premises data gateway""],
    [""Logic Apps"", ""calls"", ""SQL Server""],
    [""SQL Server"", ""part-of"", ""Logic Apps""]
  ]
}
```",3.2056098760222085
36,gemma:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API management platform""],
    [""Patient API"", ""component"", ""API for managing patient data""],
    [""Audit API"", ""component"", ""API for creating audit records""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database""],
    [""CreatePatient"", ""function"", ""Function for creating patient data""],
    [""CreateAuditRecord"", ""function"", ""Function for creating audit records""]
  ],
  ""relationships"": [
    [""APIM"", ""calls"", ""Patient API""],
    [""Patient API"", ""calls"", ""CreateAuditRecord""],
    [""CreateAuditRecord"", ""part-of"", ""Audit API""],
    [""CreatePatient"", ""part-of"", ""Patient API""],
    [""Patient API"", ""contains"", ""CreatePatient""],
    [""Audit API"", ""contains"", ""CreateAuditRecord""]
  ]
}
```","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API management platform""],
    [""Patient API"", ""component"", ""API for managing patient data""],
    [""Audit API"", ""component"", ""API for creating audit records""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database""],
    [""CreatePatient"", ""function"", ""Function for creating patient data""],
    [""CreateAuditRecord"", ""function"", ""Function for creating audit records""]
  ],
  ""relationships"": [
    [""APIM"", ""calls"", ""Patient API""],
    [""Patient API"", ""calls"", ""CreateAuditRecord""],
    [""CreateAuditRecord"", ""part-of"", ""Audit API""],
    [""CreatePatient"", ""part-of"", ""Patient API""],
    [""Patient API"", ""contains"", ""CreatePatient""],
    [""Audit API"", ""contains"", ""CreateAuditRecord""]
  ]
}
```",2.899169587995857
37,gemma:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Mainframe"", ""system""],
    [""Midrange systems"", ""system""],
    [""Publisher"", ""component""],
    [""Listener"", ""component""],
    [""Enhanced-security connection"", ""system""],
    [""Replicator Engine"", ""component""],
    [""Target"", ""system""],
    [""Azure Databricks"", ""system""],
    [""Azure data platform services"", ""system""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""part-of"", ""Mainframe"", ""Publisher""],
    [""part-of"", ""Midrange systems"", ""Listener""],
    [""calls"", ""Publisher or Listener"", ""Replicator Engine""],
    [""contains"", ""Replicator Engine"", ""Target""],
    [""calls"", ""Replicator Engine"", ""Azure Databricks""],
    [""calls"", ""Azure Databricks"", ""Azure data platform services""]
  ]
}
```","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Mainframe"", ""system""],
    [""Midrange systems"", ""system""],
    [""Publisher"", ""component""],
    [""Listener"", ""component""],
    [""Enhanced-security connection"", ""system""],
    [""Replicator Engine"", ""component""],
    [""Target"", ""system""],
    [""Azure Databricks"", ""system""],
    [""Azure data platform services"", ""system""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""part-of"", ""Mainframe"", ""Publisher""],
    [""part-of"", ""Midrange systems"", ""Listener""],
    [""calls"", ""Publisher or Listener"", ""Replicator Engine""],
    [""contains"", ""Replicator Engine"", ""Target""],
    [""calls"", ""Replicator Engine"", ""Azure Databricks""],
    [""calls"", ""Azure Databricks"", ""Azure data platform services""]
  ]
}
```",3.1075317229842767
38,gemma:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Custom API"", ""component"", ""Custom API hosted in Azure API Management""],
    [""API Coordinator"", ""person""],
    [""Power Apps"", ""system""],
    [""Microsoft Teams"", ""system""],
    [""Graph API"", ""component""],
    [""Azure Storage Account"", ""system""],
    [""Azure Functions"", ""system""],
    [""Azure Event Hubs"", ""system""],
    [""Azure Data Lake"", ""system""],
    [""Power BI"", ""system""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API""],
    [""Custom API"", ""contains"", ""Custom API Coordinator""],
    [""Custom API"", ""calls"", ""Power Apps""],
    [""Custom API"", ""contains"", ""Graph API""],
    [""Custom API"", ""calls"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Custom Bot"", ""calls"", ""Custom API""],
    [""Custom API"", ""contains"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""calls"", ""Power BI""]
  ]
}
```","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Custom API"", ""component"", ""Custom API hosted in Azure API Management""],
    [""API Coordinator"", ""person""],
    [""Power Apps"", ""system""],
    [""Microsoft Teams"", ""system""],
    [""Graph API"", ""component""],
    [""Azure Storage Account"", ""system""],
    [""Azure Functions"", ""system""],
    [""Azure Event Hubs"", ""system""],
    [""Azure Data Lake"", ""system""],
    [""Power BI"", ""system""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API""],
    [""Custom API"", ""contains"", ""Custom API Coordinator""],
    [""Custom API"", ""calls"", ""Power Apps""],
    [""Custom API"", ""contains"", ""Graph API""],
    [""Custom API"", ""calls"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Custom Bot"", ""calls"", ""Custom API""],
    [""Custom API"", ""contains"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""calls"", ""Power BI""]
  ]
}
```",3.6303781640017405
39,gemma:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity provider configured with a custom domain for sign-in""],
    [""Azure Front Door"", ""component"", ""Gateway that distributes traffic to backend services""],
    [""API Management"", ""system"", ""Platform for managing and securing APIs""],
    [""App Service Environment"", ""component"", ""Environment for deploying and scaling cloud-based apps""],
    [""Function Apps"", ""component"", ""Environment for deploying stateless, short-lived functions""],
    [""AKS"", ""component"", ""Environment for deploying containerized applications""],
    [""JWT"", ""component"", ""JSON Web Token containing authentication information""],
    [""Client"", ""person"", ""Application or user that interacts with the system""],
    [""Backend API"", ""component"", ""Set of APIs running in different environments""]
  ],
  ""relationships"": [
    [""Client"", ""calls"", ""Azure Front Door""],
    [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""contains"", ""Backend API""],
    [""Backend API"", ""part-of"", ""App Service Environment""],
    [""Backend API"", ""part-of"", ""Function Apps""],
    [""Backend API"", ""part-of"", ""AKS""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity provider configured with a custom domain for sign-in""],
    [""Azure Front Door"", ""component"", ""Gateway that distributes traffic to backend services""],
    [""API Management"", ""system"", ""Platform for managing and securing APIs""],
    [""App Service Environment"", ""component"", ""Environment for deploying and scaling cloud-based apps""],
    [""Function Apps"", ""component"", ""Environment for deploying stateless, short-lived functions""],
    [""AKS"", ""component"", ""Environment for deploying containerized applications""],
    [""JWT"", ""component"", ""JSON Web Token containing authentication information""],
    [""Client"", ""person"", ""Application or user that interacts with the system""],
    [""Backend API"", ""component"", ""Set of APIs running in different environments""]
  ],
  ""relationships"": [
    [""Client"", ""calls"", ""Azure Front Door""],
    [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""contains"", ""Backend API""],
    [""Backend API"", ""part-of"", ""App Service Environment""],
    [""Backend API"", ""part-of"", ""Function Apps""],
    [""Backend API"", ""part-of"", ""AKS""]
  ]
}
```",4.472364004002884
40,gemma:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform triggered by HTTP requests""],
    [""Azure API Management"", ""system"", ""API gateway and developer portal""],
    [""Azure DNS"", ""system"", ""Domain name resolution service""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service""],
    [""API gateway"", ""component"", ""Accepts HTTP calls""],
    [""Developer portal"", ""component"", ""Access to API documentation and code samples""],
    [""Azure"", ""system""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""contains"", ""Workflows""],
    [""Azure Logic Apps"", ""calls"", ""Connectors""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure API Management"", ""part-of"", ""Azure""],
    [""Azure DNS"", ""part-of"", ""Azure""],
    [""Microsoft Entra ID"", ""part-of"", ""Azure""],
    [""API gateway"", ""calls"", ""Microsoft Entra ID""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform triggered by HTTP requests""],
    [""Azure API Management"", ""system"", ""API gateway and developer portal""],
    [""Azure DNS"", ""system"", ""Domain name resolution service""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service""],
    [""API gateway"", ""component"", ""Accepts HTTP calls""],
    [""Developer portal"", ""component"", ""Access to API documentation and code samples""],
    [""Azure"", ""system""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""contains"", ""Workflows""],
    [""Azure Logic Apps"", ""calls"", ""Connectors""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure API Management"", ""part-of"", ""Azure""],
    [""Azure DNS"", ""part-of"", ""Azure""],
    [""Microsoft Entra ID"", ""part-of"", ""Azure""],
    [""API gateway"", ""calls"", ""Microsoft Entra ID""]
  ]
}
```",3.3915362829575315
41,gemma:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
    [""Command Message"", ""system"", ""Message containing metadata""],
    [""Durable Functions"", ""system"", ""Orchestration framework""],
    [""Computer Vision Read API"", ""component"", ""Optical character recognition API""],
    [""Document Classifier Service"", ""component"", ""Service for classifying document types""],
    [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database""],
    [""Cognitive Search"", ""system"", ""Semantic search engine""],
    [""Correlation ID"", ""system"", ""Unique identifier""]
  ],
  ""relationships"": [
    [""Command Message"", ""calls"", ""Durable Functions""],
    [""Durable Functions"", ""contains"", ""Scan activity function""],
    [""Scan activity function"", ""calls"", ""Computer Vision Read API""],
    [""Durable Functions"", ""contains"", ""Classify activity function""],
    [""Classify activity function"", ""calls"", ""Document Classifier Service""],
    [""Durable Functions"", ""contains"", ""Metadata Store activity function""],
    [""Durable Functions"", ""contains"", ""Indexing activity function""],
    [""Classify activity function"", ""returns"", ""Document type and page range information""],
    [""Metadata Store activity function"", ""stores"", ""Document type and page range information""],
    [""Indexing activity function"", ""creates"", ""Search document""],
    [""Indexing activity function"", ""includes"", ""OCR results""],
    [""Indexing activity function"", ""includes"", ""Document information""],
    [""Indexing activity function"", ""adds"", ""Correlation ID""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
    [""Command Message"", ""system"", ""Message containing metadata""],
    [""Durable Functions"", ""system"", ""Orchestration framework""],
    [""Computer Vision Read API"", ""component"", ""Optical character recognition API""],
    [""Document Classifier Service"", ""component"", ""Service for classifying document types""],
    [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database""],
    [""Cognitive Search"", ""system"", ""Semantic search engine""],
    [""Correlation ID"", ""system"", ""Unique identifier""]
  ],
  ""relationships"": [
    [""Command Message"", ""calls"", ""Durable Functions""],
    [""Durable Functions"", ""contains"", ""Scan activity function""],
    [""Scan activity function"", ""calls"", ""Computer Vision Read API""],
    [""Durable Functions"", ""contains"", ""Classify activity function""],
    [""Classify activity function"", ""calls"", ""Document Classifier Service""],
    [""Durable Functions"", ""contains"", ""Metadata Store activity function""],
    [""Durable Functions"", ""contains"", ""Indexing activity function""],
    [""Classify activity function"", ""returns"", ""Document type and page range information""],
    [""Metadata Store activity function"", ""stores"", ""Document type and page range information""],
    [""Indexing activity function"", ""creates"", ""Search document""],
    [""Indexing activity function"", ""includes"", ""OCR results""],
    [""Indexing activity function"", ""includes"", ""Document information""],
    [""Indexing activity function"", ""adds"", ""Correlation ID""]
  ]
}
```",4.802291013998911
42,gemma:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""Version control platform""],
    [""CI/CD pipeline"", ""system"", ""Continuous integration and continuous delivery pipeline""],
    [""Azure VM"", ""component"", ""Virtual machine""],
    [""API application"", ""component"", ""Application""],
    [""ILB ASE"", ""system"", ""Azure Service Environment""],
    [""API Management"", ""component"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Ingress controller""],
    [""Azure Private DNS"", ""component"", ""DNS service""],
    [""External users"", ""person"", ""Users""],
  ],
  ""relationships"": [
    [""developers"", ""calls"", ""GitHub"", ""Checks code in""],
    [""GitHub"", ""contains"", ""CI/CD pipeline""],
    [""CI/CD pipeline"", ""part-of"", ""Azure VM""],
    [""CI/CD pipeline"", ""calls"", ""API application""],
    [""API Management"", ""contains"", ""HOST header""],
    [""API Management"", ""part-of"", ""ILB ASE""],
    [""Application Gateway"", ""contains"", ""API Management""],
    [""API Management"", ""calls"", ""Azure Private DNS""],
    [""Azure Private DNS"", ""part-of"", ""ASE""],
    [""ASE"", ""calls"", ""API Management""],
    [""API Management"", ""calls"", ""Application Gateway""],
    [""Application Gateway"", ""calls"", ""External users""]
  ]
}
```","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""Version control platform""],
    [""CI/CD pipeline"", ""system"", ""Continuous integration and continuous delivery pipeline""],
    [""Azure VM"", ""component"", ""Virtual machine""],
    [""API application"", ""component"", ""Application""],
    [""ILB ASE"", ""system"", ""Azure Service Environment""],
    [""API Management"", ""component"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Ingress controller""],
    [""Azure Private DNS"", ""component"", ""DNS service""],
    [""External users"", ""person"", ""Users""],
  ],
  ""relationships"": [
    [""developers"", ""calls"", ""GitHub"", ""Checks code in""],
    [""GitHub"", ""contains"", ""CI/CD pipeline""],
    [""CI/CD pipeline"", ""part-of"", ""Azure VM""],
    [""CI/CD pipeline"", ""calls"", ""API application""],
    [""API Management"", ""contains"", ""HOST header""],
    [""API Management"", ""part-of"", ""ILB ASE""],
    [""Application Gateway"", ""contains"", ""API Management""],
    [""API Management"", ""calls"", ""Azure Private DNS""],
    [""Azure Private DNS"", ""part-of"", ""ASE""],
    [""ASE"", ""calls"", ""API Management""],
    [""API Management"", ""calls"", ""Application Gateway""],
    [""Application Gateway"", ""calls"", ""External users""]
  ]
}
```",4.204674469016027
43,gemma:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API management system that accepts HTTP requests""],
    [""Logic Apps"", ""component"", ""Cloud-based workflow service""],
    [""Azure Key Vault"", ""system"", ""Cloud-based service for securely storing and retrieving secrets""],
    [""Transport Layer Security"", ""system"", ""Security protocol for encrypting data""],
    [""Data Gateway"", ""component"", ""Software that connects Logic Apps to on-premises databases""],
    [""SQL Server"", ""system"", ""Relational database management system""],
    [""Azure Monitor"", ""system"", ""Service for collecting and analyzing data""]
  ],
  ""relationships"": [
    [""API Management"", ""calls"", ""Logic Apps"", ""API Management initiates workflows in Logic Apps""],
    [""Logic Apps"", ""contains"", ""Azure Key Vault"", ""Logic Apps retrieves database credentials from Azure Key Vault""],
    [""Logic Apps"", ""contains"", ""Transport Layer Security"", ""Logic Apps uses TLS for secure communication""],
    [""Logic Apps"", ""contains"", ""Data Gateway"", ""Logic Apps connects to Data Gateway using the Data Gateway component""],
    [""Data Gateway"", ""part-of"", ""SQL Server"", ""Data Gateway connects to SQL Server""],
    [""Logic Apps"", ""calls"", ""Azure Monitor"", ""Logic Apps events are monitored by Azure Monitor""]
  ]
}
```","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API management system that accepts HTTP requests""],
    [""Logic Apps"", ""component"", ""Cloud-based workflow service""],
    [""Azure Key Vault"", ""system"", ""Cloud-based service for securely storing and retrieving secrets""],
    [""Transport Layer Security"", ""system"", ""Security protocol for encrypting data""],
    [""Data Gateway"", ""component"", ""Software that connects Logic Apps to on-premises databases""],
    [""SQL Server"", ""system"", ""Relational database management system""],
    [""Azure Monitor"", ""system"", ""Service for collecting and analyzing data""]
  ],
  ""relationships"": [
    [""API Management"", ""calls"", ""Logic Apps"", ""API Management initiates workflows in Logic Apps""],
    [""Logic Apps"", ""contains"", ""Azure Key Vault"", ""Logic Apps retrieves database credentials from Azure Key Vault""],
    [""Logic Apps"", ""contains"", ""Transport Layer Security"", ""Logic Apps uses TLS for secure communication""],
    [""Logic Apps"", ""contains"", ""Data Gateway"", ""Logic Apps connects to Data Gateway using the Data Gateway component""],
    [""Data Gateway"", ""part-of"", ""SQL Server"", ""Data Gateway connects to SQL Server""],
    [""Logic Apps"", ""calls"", ""Azure Monitor"", ""Logic Apps events are monitored by Azure Monitor""]
  ]
}
```",3.7744892140035518
44,gemma:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system containing supplier data""],
    [""Data Factory"", ""component"", ""Data pipeline from CDS to SQL""],
    [""ERP system"", ""system"", ""System containing purchase order data""],
    [""Azure SQL Database"", ""system"", ""Database storing purchase order data""],
    [""Azure Functions"", ""component"", ""API for surfacing purchase order data""],
    [""Power Apps"", ""component"", ""Application for reviewing and updating purchase orders""],
    [""Suppliers"", ""system"", ""External entities receiving purchase order updates""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""ERP system"", ""part-of"", ""Azure SQL Database""],
    [""Azure Functions"", ""calls"", ""API""],
    [""Azure SQL Database"", ""contains"", ""Purchase order data""],
    [""Power Apps"", ""calls"", ""Azure SQL Database""],
    [""Power Apps"", ""contains"", ""Purchase order data""],
    [""Power Apps"", ""calls"", ""Suppliers""]
  ]
}
```","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system containing supplier data""],
    [""Data Factory"", ""component"", ""Data pipeline from CDS to SQL""],
    [""ERP system"", ""system"", ""System containing purchase order data""],
    [""Azure SQL Database"", ""system"", ""Database storing purchase order data""],
    [""Azure Functions"", ""component"", ""API for surfacing purchase order data""],
    [""Power Apps"", ""component"", ""Application for reviewing and updating purchase orders""],
    [""Suppliers"", ""system"", ""External entities receiving purchase order updates""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""ERP system"", ""part-of"", ""Azure SQL Database""],
    [""Azure Functions"", ""calls"", ""API""],
    [""Azure SQL Database"", ""contains"", ""Purchase order data""],
    [""Power Apps"", ""calls"", ""Azure SQL Database""],
    [""Power Apps"", ""contains"", ""Purchase order data""],
    [""Power Apps"", ""calls"", ""Suppliers""]
  ]
}
```",3.150828272977378
45,gemma:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF attachments""],
    [""Logic app"", ""component"", ""Logic app for processing email attachments""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud-based storage platform for storing data""],
    [""PDF container"", ""component"", ""Container for storing PDF files""],
    [""Function app"", ""component"", ""Function app for processing PDF forms""],
    [""Azure AI Document Intelligence"", ""system"", ""Azure service for document analysis""],
    [""Cosmos DB"", ""system"", ""NoSQL database for storing data""],
    [""Power BI"", ""component"", ""Business intelligence and data analytics platform""]
  ],
  ""relationships"": [
    [""contains"", ""Logic app"", ""PDF container""],
    [""contains"", ""PDF container"", ""PDF files""],
    [""calls"", ""PDF container"", ""Logic app""],
    [""called-from"", ""Logic app"", ""Function app""],
    [""contains"", ""Function app"", ""Single-page PDF files""],
    [""calls"", ""Function app"", ""Azure AI Document Intelligence""],
    [""contains"", ""Function app"", ""Processed response data""],
    [""contains"", ""Function app"", ""JSON data file""],
    [""contains"", ""Cosmos DB"", ""Processed data""],
    [""calls"", ""Forms processing logic app"", ""Cosmos DB""]
  ]
}
```","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF attachments""],
    [""Logic app"", ""component"", ""Logic app for processing email attachments""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud-based storage platform for storing data""],
    [""PDF container"", ""component"", ""Container for storing PDF files""],
    [""Function app"", ""component"", ""Function app for processing PDF forms""],
    [""Azure AI Document Intelligence"", ""system"", ""Azure service for document analysis""],
    [""Cosmos DB"", ""system"", ""NoSQL database for storing data""],
    [""Power BI"", ""component"", ""Business intelligence and data analytics platform""]
  ],
  ""relationships"": [
    [""contains"", ""Logic app"", ""PDF container""],
    [""contains"", ""PDF container"", ""PDF files""],
    [""calls"", ""PDF container"", ""Logic app""],
    [""called-from"", ""Logic app"", ""Function app""],
    [""contains"", ""Function app"", ""Single-page PDF files""],
    [""calls"", ""Function app"", ""Azure AI Document Intelligence""],
    [""contains"", ""Function app"", ""Processed response data""],
    [""contains"", ""Function app"", ""JSON data file""],
    [""contains"", ""Cosmos DB"", ""Processed data""],
    [""calls"", ""Forms processing logic app"", ""Cosmos DB""]
  ]
}
```",4.277275622996967
46,gemma:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Data pipeline platform""],
    [""Blob storage"", ""component"", ""Data storage""],
    [""Microsoft-managed keys"", ""component"", ""Encryption key management""],
    [""Azure Virtual Desktop"", ""component"", ""Remote access""],
    [""Data science VMs"", ""component"", ""Compute environment""],
    [""Azure Machine Learning"", ""system"", ""Machine learning platform""],
    [""Logic app"", ""component"", ""Workflow engine""],
    [""Data owners"", ""person""],
    [""Researchers"", ""person""],
    [""Data"", ""component"", ""Dataset""],
    [""Models"", ""component"", ""Machine learning models""]
  ],
  ""relationships"": [
    [""contains"", ""Azure Data Factory"", ""Blob storage""],
    [""contains"", ""Azure Data Factory"", ""Microsoft-managed keys""],
    [""part-of"", ""Azure Virtual Desktop"", ""Secure environment""],
    [""part-of"", ""Data science VMs"", ""Secure environment""],
    [""part-of"", ""Azure Machine Learning"", ""Secure environment""],
    [""calls"", ""Data Factory"", ""Azure Data Factory trigger""],
    [""calls"", ""Logic app"", ""Data exfiltration trigger""],
    [""is called from"", ""Logic app"", ""Data exfiltration process""],
    [""contains"", ""Secure environment"", ""Data science VMs""],
    [""contains"", ""Secure environment"", ""Azure Machine Learning""],
    [""contains"", ""Secure storage account"", ""Dataset""],
    [""contains"", ""Secure storage account"", ""Models""],
    [""calls"", ""Data exfiltration process"", ""Logic app""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Data pipeline platform""],
    [""Blob storage"", ""component"", ""Data storage""],
    [""Microsoft-managed keys"", ""component"", ""Encryption key management""],
    [""Azure Virtual Desktop"", ""component"", ""Remote access""],
    [""Data science VMs"", ""component"", ""Compute environment""],
    [""Azure Machine Learning"", ""system"", ""Machine learning platform""],
    [""Logic app"", ""component"", ""Workflow engine""],
    [""Data owners"", ""person""],
    [""Researchers"", ""person""],
    [""Data"", ""component"", ""Dataset""],
    [""Models"", ""component"", ""Machine learning models""]
  ],
  ""relationships"": [
    [""contains"", ""Azure Data Factory"", ""Blob storage""],
    [""contains"", ""Azure Data Factory"", ""Microsoft-managed keys""],
    [""part-of"", ""Azure Virtual Desktop"", ""Secure environment""],
    [""part-of"", ""Data science VMs"", ""Secure environment""],
    [""part-of"", ""Azure Machine Learning"", ""Secure environment""],
    [""calls"", ""Data Factory"", ""Azure Data Factory trigger""],
    [""calls"", ""Logic app"", ""Data exfiltration trigger""],
    [""is called from"", ""Logic app"", ""Data exfiltration process""],
    [""contains"", ""Secure environment"", ""Data science VMs""],
    [""contains"", ""Secure environment"", ""Azure Machine Learning""],
    [""contains"", ""Secure storage account"", ""Dataset""],
    [""contains"", ""Secure storage account"", ""Models""],
    [""calls"", ""Data exfiltration process"", ""Logic app""]
  ]
}
```",4.9777061569620855
47,gemma:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech transcription service""],
    [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
    [""Azure OpenAI"", ""system"", ""Transcript processing and analysis service""],
    [""Ingestion Client tool"", ""component"", ""Tool for call center transcription""],
    [""Power BI"", ""component"", ""Data visualization tool""],
    [""App Service"", ""component"", ""Web application hosting service""],
    [""CRM"", ""component"", ""Customer relationship management system""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
    [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
    [""Azure AI Speech"", ""part-of"", ""Azure AI Language""],
    [""Ingestion Client tool"", ""part-of"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""part-of"", ""Azure AI Speech""],
    [""Azure AI Speech"", ""contains"", ""Transcription results""],
    [""Azure AI Language"", ""contains"", ""Personal data""],
    [""Azure OpenAI"", ""contains"", ""Processed output""],
    [""Azure OpenAI"", ""part-of"", ""Azure AI Speech""],
    [""Power BI"", ""calls"", ""Azure OpenAI""],
    [""App Service"", ""calls"", ""Azure OpenAI""],
    [""Azure OpenAI"", ""contains"", ""Entities""],
    [""Azure OpenAI"", ""contains"", ""Summary""],
    [""Azure OpenAI"", ""contains"", ""Sentiment analysis""],
    [""Azure OpenAI"", ""part-of"", ""Data storage""],
    [""Data storage"", ""part-of"", ""CRM""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech transcription service""],
    [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
    [""Azure OpenAI"", ""system"", ""Transcript processing and analysis service""],
    [""Ingestion Client tool"", ""component"", ""Tool for call center transcription""],
    [""Power BI"", ""component"", ""Data visualization tool""],
    [""App Service"", ""component"", ""Web application hosting service""],
    [""CRM"", ""component"", ""Customer relationship management system""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
    [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
    [""Azure AI Speech"", ""part-of"", ""Azure AI Language""],
    [""Ingestion Client tool"", ""part-of"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""part-of"", ""Azure AI Speech""],
    [""Azure AI Speech"", ""contains"", ""Transcription results""],
    [""Azure AI Language"", ""contains"", ""Personal data""],
    [""Azure OpenAI"", ""contains"", ""Processed output""],
    [""Azure OpenAI"", ""part-of"", ""Azure AI Speech""],
    [""Power BI"", ""calls"", ""Azure OpenAI""],
    [""App Service"", ""calls"", ""Azure OpenAI""],
    [""Azure OpenAI"", ""contains"", ""Entities""],
    [""Azure OpenAI"", ""contains"", ""Summary""],
    [""Azure OpenAI"", ""contains"", ""Sentiment analysis""],
    [""Azure OpenAI"", ""part-of"", ""Data storage""],
    [""Data storage"", ""part-of"", ""CRM""]
  ]
}
```",5.619319251039997
48,gemma:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Custom API"", ""component"", ""API hosted in Azure API Management""],
    [""API Coordinator"", ""component"", ""Custom API coordinator""],
    [""Power Apps"", ""system""],
    [""Microsoft Teams"", ""system""],
    [""Graph API"", ""component""],
    [""Azure Storage Account"", ""system""],
    [""Azure Functions"", ""system""],
    [""Azure Bot Service"", ""system""],
    [""Azure Data Lake"", ""system""],
    [""Power BI"", ""system""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API""],
    [""Custom API"", ""part-of"", ""Custom API Coordinator""],
    [""Custom API Coordinator"", ""calls"", ""Power Apps""],
    [""Custom API Coordinator"", ""part-of"", ""Airline System""],
    [""Power Apps"", ""part-of"", ""Microsoft Teams""],
    [""Custom API"", ""part-of"", ""Graph API""],
    [""Graph API"", ""part-of"", ""Azure Storage Account""],
    [""Azure Storage Account"", ""part-of"", ""Azure Functions""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Custom Bot Service"", ""part-of"", ""Airline System""],
    [""Custom Bot Service"", ""calls"", ""Custom API""],
    [""Azure Data Lake"", ""part-of"", ""Event Hubs""],
    [""Event Hubs"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""calls"", ""Power BI""]
  ]
}
```","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Custom API"", ""component"", ""API hosted in Azure API Management""],
    [""API Coordinator"", ""component"", ""Custom API coordinator""],
    [""Power Apps"", ""system""],
    [""Microsoft Teams"", ""system""],
    [""Graph API"", ""component""],
    [""Azure Storage Account"", ""system""],
    [""Azure Functions"", ""system""],
    [""Azure Bot Service"", ""system""],
    [""Azure Data Lake"", ""system""],
    [""Power BI"", ""system""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API""],
    [""Custom API"", ""part-of"", ""Custom API Coordinator""],
    [""Custom API Coordinator"", ""calls"", ""Power Apps""],
    [""Custom API Coordinator"", ""part-of"", ""Airline System""],
    [""Power Apps"", ""part-of"", ""Microsoft Teams""],
    [""Custom API"", ""part-of"", ""Graph API""],
    [""Graph API"", ""part-of"", ""Azure Storage Account""],
    [""Azure Storage Account"", ""part-of"", ""Azure Functions""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Custom Bot Service"", ""part-of"", ""Airline System""],
    [""Custom Bot Service"", ""calls"", ""Custom API""],
    [""Azure Data Lake"", ""part-of"", ""Event Hubs""],
    [""Event Hubs"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""calls"", ""Power BI""]
  ]
}
```",4.698885371966753
49,gemma:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF attachments""],
    [""Logic App"", ""component"", ""Logic app for processing email attachments""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud-based storage for PDF files""],
    [""Function App"", ""component"", ""Function app for processing PDF forms""],
    [""AI Document Intelligence"", ""system"", ""Azure AI service for document processing""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database for storing processed data""],
    [""Power BI"", ""component"", ""Business intelligence tool for data analysis""]
  ],
  ""relationships"": [
    [""contains"", ""Logic App"", ""Azure Data Lake Storage""],
    [""contains"", ""PDF container"", ""Azure Data Lake Storage""],
    [""calls"", ""Logic App"", ""Function App""],
    [""called-from"", ""Function App"", ""AI Document Intelligence""],
    [""contains"", ""Function App"", ""Second PDF container""],
    [""contains"", ""Function App"", ""Data structure""],
    [""contains"", ""Function App"", ""JSON data file""],
    [""part-of"", ""Azure Data Lake Storage"", ""PDF container""],
    [""part-of"", ""Azure Data Lake Storage"", ""Second PDF container""],
    [""part-of"", ""Azure Data Lake Storage"", ""Third PDF container""],
    [""calls"", ""Forms processing logic app"", ""Azure Cosmos DB""],
    [""part-of"", ""Azure Cosmos DB"", ""Azure Data Lake Storage""],
    [""calls"", ""Power BI"", ""Azure Cosmos DB""]
  ]
}
```","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF attachments""],
    [""Logic App"", ""component"", ""Logic app for processing email attachments""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud-based storage for PDF files""],
    [""Function App"", ""component"", ""Function app for processing PDF forms""],
    [""AI Document Intelligence"", ""system"", ""Azure AI service for document processing""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database for storing processed data""],
    [""Power BI"", ""component"", ""Business intelligence tool for data analysis""]
  ],
  ""relationships"": [
    [""contains"", ""Logic App"", ""Azure Data Lake Storage""],
    [""contains"", ""PDF container"", ""Azure Data Lake Storage""],
    [""calls"", ""Logic App"", ""Function App""],
    [""called-from"", ""Function App"", ""AI Document Intelligence""],
    [""contains"", ""Function App"", ""Second PDF container""],
    [""contains"", ""Function App"", ""Data structure""],
    [""contains"", ""Function App"", ""JSON data file""],
    [""part-of"", ""Azure Data Lake Storage"", ""PDF container""],
    [""part-of"", ""Azure Data Lake Storage"", ""Second PDF container""],
    [""part-of"", ""Azure Data Lake Storage"", ""Third PDF container""],
    [""calls"", ""Forms processing logic app"", ""Azure Cosmos DB""],
    [""part-of"", ""Azure Cosmos DB"", ""Azure Data Lake Storage""],
    [""calls"", ""Power BI"", ""Azure Cosmos DB""]
  ]
}
```",4.692772712034639
50,gemma:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system containing supplier data""],
    [""Data Factory"", ""component"", ""Data pipeline that moves data from CDS to SQL""],
    [""ERP system"", ""system"", ""System containing purchase order data""],
    [""Azure SQL Database"", ""system"", ""Database containing purchase order data""],
    [""Azure Functions"", ""component"", ""Automated process that retrieves PO data""],
    [""API"", ""component"", ""Interface for accessing purchase order data""],
    [""Power Apps"", ""component"", ""Application for reviewing and updating purchase orders""],
    [""Suppliers"", ""component"", ""Collection of suppliers""],
    [""CSV"", ""component"", ""Comma-separated values file""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""ERP system"", ""contains"", ""Purchase order data""],
    [""Azure Functions"", ""calls"", ""API""],
    [""Azure SQL Database"", ""contains"", ""Purchase order data""],
    [""API"", ""calls"", ""Power Apps""],
    [""Power Apps"", ""calls"", ""CSV""],
    [""Power Apps"", ""contains"", ""Purchase order data""]
  ]
}
```","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system containing supplier data""],
    [""Data Factory"", ""component"", ""Data pipeline that moves data from CDS to SQL""],
    [""ERP system"", ""system"", ""System containing purchase order data""],
    [""Azure SQL Database"", ""system"", ""Database containing purchase order data""],
    [""Azure Functions"", ""component"", ""Automated process that retrieves PO data""],
    [""API"", ""component"", ""Interface for accessing purchase order data""],
    [""Power Apps"", ""component"", ""Application for reviewing and updating purchase orders""],
    [""Suppliers"", ""component"", ""Collection of suppliers""],
    [""CSV"", ""component"", ""Comma-separated values file""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""ERP system"", ""contains"", ""Purchase order data""],
    [""Azure Functions"", ""calls"", ""API""],
    [""Azure SQL Database"", ""contains"", ""Purchase order data""],
    [""API"", ""calls"", ""Power Apps""],
    [""Power Apps"", ""calls"", ""CSV""],
    [""Power Apps"", ""contains"", ""Purchase order data""]
  ]
}
```",3.559217487985734
51,gemma:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Mainframe"", ""system"", ""Mainframe system""],
    [""Midrange systems"", ""system"", ""Midrange systems""],
    [""Publisher"", ""component"", ""Manage data migration""],
    [""Listener"", ""component"", ""Manage data migration""],
    [""Enhanced-security connection"", ""component"", ""Secure data transfer""],
    [""Connect Replicator Engine"", ""component"", ""Capture and apply changes""],
    [""Target"", ""system"", ""Database""],
    [""Azure Databricks"", ""system"", ""Consume changed data""],
    [""Azure data platform services"", ""system"", ""Apply changes""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""part-of"", ""Mainframe"", ""Publisher""],
    [""part-of"", ""Midrange systems"", ""Listener""],
    [""calls"", ""Publisher"", ""Enhanced-security connection""],
    [""called-from"", ""Enhanced-security connection"", ""Publisher""],
    [""contains"", ""Connect Replicator Engine"", ""Publisher""],
    [""part-of"", ""Target"", ""Azure Databricks""],
    [""calls"", ""Azure Databricks"", ""Target""]
  ]
}
```","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Mainframe"", ""system"", ""Mainframe system""],
    [""Midrange systems"", ""system"", ""Midrange systems""],
    [""Publisher"", ""component"", ""Manage data migration""],
    [""Listener"", ""component"", ""Manage data migration""],
    [""Enhanced-security connection"", ""component"", ""Secure data transfer""],
    [""Connect Replicator Engine"", ""component"", ""Capture and apply changes""],
    [""Target"", ""system"", ""Database""],
    [""Azure Databricks"", ""system"", ""Consume changed data""],
    [""Azure data platform services"", ""system"", ""Apply changes""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""part-of"", ""Mainframe"", ""Publisher""],
    [""part-of"", ""Midrange systems"", ""Listener""],
    [""calls"", ""Publisher"", ""Enhanced-security connection""],
    [""called-from"", ""Enhanced-security connection"", ""Publisher""],
    [""contains"", ""Connect Replicator Engine"", ""Publisher""],
    [""part-of"", ""Target"", ""Azure Databricks""],
    [""calls"", ""Azure Databricks"", ""Target""]
  ]
}
```",3.9067989350296557
52,gemma:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""Version control platform""],
    [""CI/CD pipeline"", ""system"", ""Continuous integration and continuous delivery pipeline""],
    [""Azure VM"", ""component"", ""Virtual machine""],
    [""API application"", ""component"", ""Application""],
    [""ILB ASE"", ""system"", ""Azure Service Environment""],
    [""API Management"", ""component"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Reverse proxy""],
    [""Azure Private DNS"", ""component"", ""Domain name system""],
    [""External users"", ""person"", ""Users outside the organization""]
  ],
  ""relationships"": [
    [""GitHub"", ""contains"", ""CI/CD pipeline""],
    [""CI/CD pipeline"", ""calls"", ""API application""],
    [""API Management"", ""calls"", ""API application""],
    [""API Management"", ""contains"", ""HOST header""],
    [""API Management"", ""contains"", ""DNS name""],
    [""Application Gateway"", ""contains"", ""API Management""],
    [""Azure Private DNS"", ""part-of"", ""ASE""],
    [""ASE"", ""part-of"", ""API Management""],
    [""API Management"", ""part-of"", ""Application Gateway""],
    [""Application Gateway"", ""contains"", ""External users""]
  ]
}
```","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""Version control platform""],
    [""CI/CD pipeline"", ""system"", ""Continuous integration and continuous delivery pipeline""],
    [""Azure VM"", ""component"", ""Virtual machine""],
    [""API application"", ""component"", ""Application""],
    [""ILB ASE"", ""system"", ""Azure Service Environment""],
    [""API Management"", ""component"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Reverse proxy""],
    [""Azure Private DNS"", ""component"", ""Domain name system""],
    [""External users"", ""person"", ""Users outside the organization""]
  ],
  ""relationships"": [
    [""GitHub"", ""contains"", ""CI/CD pipeline""],
    [""CI/CD pipeline"", ""calls"", ""API application""],
    [""API Management"", ""calls"", ""API application""],
    [""API Management"", ""contains"", ""HOST header""],
    [""API Management"", ""contains"", ""DNS name""],
    [""Application Gateway"", ""contains"", ""API Management""],
    [""Azure Private DNS"", ""part-of"", ""ASE""],
    [""ASE"", ""part-of"", ""API Management""],
    [""API Management"", ""part-of"", ""Application Gateway""],
    [""Application Gateway"", ""contains"", ""External users""]
  ]
}
```",3.8483828299795277
53,gemma:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}","```json
{
  ""entities"": [
    [ ""API Management"", ""system"", ""API Management securely routes the HTTP requests to Logic Apps."" ],
    [ ""Logic Apps"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault."" ],
    [ ""Azure Key Vault"", ""system"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault."" ],
    [ ""Data Gateway"", ""component"", ""Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway."" ],
    [ ""SQL Server"", ""system"", ""The on-premises data gateway connects to a SQL Server database to run the statement."" ]
  ],
  ""relationships"": [
    [ ""contains"", ""API Management"", ""Logic Apps"" ],
    [ ""calls"", ""Logic Apps"", ""Data Gateway"" ],
    [ ""part-of"", ""Data Gateway"", ""SQL Server"" ]
  ]
}
```","```json
{
  ""entities"": [
    [ ""API Management"", ""system"", ""API Management securely routes the HTTP requests to Logic Apps."" ],
    [ ""Logic Apps"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault."" ],
    [ ""Azure Key Vault"", ""system"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault."" ],
    [ ""Data Gateway"", ""component"", ""Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway."" ],
    [ ""SQL Server"", ""system"", ""The on-premises data gateway connects to a SQL Server database to run the statement."" ]
  ],
  ""relationships"": [
    [ ""contains"", ""API Management"", ""Logic Apps"" ],
    [ ""calls"", ""Logic Apps"", ""Data Gateway"" ],
    [ ""part-of"", ""Data Gateway"", ""SQL Server"" ]
  ]
}
```",2.8855754929827526
54,gemma:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform using HTTP triggers and connectors""],
    [""Azure API Management"", ""system"", ""API gateway and developer portal for managing APIs""],
    [""Azure DNS"", ""system"", ""Name resolution service""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service for API Gateway""],
    [""API gateway"", ""component"", ""Accepts HTTP calls and routes them to backend""],
    [""Developer portal"", ""component"", ""Access to API documentation and code samples""],
    [""Azure"", ""system"", ""Infrastructure provider""],
    [""Contoso.com"", ""component"", ""Custom domain name""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""contains"", ""Workflow""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure API Management"", ""part-of"", ""Azure""],
    [""Azure DNS"", ""part-of"", ""Azure""],
    [""Microsoft Entra ID"", ""part-of"", ""Azure""],
    [""API gateway"", ""calls"", ""Microsoft Entra ID""],
    [""Microsoft Entra ID"", ""called-from"", ""API Gateway""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform using HTTP triggers and connectors""],
    [""Azure API Management"", ""system"", ""API gateway and developer portal for managing APIs""],
    [""Azure DNS"", ""system"", ""Name resolution service""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service for API Gateway""],
    [""API gateway"", ""component"", ""Accepts HTTP calls and routes them to backend""],
    [""Developer portal"", ""component"", ""Access to API documentation and code samples""],
    [""Azure"", ""system"", ""Infrastructure provider""],
    [""Contoso.com"", ""component"", ""Custom domain name""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""contains"", ""Workflow""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure API Management"", ""part-of"", ""Azure""],
    [""Azure DNS"", ""part-of"", ""Azure""],
    [""Microsoft Entra ID"", ""part-of"", ""Azure""],
    [""API gateway"", ""calls"", ""Microsoft Entra ID""],
    [""Microsoft Entra ID"", ""called-from"", ""API Gateway""]
  ]
}
```",3.9089878459926695
55,gemma:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Data processing platform""],
    [""Azure Virtual Desktop"", ""component"", ""Remote access application""],
    [""Dataset"", ""component"", ""Set of data""],
    [""Import path"", ""component"", ""Storage location""],
    [""Secure environment"", ""system"", ""Restricted access""],
    [""Data science VMs"", ""component"", ""Virtual machines""],
    [""Machine Learning compute"", ""component"", ""Machine learning processing""],
    [""Logic app"", ""component"", ""Automated process""],
    [""Reviewers"", ""person"", ""Data security experts""],
    [""Public storage account"", ""system"", ""Shared data storage""]
  ],
  ""relationships"": [
    [""contains"", ""Secure environment"", ""Azure Virtual Desktop""],
    [""part-of"", ""Secure environment"", ""Data science VMs""],
    [""contains"", ""Secure environment"", ""Machine Learning compute""],
    [""calls"", ""Data Factory"", ""Trigger""],
    [""part-of"", ""Dataset"", ""Import path""],
    [""contains"", ""Secure storage account"", ""Dataset""],
    [""part-of"", ""Secure storage account"", ""Model data""],
    [""calls"", ""Logic app"", ""Data queued for export""],
    [""called-from"", ""Logic app"", ""Data Factory""],
    [""contains"", ""Secure environment"", ""Reviewers""],
    [""part-of"", ""Public storage account"", ""Model data""],
    [""part-of"", ""Public storage account"", ""Dataset""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Data processing platform""],
    [""Azure Virtual Desktop"", ""component"", ""Remote access application""],
    [""Dataset"", ""component"", ""Set of data""],
    [""Import path"", ""component"", ""Storage location""],
    [""Secure environment"", ""system"", ""Restricted access""],
    [""Data science VMs"", ""component"", ""Virtual machines""],
    [""Machine Learning compute"", ""component"", ""Machine learning processing""],
    [""Logic app"", ""component"", ""Automated process""],
    [""Reviewers"", ""person"", ""Data security experts""],
    [""Public storage account"", ""system"", ""Shared data storage""]
  ],
  ""relationships"": [
    [""contains"", ""Secure environment"", ""Azure Virtual Desktop""],
    [""part-of"", ""Secure environment"", ""Data science VMs""],
    [""contains"", ""Secure environment"", ""Machine Learning compute""],
    [""calls"", ""Data Factory"", ""Trigger""],
    [""part-of"", ""Dataset"", ""Import path""],
    [""contains"", ""Secure storage account"", ""Dataset""],
    [""part-of"", ""Secure storage account"", ""Model data""],
    [""calls"", ""Logic app"", ""Data queued for export""],
    [""called-from"", ""Logic app"", ""Data Factory""],
    [""contains"", ""Secure environment"", ""Reviewers""],
    [""part-of"", ""Public storage account"", ""Model data""],
    [""part-of"", ""Public storage account"", ""Dataset""]
  ]
}
```",4.656509752967395
56,gemma:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity provider""],
    [""Azure Front Door"", ""component"", ""Proxy service""],
    [""API Management"", ""system"", ""API gateway""],
    [""App Service Environment"", ""component"", ""Environment""],
    [""Function Apps"", ""component"", ""Environment""],
    [""AKS"", ""component"", ""Environment""],
    [""JWT"", ""component"", ""Authentication token""],
    [""Client"", ""person"", ""Application user""],
    [""Backend API"", ""component"", ""Application""]
  ],
  ""relationships"": [
    [""Client"", ""calls"", ""Azure Front Door""],
    [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""Backend API""],
    [""Client"", ""calls"", ""Backend API""],
    [""Backend API"", ""part-of"", ""App Service Environment""],
    [""Backend API"", ""part-of"", ""Function Apps""],
    [""Backend API"", ""part-of"", ""AKS""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity provider""],
    [""Azure Front Door"", ""component"", ""Proxy service""],
    [""API Management"", ""system"", ""API gateway""],
    [""App Service Environment"", ""component"", ""Environment""],
    [""Function Apps"", ""component"", ""Environment""],
    [""AKS"", ""component"", ""Environment""],
    [""JWT"", ""component"", ""Authentication token""],
    [""Client"", ""person"", ""Application user""],
    [""Backend API"", ""component"", ""Application""]
  ],
  ""relationships"": [
    [""Client"", ""calls"", ""Azure Front Door""],
    [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""Backend API""],
    [""Client"", ""calls"", ""Backend API""],
    [""Backend API"", ""part-of"", ""App Service Environment""],
    [""Backend API"", ""part-of"", ""Function Apps""],
    [""Backend API"", ""part-of"", ""AKS""]
  ]
}
```",3.656027241027914
57,gemma:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API gateway that receives client requests""],
    [""Patient API"", ""component"", ""API that manages patient data""],
    [""Audit API"", ""component"", ""API for creating audit records""],
    [""Azure Cosmos DB"", ""system"", ""Database for patient and audit records""],
    [""CreatePatient"", ""function"", ""Function in Patient API to create a patient""],
    [""CreateAuditRecord"", ""function"", ""Function in Audit API to create an audit record""]
  ],
  ""relationships"": [
    [""calls"", ""APIM"", ""Patient API""],
    [""calls"", ""Patient API"", ""CreatePatient""],
    [""calls"", ""CreatePatient"", ""Audit API""],
    [""contains"", ""CreatePatient"", ""Patient document""],
    [""contains"", ""CreateAuditRecord"", ""Audit record""],
    [""called-from"", ""Audit API"", ""CreateAuditRecord""],
    [""called-from"", ""Patient API"", ""CreatePatient""]
  ]
}
```","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API gateway that receives client requests""],
    [""Patient API"", ""component"", ""API that manages patient data""],
    [""Audit API"", ""component"", ""API for creating audit records""],
    [""Azure Cosmos DB"", ""system"", ""Database for patient and audit records""],
    [""CreatePatient"", ""function"", ""Function in Patient API to create a patient""],
    [""CreateAuditRecord"", ""function"", ""Function in Audit API to create an audit record""]
  ],
  ""relationships"": [
    [""calls"", ""APIM"", ""Patient API""],
    [""calls"", ""Patient API"", ""CreatePatient""],
    [""calls"", ""CreatePatient"", ""Audit API""],
    [""contains"", ""CreatePatient"", ""Patient document""],
    [""contains"", ""CreateAuditRecord"", ""Audit record""],
    [""called-from"", ""Audit API"", ""CreateAuditRecord""],
    [""called-from"", ""Patient API"", ""CreatePatient""]
  ]
}
```",3.060288406035397
58,gemma:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech transcription service""],
    [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
    [""Ingestion Client tool"", ""component"", ""Tool for call center transcription""],
    [""Azure OpenAI"", ""system"", ""Natural language processing service""],
    [""Power BI"", ""component"", ""Data visualization tool""],
    [""App Service"", ""component"", ""Web hosting service""],
    [""CRM"", ""component"", ""Customer relationship management system""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
    [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
    [""Ingestion Client tool"", ""part-of"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
    [""Azure AI Speech"", ""contains"", ""Transcription results""],
    [""Azure AI Language"", ""contains"", ""Processed transcript""],
    [""Azure OpenAI"", ""contains"", ""Processed transcript""],
    [""Azure OpenAI"", ""contains"", ""Entities""],
    [""Azure OpenAI"", ""contains"", ""Summary""],
    [""Azure OpenAI"", ""contains"", ""Sentiment analysis""],
    [""Azure OpenAI"", ""part-of"", ""Azure Blob Storage""],
    [""Power BI"", ""calls"", ""Azure AI Language""],
    [""App Service"", ""calls"", ""Power BI""],
    [""Power BI"", ""contains"", ""Visualization""],
    [""Azure AI Language"", ""part-of"", ""CRM""],
    [""Azure OpenAI"", ""part-of"", ""CRM""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech transcription service""],
    [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
    [""Ingestion Client tool"", ""component"", ""Tool for call center transcription""],
    [""Azure OpenAI"", ""system"", ""Natural language processing service""],
    [""Power BI"", ""component"", ""Data visualization tool""],
    [""App Service"", ""component"", ""Web hosting service""],
    [""CRM"", ""component"", ""Customer relationship management system""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
    [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
    [""Ingestion Client tool"", ""part-of"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
    [""Azure AI Speech"", ""contains"", ""Transcription results""],
    [""Azure AI Language"", ""contains"", ""Processed transcript""],
    [""Azure OpenAI"", ""contains"", ""Processed transcript""],
    [""Azure OpenAI"", ""contains"", ""Entities""],
    [""Azure OpenAI"", ""contains"", ""Summary""],
    [""Azure OpenAI"", ""contains"", ""Sentiment analysis""],
    [""Azure OpenAI"", ""part-of"", ""Azure Blob Storage""],
    [""Power BI"", ""calls"", ""Azure AI Language""],
    [""App Service"", ""calls"", ""Power BI""],
    [""Power BI"", ""contains"", ""Visualization""],
    [""Azure AI Language"", ""part-of"", ""CRM""],
    [""Azure OpenAI"", ""part-of"", ""CRM""]
  ]
}
```",5.300556555972435
59,gemma:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
    [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
    [""Command Message"", ""component"", ""Message containing metadata""],
    [""Durable Functions"", ""component"", ""Orchestration framework""],
    [""Computer Vision Read API"", ""component"", ""Optical character recognition API""],
    [""Document Classifier Service"", ""component"", ""Service for classifying document types""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing document metadata""],
    [""Cognitive Search"", ""system"", ""Search engine""],
    [""Correlation ID"", ""component"", ""Unique identifier""]
  ],
  ""relationships"": [
    [""Command Message"", ""calls"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Computer Vision Read API""],
    [""Computer Vision Read API"", ""returns"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Classify activity""],
    [""Classify activity"", ""calls"", ""Document Classifier Service""],
    [""Document Classifier Service"", ""returns"", ""Classify activity""],
    [""Durable Functions"", ""calls"", ""Metadata Store activity""],
    [""Durable Functions"", ""calls"", ""Indexing activity""],
    [""Indexing activity"", ""creates"", ""Cognitive Search""],
    [""Indexing activity"", ""uses"", ""Azure AI Search libraries""],
    [""Durable Functions"", ""returns"", ""Correlation ID""],
    [""Correlation ID"", ""related-to"", ""Azure Cosmos DB""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
    [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
    [""Command Message"", ""component"", ""Message containing metadata""],
    [""Durable Functions"", ""component"", ""Orchestration framework""],
    [""Computer Vision Read API"", ""component"", ""Optical character recognition API""],
    [""Document Classifier Service"", ""component"", ""Service for classifying document types""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing document metadata""],
    [""Cognitive Search"", ""system"", ""Search engine""],
    [""Correlation ID"", ""component"", ""Unique identifier""]
  ],
  ""relationships"": [
    [""Command Message"", ""calls"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Computer Vision Read API""],
    [""Computer Vision Read API"", ""returns"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Classify activity""],
    [""Classify activity"", ""calls"", ""Document Classifier Service""],
    [""Document Classifier Service"", ""returns"", ""Classify activity""],
    [""Durable Functions"", ""calls"", ""Metadata Store activity""],
    [""Durable Functions"", ""calls"", ""Indexing activity""],
    [""Indexing activity"", ""creates"", ""Cognitive Search""],
    [""Indexing activity"", ""uses"", ""Azure AI Search libraries""],
    [""Durable Functions"", ""returns"", ""Correlation ID""],
    [""Correlation ID"", ""related-to"", ""Azure Cosmos DB""]
  ]
}
```",4.604902392951772
60,gemma:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system for suppliers""],
    [""Data Factory"", ""component"", ""Data migration tool""],
    [""ERP system"", ""system"", ""Purchase order management system""],
    [""Azure SQL Database"", ""system"", ""Relational database""],
    [""Azure Functions"", ""component"", ""Data extraction and transformation tool""],
    [""API"", ""component"", ""Application programming interface""],
    [""Power Apps"", ""component"", ""Data review and update platform""],
    [""Suppliers"", ""person"", ""External data source""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""Data Factory"", ""part-of"", ""CDS""],
    [""ERP system"", ""contains"", ""Purchase order data""],
    [""Purchase order data"", ""part-of"", ""ERP system""],
    [""Azure Functions"", ""calls"", ""API""],
    [""API"", ""part-of"", ""Azure Functions""],
    [""Azure Functions"", ""contains"", ""Monthly PO data""],
    [""Power Apps"", ""calls"", ""Azure SQL Database""],
    [""Azure SQL Database"", ""part-of"", ""Power Apps""],
    [""Power Apps"", ""contains"", ""PO updates""],
    [""PO updates"", ""calls"", ""Suppliers""],
    [""Suppliers"", ""part-of"", ""PO updates""],
    [""Power BI"", ""calls"", ""Power Apps""]
  ]
}
```","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system for suppliers""],
    [""Data Factory"", ""component"", ""Data migration tool""],
    [""ERP system"", ""system"", ""Purchase order management system""],
    [""Azure SQL Database"", ""system"", ""Relational database""],
    [""Azure Functions"", ""component"", ""Data extraction and transformation tool""],
    [""API"", ""component"", ""Application programming interface""],
    [""Power Apps"", ""component"", ""Data review and update platform""],
    [""Suppliers"", ""person"", ""External data source""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""Data Factory"", ""part-of"", ""CDS""],
    [""ERP system"", ""contains"", ""Purchase order data""],
    [""Purchase order data"", ""part-of"", ""ERP system""],
    [""Azure Functions"", ""calls"", ""API""],
    [""API"", ""part-of"", ""Azure Functions""],
    [""Azure Functions"", ""contains"", ""Monthly PO data""],
    [""Power Apps"", ""calls"", ""Azure SQL Database""],
    [""Azure SQL Database"", ""part-of"", ""Power Apps""],
    [""Power Apps"", ""contains"", ""PO updates""],
    [""PO updates"", ""calls"", ""Suppliers""],
    [""Suppliers"", ""part-of"", ""PO updates""],
    [""Power BI"", ""calls"", ""Power Apps""]
  ]
}
```",4.169492529006675
61,gemma:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Data processing platform""],
    [""Microsoft-managed keys"", ""component"", ""Encryption key""],
    [""Secure storage account"", ""component"", ""Data storage""],
    [""Streaming application"", ""component"", ""Access tool""],
    [""Data science VMs"", ""component"", ""Research environment""],
    [""Azure Machine Learning"", ""system"", ""Machine learning platform""],
    [""Logic app"", ""component"", ""Approval process""],
    [""Data"", ""component"", ""Dataset""],
    [""Public storage account"", ""component"", ""Data access""]
  ],
  ""relationships"": [
    [""Azure Data Factory"", ""contains"", ""Microsoft-managed keys""],
    [""Azure Data Factory"", ""part-of"", ""Secure storage account""],
    [""Secure storage account"", ""part-of"", ""Azure Virtual Desktop""],
    [""Secure storage account"", ""contains"", ""Data""],
    [""Data"", ""part-of"", ""Data science VMs""],
    [""Data science VMs"", ""contains"", ""Azure Machine Learning""],
    [""Azure Machine Learning"", ""contains"", ""Data""],
    [""Data"", ""calls"", ""Logic app""],
    [""Logic app"", ""calls"", ""Reviewers""],
    [""Reviewers"", ""calls"", ""Data""],
    [""Data"", ""part-of"", ""Public storage account""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Data processing platform""],
    [""Microsoft-managed keys"", ""component"", ""Encryption key""],
    [""Secure storage account"", ""component"", ""Data storage""],
    [""Streaming application"", ""component"", ""Access tool""],
    [""Data science VMs"", ""component"", ""Research environment""],
    [""Azure Machine Learning"", ""system"", ""Machine learning platform""],
    [""Logic app"", ""component"", ""Approval process""],
    [""Data"", ""component"", ""Dataset""],
    [""Public storage account"", ""component"", ""Data access""]
  ],
  ""relationships"": [
    [""Azure Data Factory"", ""contains"", ""Microsoft-managed keys""],
    [""Azure Data Factory"", ""part-of"", ""Secure storage account""],
    [""Secure storage account"", ""part-of"", ""Azure Virtual Desktop""],
    [""Secure storage account"", ""contains"", ""Data""],
    [""Data"", ""part-of"", ""Data science VMs""],
    [""Data science VMs"", ""contains"", ""Azure Machine Learning""],
    [""Azure Machine Learning"", ""contains"", ""Data""],
    [""Data"", ""calls"", ""Logic app""],
    [""Logic app"", ""calls"", ""Reviewers""],
    [""Reviewers"", ""calls"", ""Data""],
    [""Data"", ""part-of"", ""Public storage account""]
  ]
}
```",4.303176827961579
62,gemma:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}","```json
{
  ""entities"": [
    [""Azure VM"", ""system"", ""Cloud-based virtual machine""],
    [""GitHub repository"", ""system"", ""Version control system""],
    [""CI/CD pipeline"", ""system"", ""Continuous integration and continuous delivery pipeline""],
    [""API application"", ""component"", ""Software application""],
    [""Azure API Management"", ""system"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Traffic routing service""],
    [""ILB ASE"", ""system"", ""Azure Service Environment""],
    [""External users"", ""person"", ""Individuals accessing the API""],
    [""Private DNS"", ""component"", ""Domain name system""],
  ],
  ""relationships"": [
    [""GitHub repository"", ""contains"", ""CI/CD pipeline""],
    [""CI/CD pipeline"", ""part-of"", ""Azure VM""],
    [""CI/CD pipeline"", ""calls"", ""API application""],
    [""API application"", ""part-of"", ""ILB ASE""],
    [""Azure API Management"", ""contains"", ""HOST header""],
    [""Azure API Management"", ""part-of"", ""ILB ASE""],
    [""Application Gateway"", ""contains"", ""API Management""],
    [""Application Gateway"", ""part-of"", ""ILB ASE""],
    [""ILB ASE"", ""contains"", ""API application""],
    [""Azure Private DNS"", ""part-of"", ""ILB ASE""],
    [""Application Gateway"", ""calls"", ""External users""],
    [""External users"", ""calls"", ""Application Gateway""],
  ]
}
```","```json
{
  ""entities"": [
    [""Azure VM"", ""system"", ""Cloud-based virtual machine""],
    [""GitHub repository"", ""system"", ""Version control system""],
    [""CI/CD pipeline"", ""system"", ""Continuous integration and continuous delivery pipeline""],
    [""API application"", ""component"", ""Software application""],
    [""Azure API Management"", ""system"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Traffic routing service""],
    [""ILB ASE"", ""system"", ""Azure Service Environment""],
    [""External users"", ""person"", ""Individuals accessing the API""],
    [""Private DNS"", ""component"", ""Domain name system""],
  ],
  ""relationships"": [
    [""GitHub repository"", ""contains"", ""CI/CD pipeline""],
    [""CI/CD pipeline"", ""part-of"", ""Azure VM""],
    [""CI/CD pipeline"", ""calls"", ""API application""],
    [""API application"", ""part-of"", ""ILB ASE""],
    [""Azure API Management"", ""contains"", ""HOST header""],
    [""Azure API Management"", ""part-of"", ""ILB ASE""],
    [""Application Gateway"", ""contains"", ""API Management""],
    [""Application Gateway"", ""part-of"", ""ILB ASE""],
    [""ILB ASE"", ""contains"", ""API application""],
    [""Azure Private DNS"", ""part-of"", ""ILB ASE""],
    [""Application Gateway"", ""calls"", ""External users""],
    [""External users"", ""calls"", ""Application Gateway""],
  ]
}
```",4.460387350991368
63,gemma:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Mainframe"", ""system"", ""System for mainframe data capture""],
    [""Midrange systems"", ""system"", ""Systems for midrange data capture""],
    [""Publisher"", ""component"", ""Data migration manager (mainframe)""],
    [""Listener"", ""component"", ""Data migration manager (midrange)""],
    [""Enhanced-security connection"", ""component"", ""Data transmission security""],
    [""Replicator Engine"", ""component"", ""Data capture and transformation""],
    [""Target"", ""component"", ""Database""],
    [""Azure Data Platform services"", ""system"", ""Data storage and processing""],
    [""Azure Event Hubs"", ""component"", ""Data ingestion channel""],
    [""Azure Databricks"", ""component"", ""Data consumption and transformation""]
  ],
  ""relationships"": [
    [""Contains"", ""Connect agent component"", ""Publisher""],
    [""Contains"", ""Mainframe"", ""Publisher""],
    [""Part-of"", ""Midrange systems"", ""Listener""],
    [""Contains"", ""Listener"", ""Windows or Linux machine""],
    [""Calls"", ""Publisher or listener"", ""Enhanced-security connection""],
    [""Called-from"", ""Enhanced-security connection"", ""Publisher or listener""],
    [""Contains"", ""Publisher or listener"", ""Replicator Engine""],
    [""Part-of"", ""Replicator Engine"", ""Target""],
    [""Contains"", ""Target"", ""Azure Data Platform services""],
    [""Ingests"", ""Azure Event Hubs"", ""Target""],
    [""Consumes"", ""Azure Databricks"", ""Target""]
  ]
}
```","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Mainframe"", ""system"", ""System for mainframe data capture""],
    [""Midrange systems"", ""system"", ""Systems for midrange data capture""],
    [""Publisher"", ""component"", ""Data migration manager (mainframe)""],
    [""Listener"", ""component"", ""Data migration manager (midrange)""],
    [""Enhanced-security connection"", ""component"", ""Data transmission security""],
    [""Replicator Engine"", ""component"", ""Data capture and transformation""],
    [""Target"", ""component"", ""Database""],
    [""Azure Data Platform services"", ""system"", ""Data storage and processing""],
    [""Azure Event Hubs"", ""component"", ""Data ingestion channel""],
    [""Azure Databricks"", ""component"", ""Data consumption and transformation""]
  ],
  ""relationships"": [
    [""Contains"", ""Connect agent component"", ""Publisher""],
    [""Contains"", ""Mainframe"", ""Publisher""],
    [""Part-of"", ""Midrange systems"", ""Listener""],
    [""Contains"", ""Listener"", ""Windows or Linux machine""],
    [""Calls"", ""Publisher or listener"", ""Enhanced-security connection""],
    [""Called-from"", ""Enhanced-security connection"", ""Publisher or listener""],
    [""Contains"", ""Publisher or listener"", ""Replicator Engine""],
    [""Part-of"", ""Replicator Engine"", ""Target""],
    [""Contains"", ""Target"", ""Azure Data Platform services""],
    [""Ingests"", ""Azure Event Hubs"", ""Target""],
    [""Consumes"", ""Azure Databricks"", ""Target""]
  ]
}
```",4.770205957000144
64,gemma:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF files""],
    [""Logic App"", ""component"", ""Logic app for processing email attachments""],
    [""Azure Data Lake Storage"", ""system"", ""Storage container for PDF files""],
    [""Function App"", ""component"", ""Function app for processing PDF forms""],
    [""AI Document Intelligence"", ""system"", ""Azure AI service for document processing""],
    [""Cosmos DB"", ""system"", ""NoSQL database for storing processed data""],
    [""Power BI"", ""component"", ""Data analytics platform""]
  ],
  ""relationships"": [
    [""contains"", ""Logic App"", ""Outlook email account""],
    [""part-of"", ""PDF container"", ""Azure Data Lake Storage""],
    [""calls"", ""Logic App"", ""Function App""],
    [""called-from"", ""Function App"", ""AI Document Intelligence""],
    [""contains"", ""Function App"", ""Single-page PDF file""],
    [""contains"", ""Function App"", ""Processed response data""],
    [""part-of"", ""Processed response data"", ""Cosmos DB""],
    [""contains"", ""Cosmos DB"", ""Processed data""]
  ]
}
```","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF files""],
    [""Logic App"", ""component"", ""Logic app for processing email attachments""],
    [""Azure Data Lake Storage"", ""system"", ""Storage container for PDF files""],
    [""Function App"", ""component"", ""Function app for processing PDF forms""],
    [""AI Document Intelligence"", ""system"", ""Azure AI service for document processing""],
    [""Cosmos DB"", ""system"", ""NoSQL database for storing processed data""],
    [""Power BI"", ""component"", ""Data analytics platform""]
  ],
  ""relationships"": [
    [""contains"", ""Logic App"", ""Outlook email account""],
    [""part-of"", ""PDF container"", ""Azure Data Lake Storage""],
    [""calls"", ""Logic App"", ""Function App""],
    [""called-from"", ""Function App"", ""AI Document Intelligence""],
    [""contains"", ""Function App"", ""Single-page PDF file""],
    [""contains"", ""Function App"", ""Processed response data""],
    [""part-of"", ""Processed response data"", ""Cosmos DB""],
    [""contains"", ""Cosmos DB"", ""Processed data""]
  ]
}
```",3.7777573359780945
65,gemma:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}","```json
{
""entities"": [
  [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
  [""Azure AI Speech"", ""system"", ""Speech transcription service""],
  [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
  [""Azure OpenAI"", ""system"", ""Transcript processing and analysis service""],
  [""Power BI"", ""component"", ""Visualization tool""],
  [""App Service"", ""component"", ""Web application hosting service""],
  [""Azure Storage Account"", ""component"", ""Storage location for audio files""],
  [""Ingestion Client tool"", ""component"", ""Transcription and data processing tool""]
],
""relationships"": [
  [""Azure Blob Storage"", ""contains"", ""Audio files""],
  [""Azure AI Speech"", ""calls"", ""Azure Storage Account""],
  [""Azure AI Language"", ""part-of"", ""Azure AI Speech""],
  [""Azure OpenAI"", ""part-of"", ""Azure AI Speech""],
  [""Azure OpenAI"", ""contains"", ""Transcript""],
  [""Azure OpenAI"", ""calls"", ""Ingestion Client tool""],
  [""Ingestion Client tool"", ""part-of"", ""Azure AI Speech""],
  [""Azure OpenAI"", ""contains"", ""Visualization""],
  [""Visualization"", ""part-of"", ""Power BI""],
  [""Power BI"", ""part-of"", ""App Service""],
  [""App Service"", ""part-of"", ""Azure OpenAI""]
]
}
```","```json
{
""entities"": [
  [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
  [""Azure AI Speech"", ""system"", ""Speech transcription service""],
  [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
  [""Azure OpenAI"", ""system"", ""Transcript processing and analysis service""],
  [""Power BI"", ""component"", ""Visualization tool""],
  [""App Service"", ""component"", ""Web application hosting service""],
  [""Azure Storage Account"", ""component"", ""Storage location for audio files""],
  [""Ingestion Client tool"", ""component"", ""Transcription and data processing tool""]
],
""relationships"": [
  [""Azure Blob Storage"", ""contains"", ""Audio files""],
  [""Azure AI Speech"", ""calls"", ""Azure Storage Account""],
  [""Azure AI Language"", ""part-of"", ""Azure AI Speech""],
  [""Azure OpenAI"", ""part-of"", ""Azure AI Speech""],
  [""Azure OpenAI"", ""contains"", ""Transcript""],
  [""Azure OpenAI"", ""calls"", ""Ingestion Client tool""],
  [""Ingestion Client tool"", ""part-of"", ""Azure AI Speech""],
  [""Azure OpenAI"", ""contains"", ""Visualization""],
  [""Visualization"", ""part-of"", ""Power BI""],
  [""Power BI"", ""part-of"", ""App Service""],
  [""App Service"", ""part-of"", ""Azure OpenAI""]
]
}
```",4.359097724955063
66,gemma:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing documents""],
    [""Command Message"", ""component"", ""Message containing metadata about the document""],
    [""Durable Functions"", ""system"", ""Orchestration engine""],
    [""Computer Vision Read API"", ""component"", ""API for optical character recognition""],
    [""Document Classifier Service"", ""component"", ""Service for classifying document types""],
    [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing document metadata""],
    [""Cognitive Search"", ""system"", ""Search engine for storing and retrieving documents""],
    [""Correlation ID"", ""component"", ""Unique identifier for matching search results with document metadata""]
  ],
  ""relationships"": [
    [""contains"", ""Command Message"", ""Durable Functions""],
    [""calls"", ""Scan activity function"", ""Computer Vision Read API""],
    [""calls"", ""Classify activity function"", ""Document Classifier Service""],
    [""contains"", ""Document Classifier Service"", ""Azure Kubernetes Service""],
    [""part-of"", ""Document Classifier Service"", ""Azure Kubernetes Service""],
    [""contains"", ""Durable Functions"", ""Azure Cosmos DB""],
    [""part-of"", ""Azure Cosmos DB"", ""Azure Blob Storage""],
    [""contains"", ""Durable Functions"", ""Indexing activity function""],
    [""calls"", ""Indexing activity function"", ""Cognitive Search""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing documents""],
    [""Command Message"", ""component"", ""Message containing metadata about the document""],
    [""Durable Functions"", ""system"", ""Orchestration engine""],
    [""Computer Vision Read API"", ""component"", ""API for optical character recognition""],
    [""Document Classifier Service"", ""component"", ""Service for classifying document types""],
    [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing document metadata""],
    [""Cognitive Search"", ""system"", ""Search engine for storing and retrieving documents""],
    [""Correlation ID"", ""component"", ""Unique identifier for matching search results with document metadata""]
  ],
  ""relationships"": [
    [""contains"", ""Command Message"", ""Durable Functions""],
    [""calls"", ""Scan activity function"", ""Computer Vision Read API""],
    [""calls"", ""Classify activity function"", ""Document Classifier Service""],
    [""contains"", ""Document Classifier Service"", ""Azure Kubernetes Service""],
    [""part-of"", ""Document Classifier Service"", ""Azure Kubernetes Service""],
    [""contains"", ""Durable Functions"", ""Azure Cosmos DB""],
    [""part-of"", ""Azure Cosmos DB"", ""Azure Blob Storage""],
    [""contains"", ""Durable Functions"", ""Indexing activity function""],
    [""calls"", ""Indexing activity function"", ""Cognitive Search""]
  ]
}
```",4.284576325037051
67,gemma:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}","```json
{
""entities"": [
  [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services.""],
  [""Azure API Management"", ""system"", ""API Management consists of two related components""],
  [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend.""],
  [""Developer portal"", ""component"", ""Each instance of Azure API Management provides access to a developer portal.""],
  [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""],
  [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""],
  [""Standard or Premium tier"", ""component"", ""If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal""]
],
""relationships"": [
  [""Azure Logic Apps"", ""calls"", ""API gateway""],
  [""API gateway"", ""part-of"", ""Azure API Management""],
  [""Azure API Management"", ""contains"", ""API gateway""],
  [""Azure API Management"", ""contains"", ""Developer portal""],
  [""Azure DNS"", ""part-of"", ""Azure infrastructure""],
  [""Microsoft Entra ID"", ""calls"", ""API gateway""],
  [""Standard or Premium tier"", ""part-of"", ""Azure API Management""]
]
}
```","```json
{
""entities"": [
  [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services.""],
  [""Azure API Management"", ""system"", ""API Management consists of two related components""],
  [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend.""],
  [""Developer portal"", ""component"", ""Each instance of Azure API Management provides access to a developer portal.""],
  [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""],
  [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""],
  [""Standard or Premium tier"", ""component"", ""If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal""]
],
""relationships"": [
  [""Azure Logic Apps"", ""calls"", ""API gateway""],
  [""API gateway"", ""part-of"", ""Azure API Management""],
  [""Azure API Management"", ""contains"", ""API gateway""],
  [""Azure API Management"", ""contains"", ""Developer portal""],
  [""Azure DNS"", ""part-of"", ""Azure infrastructure""],
  [""Microsoft Entra ID"", ""calls"", ""API gateway""],
  [""Standard or Premium tier"", ""part-of"", ""Azure API Management""]
]
}
```",4.087691740016453
68,gemma:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""],
    [""Patient API"", ""component"", ""Patient API calls the CreatePatient function""],
    [""CreatePatient"", ""function"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function""],
    [""Audit API"", ""component"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API""],
    [""CreateAuditRecord"", ""function"", ""The Audit API CreateAuditRecord function creates the audit record""],
    [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""],
    [""Patient document"", ""component"", ""The CreatePatient function creates the patient document""]
  ],
  ""relationships"": [
    [""APIM"", ""calls"", ""Patient API""],
    [""Patient API"", ""contains"", ""CreatePatient""],
    [""CreatePatient"", ""calls"", ""Audit API""],
    [""Audit API"", ""contains"", ""CreateAuditRecord""],
    [""CreateAuditRecord"", ""calls"", ""Azure Cosmos DB""],
    [""Patient API"", ""contains"", ""Patient document""]
  ]
}
```","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""],
    [""Patient API"", ""component"", ""Patient API calls the CreatePatient function""],
    [""CreatePatient"", ""function"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function""],
    [""Audit API"", ""component"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API""],
    [""CreateAuditRecord"", ""function"", ""The Audit API CreateAuditRecord function creates the audit record""],
    [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""],
    [""Patient document"", ""component"", ""The CreatePatient function creates the patient document""]
  ],
  ""relationships"": [
    [""APIM"", ""calls"", ""Patient API""],
    [""Patient API"", ""contains"", ""CreatePatient""],
    [""CreatePatient"", ""calls"", ""Audit API""],
    [""Audit API"", ""contains"", ""CreateAuditRecord""],
    [""CreateAuditRecord"", ""calls"", ""Azure Cosmos DB""],
    [""Patient API"", ""contains"", ""Patient document""]
  ]
}
```",3.4629797950037755
69,gemma:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Azure Active Directory B2C authentication service""],
    [""Azure Front Door"", ""component"", ""Azure service for load balancing and routing traffic""],
    [""API Management"", ""component"", ""Microsoft API management platform""],
    [""App Service Environment"", ""component"", ""Microsoft cloud service for deploying web apps""],
    [""Function Apps"", ""component"", ""Microsoft cloud service for deploying serverless functions""],
    [""AKS"", ""component"", ""Kubernetes service""],
    [""JWT"", ""component"", ""JSON Web Token""],
    [""User"", ""person"", ""Application user""]
  ],
  ""relationships"": [
    [""User"", ""calls"", ""Azure Front Door""],
    [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure AD B2C"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""App Service Environment""],
    [""API Management"", ""contains"", ""Function Apps""],
    [""API Management"", ""contains"", ""AKS""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Azure Active Directory B2C authentication service""],
    [""Azure Front Door"", ""component"", ""Azure service for load balancing and routing traffic""],
    [""API Management"", ""component"", ""Microsoft API management platform""],
    [""App Service Environment"", ""component"", ""Microsoft cloud service for deploying web apps""],
    [""Function Apps"", ""component"", ""Microsoft cloud service for deploying serverless functions""],
    [""AKS"", ""component"", ""Kubernetes service""],
    [""JWT"", ""component"", ""JSON Web Token""],
    [""User"", ""person"", ""Application user""]
  ],
  ""relationships"": [
    [""User"", ""calls"", ""Azure Front Door""],
    [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure AD B2C"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""App Service Environment""],
    [""API Management"", ""contains"", ""Function Apps""],
    [""API Management"", ""contains"", ""AKS""]
  ]
}
```",3.9860556659987196
70,gemma:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}","```json
{
  ""entities"": [
    [ ""API Management"", ""system"", ""API management platform that accepts API calls"" ],
    [ ""Logic Apps"", ""component"", ""Workflow engine that processes API calls"" ],
    [ ""Azure Key Vault"", ""component"", ""Service for securely storing and retrieving secrets"" ],
    [ ""Data Gateway"", ""component"", ""Gateway that connects Logic Apps to on-premises SQL Server"" ],
    [ ""SQL Server"", ""system"", ""Relational database for storing data"" ]
  ],
  ""relationships"": [
    [ ""API Management"", ""calls"", ""Logic Apps"", ""API Management sends requests to Logic Apps"" ],
    [ ""Logic Apps"", ""contains"", ""Azure Key Vault"", ""Logic Apps retrieves credentials from Key Vault"" ],
    [ ""Logic Apps"", ""contains"", ""Data Gateway"", ""Logic Apps sends data to Data Gateway"" ],
    [ ""Data Gateway"", ""part-of"", ""SQL Server"", ""Data Gateway connects to SQL Server"" ]
  ]
}
```","```json
{
  ""entities"": [
    [ ""API Management"", ""system"", ""API management platform that accepts API calls"" ],
    [ ""Logic Apps"", ""component"", ""Workflow engine that processes API calls"" ],
    [ ""Azure Key Vault"", ""component"", ""Service for securely storing and retrieving secrets"" ],
    [ ""Data Gateway"", ""component"", ""Gateway that connects Logic Apps to on-premises SQL Server"" ],
    [ ""SQL Server"", ""system"", ""Relational database for storing data"" ]
  ],
  ""relationships"": [
    [ ""API Management"", ""calls"", ""Logic Apps"", ""API Management sends requests to Logic Apps"" ],
    [ ""Logic Apps"", ""contains"", ""Azure Key Vault"", ""Logic Apps retrieves credentials from Key Vault"" ],
    [ ""Logic Apps"", ""contains"", ""Data Gateway"", ""Logic Apps sends data to Data Gateway"" ],
    [ ""Data Gateway"", ""part-of"", ""SQL Server"", ""Data Gateway connects to SQL Server"" ]
  ]
}
```",3.1092647209879942
71,gemma:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Azure API Management"", ""system"", ""Custom API host""],
    [""Custom API Coordinator"", ""component"", ""Notification handler""],
    [""Power Apps"", ""system"", ""Flight assignment platform""],
    [""Microsoft Teams"", ""system"", ""Collaboration platform""],
    [""Graph API"", ""component"", ""Communication protocol""],
    [""Azure Storage Account"", ""system"", ""Message queue""],
    [""Azure Functions"", ""component"", ""Graph API processor""],
    [""Azure Event Hubs"", ""system"", ""Event streaming platform""],
    [""Azure Data Lake"", ""system"", ""Data storage""],
    [""Power BI"", ""system"", ""Analytics platform""],
    [""Custom Bot Messaging Service"", ""component"", ""Bot service""],
    [""Custom Bots"", ""component"", ""Flight update sender""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API Coordinator""],
    [""Custom API Coordinator"", ""calls"", ""Power Apps""],
    [""Custom API Coordinator"", ""part-of"", ""Airline System""],
    [""Custom API Coordinator"", ""calls"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Graph API"", ""part-of"", ""Azure Functions""],
    [""Azure Storage Account"", ""part-of"", ""Airline System""],
    [""Azure Functions"", ""part-of"", ""Airline System""],
    [""Azure Event Hubs"", ""part-of"", ""Airline System""],
    [""Azure Data Lake"", ""part-of"", ""Airline System""],
    [""Power BI"", ""part-of"", ""Airline System""],
    [""Custom Bot Messaging Service"", ""part-of"", ""Airline System""],
    [""Custom Bot Messaging Service"", ""calls"", ""Custom Bots""],
    [""Custom Bots"", ""part-of"", ""Airline System""]
  ]
}
```","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Azure API Management"", ""system"", ""Custom API host""],
    [""Custom API Coordinator"", ""component"", ""Notification handler""],
    [""Power Apps"", ""system"", ""Flight assignment platform""],
    [""Microsoft Teams"", ""system"", ""Collaboration platform""],
    [""Graph API"", ""component"", ""Communication protocol""],
    [""Azure Storage Account"", ""system"", ""Message queue""],
    [""Azure Functions"", ""component"", ""Graph API processor""],
    [""Azure Event Hubs"", ""system"", ""Event streaming platform""],
    [""Azure Data Lake"", ""system"", ""Data storage""],
    [""Power BI"", ""system"", ""Analytics platform""],
    [""Custom Bot Messaging Service"", ""component"", ""Bot service""],
    [""Custom Bots"", ""component"", ""Flight update sender""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API Coordinator""],
    [""Custom API Coordinator"", ""calls"", ""Power Apps""],
    [""Custom API Coordinator"", ""part-of"", ""Airline System""],
    [""Custom API Coordinator"", ""calls"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Graph API"", ""part-of"", ""Azure Functions""],
    [""Azure Storage Account"", ""part-of"", ""Airline System""],
    [""Azure Functions"", ""part-of"", ""Airline System""],
    [""Azure Event Hubs"", ""part-of"", ""Airline System""],
    [""Azure Data Lake"", ""part-of"", ""Airline System""],
    [""Power BI"", ""part-of"", ""Airline System""],
    [""Custom Bot Messaging Service"", ""part-of"", ""Airline System""],
    [""Custom Bot Messaging Service"", ""calls"", ""Custom Bots""],
    [""Custom Bots"", ""part-of"", ""Airline System""]
  ]
}
```",5.436467210005503
72,gemma:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}","```json
{
  ""entities"": [
    [""Outlook Email Account"", ""system"", ""Designated email account for receiving PDF files""],
    [""Logic App"", ""component"", ""Logic app for processing email attachments""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud-based storage service for data""],
    [""PDF Container"", ""component"", ""Container for storing PDF files""],
    [""Function App"", ""component"", ""Function app for processing PDF forms""],
    [""AI Document Intelligence"", ""system"", ""Azure service for processing documents""],
    [""Cosmos DB"", ""system"", ""Database for storing processed data""],
    [""Power BI"", ""person"", ""Business intelligence tool""]
  ],
  ""relationships"": [
    [""Contains"", ""Logic App"", ""PDF Container""],
    [""Contains"", ""PDF Container"", ""PDF files""],
    [""Contains"", ""Data Lake Storage"", ""PDF Container""],
    [""Contains"", ""Data Lake Storage"", ""Split PDF files""],
    [""Calls"", ""PDF Container"", ""Logic App""],
    [""Called-from"", ""Function App"", ""AI Document Intelligence""],
    [""Contains"", ""Data Lake Storage"", ""Processed response data""],
    [""Contains"", ""Cosmos DB"", ""Processed data""],
    [""Uses"", ""Power BI"", ""Cosmos DB""]
  ]
}
```","```json
{
  ""entities"": [
    [""Outlook Email Account"", ""system"", ""Designated email account for receiving PDF files""],
    [""Logic App"", ""component"", ""Logic app for processing email attachments""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud-based storage service for data""],
    [""PDF Container"", ""component"", ""Container for storing PDF files""],
    [""Function App"", ""component"", ""Function app for processing PDF forms""],
    [""AI Document Intelligence"", ""system"", ""Azure service for processing documents""],
    [""Cosmos DB"", ""system"", ""Database for storing processed data""],
    [""Power BI"", ""person"", ""Business intelligence tool""]
  ],
  ""relationships"": [
    [""Contains"", ""Logic App"", ""PDF Container""],
    [""Contains"", ""PDF Container"", ""PDF files""],
    [""Contains"", ""Data Lake Storage"", ""PDF Container""],
    [""Contains"", ""Data Lake Storage"", ""Split PDF files""],
    [""Calls"", ""PDF Container"", ""Logic App""],
    [""Called-from"", ""Function App"", ""AI Document Intelligence""],
    [""Contains"", ""Data Lake Storage"", ""Processed response data""],
    [""Contains"", ""Cosmos DB"", ""Processed data""],
    [""Uses"", ""Power BI"", ""Cosmos DB""]
  ]
}
```",4.0238190979580395
73,gemma:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing documents""],
    [""Command message"", ""component"", ""Message containing metadata""],
    [""Durable Functions"", ""system"", ""Orchestration framework""],
    [""Computer Vision Read API"", ""component"", ""API for optical character recognition""],
    [""Document classifier service"", ""component"", ""Service for classifying document types""],
    [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing document metadata""],
    [""Cognitive Search"", ""system"", ""Search engine for searching documents""],
    [""Correlation ID"", ""component"", ""Unique identifier for linking search results to document metadata""]
  ],
  ""relationships"": [
    [""Command message"", ""calls"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Computer Vision Read API""],
    [""Computer Vision Read API"", ""calls"", ""Document classifier service""],
    [""Document classifier service"", ""returns"", ""Azure Cosmos DB""],
    [""Durable Functions"", ""contains"", ""Scan activity function""],
    [""Durable Functions"", ""contains"", ""Classify activity function""],
    [""Durable Functions"", ""contains"", ""Metadata Store activity function""],
    [""Durable Functions"", ""contains"", ""Indexing activity function""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing documents""],
    [""Command message"", ""component"", ""Message containing metadata""],
    [""Durable Functions"", ""system"", ""Orchestration framework""],
    [""Computer Vision Read API"", ""component"", ""API for optical character recognition""],
    [""Document classifier service"", ""component"", ""Service for classifying document types""],
    [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing document metadata""],
    [""Cognitive Search"", ""system"", ""Search engine for searching documents""],
    [""Correlation ID"", ""component"", ""Unique identifier for linking search results to document metadata""]
  ],
  ""relationships"": [
    [""Command message"", ""calls"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Computer Vision Read API""],
    [""Computer Vision Read API"", ""calls"", ""Document classifier service""],
    [""Document classifier service"", ""returns"", ""Azure Cosmos DB""],
    [""Durable Functions"", ""contains"", ""Scan activity function""],
    [""Durable Functions"", ""contains"", ""Classify activity function""],
    [""Durable Functions"", ""contains"", ""Metadata Store activity function""],
    [""Durable Functions"", ""contains"", ""Indexing activity function""]
  ]
}
```",3.963554876972921
74,gemma:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech recognition service for transcribing audio files""],
    [""Azure AI Language"", ""system"", ""Natural language processing service for detecting and redacting personal data""],
    [""Azure OpenAI"", ""system"", ""Natural language processing service for processing transcripts""],
    [""Power BI"", ""component"", ""Data visualization tool""],
    [""App Service"", ""component"", ""Web hosting service""],
    [""CRM"", ""component"", ""Customer relationship management system""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
    [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
    [""Azure AI Speech"", ""part-of"", ""Azure OpenAI""],
    [""Azure AI Language"", ""part-of"", ""Azure OpenAI""],
    [""Azure OpenAI"", ""contains"", ""Transcript""],
    [""Azure OpenAI"", ""part-of"", ""App Service""],
    [""App Service"", ""contains"", ""Web application""],
    [""Azure OpenAI"", ""contains"", ""CRM""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech recognition service for transcribing audio files""],
    [""Azure AI Language"", ""system"", ""Natural language processing service for detecting and redacting personal data""],
    [""Azure OpenAI"", ""system"", ""Natural language processing service for processing transcripts""],
    [""Power BI"", ""component"", ""Data visualization tool""],
    [""App Service"", ""component"", ""Web hosting service""],
    [""CRM"", ""component"", ""Customer relationship management system""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
    [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
    [""Azure AI Speech"", ""part-of"", ""Azure OpenAI""],
    [""Azure AI Language"", ""part-of"", ""Azure OpenAI""],
    [""Azure OpenAI"", ""contains"", ""Transcript""],
    [""Azure OpenAI"", ""part-of"", ""App Service""],
    [""App Service"", ""contains"", ""Web application""],
    [""Azure OpenAI"", ""contains"", ""CRM""]
  ]
}
```",4.060586917970795
75,gemma:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API Management Platform""],
    [""Patient API"", ""component"", ""API for managing patients""],
    [""Audit API"", ""component"", ""API for creating audit records""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database""],
    [""CreatePatient"", ""function"", ""Function for creating a patient""],
    [""CreateAuditRecord"", ""function"", ""Function for creating an audit record""]
  ],
  ""relationships"": [
    [""APIM"", ""calls"", ""Patient API""],
    [""Patient API"", ""calls"", ""CreateAuditRecord""],
    [""CreateAuditRecord"", ""part-of"", ""Audit API""],
    [""CreatePatient"", ""part-of"", ""Patient API""],
    [""Patient API"", ""contains"", ""CreatePatient""],
    [""Audit API"", ""contains"", ""CreateAuditRecord""]
  ]
}
```","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API Management Platform""],
    [""Patient API"", ""component"", ""API for managing patients""],
    [""Audit API"", ""component"", ""API for creating audit records""],
    [""Azure Cosmos DB"", ""system"", ""NoSQL database""],
    [""CreatePatient"", ""function"", ""Function for creating a patient""],
    [""CreateAuditRecord"", ""function"", ""Function for creating an audit record""]
  ],
  ""relationships"": [
    [""APIM"", ""calls"", ""Patient API""],
    [""Patient API"", ""calls"", ""CreateAuditRecord""],
    [""CreateAuditRecord"", ""part-of"", ""Audit API""],
    [""CreatePatient"", ""part-of"", ""Patient API""],
    [""Patient API"", ""contains"", ""CreatePatient""],
    [""Audit API"", ""contains"", ""CreateAuditRecord""]
  ]
}
```",2.8955259890062734
76,gemma:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system for suppliers""],
    [""Data Factory"", ""component"", ""Data transformation tool""],
    [""ERP system"", ""system"", ""Purchase order management system""],
    [""Azure SQL Database"", ""system"", ""Relational database""],
    [""Azure Functions"", ""component"", ""Data extraction and transformation tool""],
    [""API"", ""component"", ""Application programming interface""],
    [""Power Apps"", ""component"", ""Desktop and mobile application""],
    [""Suppliers"", ""component"", ""External entities""],
    [""PO data"", ""component"", ""Purchase order data""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""ERP system"", ""contains"", ""PO data""],
    [""Azure Functions"", ""calls"", ""API""],
    [""Azure SQL Database"", ""contains"", ""PO data""],
    [""Power Apps"", ""calls"", ""API""],
    [""Power Apps"", ""part-of"", ""Review process""],
    [""Review process"", ""part-of"", ""PO data""],
    [""Power BI"", ""calls"", ""API""]
  ]
}
```","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system for suppliers""],
    [""Data Factory"", ""component"", ""Data transformation tool""],
    [""ERP system"", ""system"", ""Purchase order management system""],
    [""Azure SQL Database"", ""system"", ""Relational database""],
    [""Azure Functions"", ""component"", ""Data extraction and transformation tool""],
    [""API"", ""component"", ""Application programming interface""],
    [""Power Apps"", ""component"", ""Desktop and mobile application""],
    [""Suppliers"", ""component"", ""External entities""],
    [""PO data"", ""component"", ""Purchase order data""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""ERP system"", ""contains"", ""PO data""],
    [""Azure Functions"", ""calls"", ""API""],
    [""Azure SQL Database"", ""contains"", ""PO data""],
    [""Power Apps"", ""calls"", ""API""],
    [""Power Apps"", ""part-of"", ""Review process""],
    [""Review process"", ""part-of"", ""PO data""],
    [""Power BI"", ""calls"", ""API""]
  ]
}
```",3.486391253012698
77,gemma:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API management tool that accepts HTTP requests""],
    [""Logic Apps"", ""component"", ""Automated workflows that process data""],
    [""Azure Key Vault"", ""system"", ""Cloud-based service for securely storing and retrieving secrets""],
    [""Data Gateway"", ""component"", ""Connects Logic Apps to on-premises SQL Server""],
    [""SQL Server"", ""system"", ""Relational database for storing and retrieving data""]
  ],
  ""relationships"": [
    [""API Management"", ""calls"", ""Logic Apps""],
    [""Logic Apps"", ""calls"", ""Data Gateway""],
    [""Data Gateway"", ""part-of"", ""SQL Server""],
    [""Logic Apps"", ""contains"", ""Azure Key Vault""]
  ]
}
```","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API management tool that accepts HTTP requests""],
    [""Logic Apps"", ""component"", ""Automated workflows that process data""],
    [""Azure Key Vault"", ""system"", ""Cloud-based service for securely storing and retrieving secrets""],
    [""Data Gateway"", ""component"", ""Connects Logic Apps to on-premises SQL Server""],
    [""SQL Server"", ""system"", ""Relational database for storing and retrieving data""]
  ],
  ""relationships"": [
    [""API Management"", ""calls"", ""Logic Apps""],
    [""Logic Apps"", ""calls"", ""Data Gateway""],
    [""Data Gateway"", ""part-of"", ""SQL Server""],
    [""Logic Apps"", ""contains"", ""Azure Key Vault""]
  ]
}
```",2.521554220991675
78,gemma:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity provider""],
    [""Azure Front Door"", ""component"", ""Reverse proxy""],
    [""API Management"", ""system"", ""API gateway""],
    [""App Service Environment"", ""component"", ""Environment""],
    [""Function Apps"", ""component"", ""Environment""],
    [""AKS"", ""component"", ""Environment""],
    [""JWT"", ""component"", ""Authentication token""],
    [""User"", ""person"", ""Application user""],
    [""Client"", ""person"", ""Application""]
  ],
  ""relationships"": [
    [""Client"", ""calls"", ""Azure Front Door""],
    [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure AD B2C"", ""part-of"", ""Client""],
    [""Azure Front Door"", ""part-of"", ""Client""],
    [""API Management"", ""contains"", ""JWT""],
    [""Client"", ""calls"", ""API Management""],
    [""API Management"", ""part-of"", ""Azure Front Door""],
    [""API Management"", ""contains"", ""App Service Environment""],
    [""API Management"", ""contains"", ""Function Apps""],
    [""API Management"", ""contains"", ""AKS""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity provider""],
    [""Azure Front Door"", ""component"", ""Reverse proxy""],
    [""API Management"", ""system"", ""API gateway""],
    [""App Service Environment"", ""component"", ""Environment""],
    [""Function Apps"", ""component"", ""Environment""],
    [""AKS"", ""component"", ""Environment""],
    [""JWT"", ""component"", ""Authentication token""],
    [""User"", ""person"", ""Application user""],
    [""Client"", ""person"", ""Application""]
  ],
  ""relationships"": [
    [""Client"", ""calls"", ""Azure Front Door""],
    [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure AD B2C"", ""part-of"", ""Client""],
    [""Azure Front Door"", ""part-of"", ""Client""],
    [""API Management"", ""contains"", ""JWT""],
    [""Client"", ""calls"", ""API Management""],
    [""API Management"", ""part-of"", ""Azure Front Door""],
    [""API Management"", ""contains"", ""App Service Environment""],
    [""API Management"", ""contains"", ""Function Apps""],
    [""API Management"", ""contains"", ""AKS""]
  ]
}
```",3.9356664849910885
79,gemma:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Custom API"", ""component"", ""API hosted in Azure API Management""],
    [""Custom API Coordinator"", ""person""],
    [""Power Apps"", ""system""],
    [""Microsoft Teams"", ""system""],
    [""Azure Storage Account"", ""system""],
    [""Azure Functions"", ""system""],
    [""Azure Event Hubs"", ""system""],
    [""Azure Data Lake"", ""system""],
    [""Power BI"", ""system""],
    [""Custom Bot Messaging Service"", ""person""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API""],
    [""Custom API Coordinator"", ""contains"", ""Custom API""],
    [""Custom API"", ""calls"", ""Power Apps""],
    [""Custom API"", ""contains"", ""Graph API call""],
    [""Graph API call"", ""part-of"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API call""],
    [""Custom Bot Messaging Service"", ""contains"", ""Azure Bot Service""],
    [""Custom Bot Messaging Service"", ""calls"", ""Custom Bot""],
    [""Custom Bot"", ""part-of"", ""Microsoft Teams""],
    [""Azure Event Hubs"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""contains"", ""Events from Event Hubs""]
  ]
}
```","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Custom API"", ""component"", ""API hosted in Azure API Management""],
    [""Custom API Coordinator"", ""person""],
    [""Power Apps"", ""system""],
    [""Microsoft Teams"", ""system""],
    [""Azure Storage Account"", ""system""],
    [""Azure Functions"", ""system""],
    [""Azure Event Hubs"", ""system""],
    [""Azure Data Lake"", ""system""],
    [""Power BI"", ""system""],
    [""Custom Bot Messaging Service"", ""person""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API""],
    [""Custom API Coordinator"", ""contains"", ""Custom API""],
    [""Custom API"", ""calls"", ""Power Apps""],
    [""Custom API"", ""contains"", ""Graph API call""],
    [""Graph API call"", ""part-of"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API call""],
    [""Custom Bot Messaging Service"", ""contains"", ""Azure Bot Service""],
    [""Custom Bot Messaging Service"", ""calls"", ""Custom Bot""],
    [""Custom Bot"", ""part-of"", ""Microsoft Teams""],
    [""Azure Event Hubs"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""contains"", ""Events from Event Hubs""]
  ]
}
```",4.15241199900629
80,gemma:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}","```json
{
  ""entities"": [
    [ ""GitHub"", ""system"", ""Version control system"" ],
    [ ""CI/CD pipeline agent"", ""component"", ""Continuous integration and continuous delivery pipeline agent"" ],
    [ ""Azure VM"", ""component"", ""Virtual Machine"" ],
    [ ""API application"", ""component"", ""Application"" ],
    [ ""ILB ASE"", ""system"", ""Internal load balancer application service environment"" ],
    [ ""Azure API Management"", ""system"", ""API gateway"" ],
    [ ""Application Gateway"", ""component"", ""Reverse proxy"" ],
    [ ""Azure Private DNS"", ""component"", ""Private domain name system"" ]
  ],
  ""relationships"": [
    [ ""GitHub"", ""contains"", ""CI/CD pipeline agent"" ],
    [ ""CI/CD pipeline agent"", ""part-of"", ""Azure VM"" ],
    [ ""Azure VM"", ""contains"", ""API application"" ],
    [ ""API application"", ""part-of"", ""ILB ASE"" ],
    [ ""Azure API Management"", ""contains"", ""API Management policy"" ],
    [ ""Azure API Management"", ""part-of"", ""ILB ASE"" ],
    [ ""Application Gateway"", ""part-of"", ""Azure API Management"" ],
    [ ""Azure API Management"", ""calls"", ""API application"" ],
    [ ""API application"", ""called-from"", ""Azure API Management"" ]
  ]
}
```","```json
{
  ""entities"": [
    [ ""GitHub"", ""system"", ""Version control system"" ],
    [ ""CI/CD pipeline agent"", ""component"", ""Continuous integration and continuous delivery pipeline agent"" ],
    [ ""Azure VM"", ""component"", ""Virtual Machine"" ],
    [ ""API application"", ""component"", ""Application"" ],
    [ ""ILB ASE"", ""system"", ""Internal load balancer application service environment"" ],
    [ ""Azure API Management"", ""system"", ""API gateway"" ],
    [ ""Application Gateway"", ""component"", ""Reverse proxy"" ],
    [ ""Azure Private DNS"", ""component"", ""Private domain name system"" ]
  ],
  ""relationships"": [
    [ ""GitHub"", ""contains"", ""CI/CD pipeline agent"" ],
    [ ""CI/CD pipeline agent"", ""part-of"", ""Azure VM"" ],
    [ ""Azure VM"", ""contains"", ""API application"" ],
    [ ""API application"", ""part-of"", ""ILB ASE"" ],
    [ ""Azure API Management"", ""contains"", ""API Management policy"" ],
    [ ""Azure API Management"", ""part-of"", ""ILB ASE"" ],
    [ ""Application Gateway"", ""part-of"", ""Azure API Management"" ],
    [ ""Azure API Management"", ""calls"", ""API application"" ],
    [ ""API application"", ""called-from"", ""Azure API Management"" ]
  ]
}
```",4.222638967970852
81,gemma:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Publisher"", ""component"", ""Manage data migration (mainframe)""],
    [""Listener"", ""component"", ""Manage data migration (midrange)""],
    [""Enhanced-security connection"", ""system""],
    [""Replicator Engine"", ""component"", ""Capture and apply data""],
    [""Target"", ""system"", ""Database""],
    [""Azure Databricks"", ""system""],
    [""Azure data platform services"", ""system""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""part-of"", ""Publisher"", ""Enhanced-security connection""],
    [""calls"", ""Publisher"", ""Replicator Engine""],
    [""called-from"", ""Replicator Engine"", ""Publisher""],
    [""contains"", ""Connect agent component"", ""Listener""],
    [""part-of"", ""Listener"", ""Enhanced-security connection""],
    [""calls"", ""Listener"", ""Replicator Engine""],
    [""called-from"", ""Replicator Engine"", ""Listener""],
    [""contains"", ""Target"", ""Azure Databricks""],
    [""contains"", ""Target"", ""Azure data platform services""]
  ]
}
```","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Publisher"", ""component"", ""Manage data migration (mainframe)""],
    [""Listener"", ""component"", ""Manage data migration (midrange)""],
    [""Enhanced-security connection"", ""system""],
    [""Replicator Engine"", ""component"", ""Capture and apply data""],
    [""Target"", ""system"", ""Database""],
    [""Azure Databricks"", ""system""],
    [""Azure data platform services"", ""system""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""part-of"", ""Publisher"", ""Enhanced-security connection""],
    [""calls"", ""Publisher"", ""Replicator Engine""],
    [""called-from"", ""Replicator Engine"", ""Publisher""],
    [""contains"", ""Connect agent component"", ""Listener""],
    [""part-of"", ""Listener"", ""Enhanced-security connection""],
    [""calls"", ""Listener"", ""Replicator Engine""],
    [""called-from"", ""Replicator Engine"", ""Listener""],
    [""contains"", ""Target"", ""Azure Databricks""],
    [""contains"", ""Target"", ""Azure data platform services""]
  ]
}
```",3.6760395760065876
82,gemma:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Cloud-based data integration service""],
    [""Blob storage"", ""component"", ""Storage for data""],
    [""Microsoft-managed keys"", ""component"", ""Encryption keys""],
    [""Data science VMs"", ""component"", ""Virtual machines for research""],
    [""Azure Machine Learning"", ""system"", ""Machine learning platform""],
    [""Logic app"", ""component"", ""Automated workflow""],
    [""Data"", ""component"", ""Set of information""],
    [""Researchers"", ""person""],
    [""Data owners"", ""person""]
  ],
  ""relationships"": [
    [""Data Factory"", ""contains"", ""Blob storage""],
    [""Data Factory"", ""contains"", ""Microsoft-managed keys""],
    [""Azure Data Factory"", ""calls"", ""Logic app""],
    [""Logic app"", ""calls"", ""Researchers""],
    [""Azure Data Factory"", ""part-of"", ""Secure environment""],
    [""Data science VMs"", ""part-of"", ""Secure environment""],
    [""Azure Machine Learning"", ""part-of"", ""Secure environment""],
    [""Secure environment"", ""contains"", ""Data""],
    [""Secure environment"", ""contains"", ""Azure Machine Learning""],
    [""Logic app"", ""called-from"", ""Data""],
    [""Data"", ""calls"", ""Data Factory""],
    [""Data Factory"", ""contains"", ""Data""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Data Factory"", ""system"", ""Cloud-based data integration service""],
    [""Blob storage"", ""component"", ""Storage for data""],
    [""Microsoft-managed keys"", ""component"", ""Encryption keys""],
    [""Data science VMs"", ""component"", ""Virtual machines for research""],
    [""Azure Machine Learning"", ""system"", ""Machine learning platform""],
    [""Logic app"", ""component"", ""Automated workflow""],
    [""Data"", ""component"", ""Set of information""],
    [""Researchers"", ""person""],
    [""Data owners"", ""person""]
  ],
  ""relationships"": [
    [""Data Factory"", ""contains"", ""Blob storage""],
    [""Data Factory"", ""contains"", ""Microsoft-managed keys""],
    [""Azure Data Factory"", ""calls"", ""Logic app""],
    [""Logic app"", ""calls"", ""Researchers""],
    [""Azure Data Factory"", ""part-of"", ""Secure environment""],
    [""Data science VMs"", ""part-of"", ""Secure environment""],
    [""Azure Machine Learning"", ""part-of"", ""Secure environment""],
    [""Secure environment"", ""contains"", ""Data""],
    [""Secure environment"", ""contains"", ""Azure Machine Learning""],
    [""Logic app"", ""called-from"", ""Data""],
    [""Data"", ""calls"", ""Data Factory""],
    [""Data Factory"", ""contains"", ""Data""]
  ]
}
```",4.228386247996241
83,gemma:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Trigger by HTTP requests, nest workflows""],
    [""Azure API Management"", ""system"", ""API gateway and Developer portal""],
    [""Azure DNS"", ""system"", ""Name resolution""],
    [""Microsoft Entra ID"", ""component"", ""Authentication""],
    [""API gateway"", ""component"", ""Accepts HTTP calls""],
    [""Developer portal"", ""component"", ""Documentation, code samples""],
    [""Custom domain name"", ""string"", "" contoso.com""],
    [""Azure"", ""system""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""calls"", ""API gateway""],
    [""API gateway"", ""calls"", ""Microsoft Entra ID""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure DNS"", ""part-of"", ""Azure""],
    [""Microsoft Entra ID"", ""part-of"", ""Azure""],
    [""Azure Logic Apps"", ""contains"", ""Connectors""],
    [""Azure API Management"", ""calls"", ""Azure DNS""],
    [""Azure DNS"", ""part-of"", ""Azure API Management""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Trigger by HTTP requests, nest workflows""],
    [""Azure API Management"", ""system"", ""API gateway and Developer portal""],
    [""Azure DNS"", ""system"", ""Name resolution""],
    [""Microsoft Entra ID"", ""component"", ""Authentication""],
    [""API gateway"", ""component"", ""Accepts HTTP calls""],
    [""Developer portal"", ""component"", ""Documentation, code samples""],
    [""Custom domain name"", ""string"", "" contoso.com""],
    [""Azure"", ""system""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""calls"", ""API gateway""],
    [""API gateway"", ""calls"", ""Microsoft Entra ID""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure DNS"", ""part-of"", ""Azure""],
    [""Microsoft Entra ID"", ""part-of"", ""Azure""],
    [""Azure Logic Apps"", ""contains"", ""Connectors""],
    [""Azure API Management"", ""calls"", ""Azure DNS""],
    [""Azure DNS"", ""part-of"", ""Azure API Management""]
  ]
}
```",3.688193866983056
84,gemma:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API Management system that accepts API calls""],
    [""Logic Apps"", ""component"", ""Orchestrator for automated workflows""],
    [""Azure Key Vault"", ""system"", ""Cloud-based service for securely storing and retrieving secrets""],
    [""Transport Layer Security"", ""component"", ""Security protocol for encrypting data in transit""],
    [""Data Gateway"", ""component"", ""Connects Azure services to on-premises resources""],
    [""SQL Server"", ""system"", ""Relational database management system""],
    [""Azure Monitor"", ""system"", ""Monitoring service for collecting and analyzing data""]
  ],
  ""relationships"": [
    [""API Management"", ""calls"", ""Logic Apps""],
    [""Logic Apps"", ""contains"", ""Azure Key Vault""],
    [""Logic Apps"", ""contains"", ""Transport Layer Security""],
    [""Logic Apps"", ""contains"", ""Data Gateway""],
    [""Data Gateway"", ""part-of"", ""SQL Server""],
    [""Logic Apps"", ""calls"", ""Azure Monitor""],
    [""Azure Monitor"", ""called-from"", ""Logic Apps""]
  ]
}
```","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API Management system that accepts API calls""],
    [""Logic Apps"", ""component"", ""Orchestrator for automated workflows""],
    [""Azure Key Vault"", ""system"", ""Cloud-based service for securely storing and retrieving secrets""],
    [""Transport Layer Security"", ""component"", ""Security protocol for encrypting data in transit""],
    [""Data Gateway"", ""component"", ""Connects Azure services to on-premises resources""],
    [""SQL Server"", ""system"", ""Relational database management system""],
    [""Azure Monitor"", ""system"", ""Monitoring service for collecting and analyzing data""]
  ],
  ""relationships"": [
    [""API Management"", ""calls"", ""Logic Apps""],
    [""Logic Apps"", ""contains"", ""Azure Key Vault""],
    [""Logic Apps"", ""contains"", ""Transport Layer Security""],
    [""Logic Apps"", ""contains"", ""Data Gateway""],
    [""Data Gateway"", ""part-of"", ""SQL Server""],
    [""Logic Apps"", ""calls"", ""Azure Monitor""],
    [""Azure Monitor"", ""called-from"", ""Logic Apps""]
  ]
}
```",3.291715376020875
85,gemma:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}","```json
{
  ""entities"": [
    [ ""Azure Data Factory"", ""system"", ""Data processing and transformation tool"" ],
    [ ""Microsoft-managed keys"", ""component"", ""Encryption keys managed by Microsoft"" ],
    [ ""Secure storage account"", ""component"", ""Storage account with security controls"" ],
    [ ""Streaming application"", ""component"", ""Application for accessing secure environment"" ],
    [ ""Azure Virtual Desktop"", ""component"", ""Remote desktop software for accessing secure environment"" ],
    [ ""Data science VMs"", ""component"", ""Virtual machines for data preparation"" ],
    [ ""Azure Machine Learning compute"", ""component"", ""Machine learning model training and deployment platform"" ],
    [ ""Logic app"", ""component"", ""Automated workflow for sending notifications"" ],
    [ ""Data"", ""system"", ""Datasets uploaded to public storage"" ],
    [ ""Models"", ""system"", ""Machine learning models trained from the dataset"" ]
  ],
  ""relationships"": [
    [ ""contains"", ""Azure Data Factory"", ""Microsoft-managed keys"" ],
    [ ""part-of"", ""Secure storage account"", ""Azure Virtual Desktop"" ],
    [ ""contains"", ""Secure storage account"", ""Data"" ],
    [ ""contains"", ""Secure storage account"", ""Models"" ],
    [ ""calls"", ""Logic app"", ""Streaming application"" ],
    [ ""called-from"", ""Streaming application"", ""Logic app"" ],
    [ ""part-of"", ""Secure environment"", ""Azure Machine Learning compute"" ],
    [ ""contains"", ""Secure storage account"", ""Data Factory"" ],
    [ ""part-of"", ""Public storage account"", ""Data Factory"" ]
  ]
}
```","```json
{
  ""entities"": [
    [ ""Azure Data Factory"", ""system"", ""Data processing and transformation tool"" ],
    [ ""Microsoft-managed keys"", ""component"", ""Encryption keys managed by Microsoft"" ],
    [ ""Secure storage account"", ""component"", ""Storage account with security controls"" ],
    [ ""Streaming application"", ""component"", ""Application for accessing secure environment"" ],
    [ ""Azure Virtual Desktop"", ""component"", ""Remote desktop software for accessing secure environment"" ],
    [ ""Data science VMs"", ""component"", ""Virtual machines for data preparation"" ],
    [ ""Azure Machine Learning compute"", ""component"", ""Machine learning model training and deployment platform"" ],
    [ ""Logic app"", ""component"", ""Automated workflow for sending notifications"" ],
    [ ""Data"", ""system"", ""Datasets uploaded to public storage"" ],
    [ ""Models"", ""system"", ""Machine learning models trained from the dataset"" ]
  ],
  ""relationships"": [
    [ ""contains"", ""Azure Data Factory"", ""Microsoft-managed keys"" ],
    [ ""part-of"", ""Secure storage account"", ""Azure Virtual Desktop"" ],
    [ ""contains"", ""Secure storage account"", ""Data"" ],
    [ ""contains"", ""Secure storage account"", ""Models"" ],
    [ ""calls"", ""Logic app"", ""Streaming application"" ],
    [ ""called-from"", ""Streaming application"", ""Logic app"" ],
    [ ""part-of"", ""Secure environment"", ""Azure Machine Learning compute"" ],
    [ ""contains"", ""Secure storage account"", ""Data Factory"" ],
    [ ""part-of"", ""Public storage account"", ""Data Factory"" ]
  ]
}
```",4.926034139993135
86,gemma:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF attachments""],
    [""Logic App"", ""component"", ""Logic app for processing email attachments""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud storage for PDF files""],
    [""Function App"", ""component"", ""Function app for processing PDF forms""],
    [""AI Document Intelligence"", ""system"", ""Microsoft's AI-powered document processing service""],
    [""Cosmos DB"", ""system"", ""NoSQL database for storing processed data""],
    [""Power BI"", ""person"", ""Business intelligence and data analytics platform""]
  ],
  ""relationships"": [
    [""contains"", ""Logic App"", ""Outlook email account""],
    [""part-of"", ""PDF container"", ""Azure Data Lake Storage""],
    [""calls"", ""Logic App"", ""Function App""],
    [""called-from"", ""Function App"", ""AI Document Intelligence""],
    [""contains"", ""Data Lake Storage"", ""PDF container""],
    [""contains"", ""Data Lake Storage"", ""Single-page PDF container""],
    [""part-of"", ""Data Lake Storage"", ""Cosmos DB""],
    [""calls"", ""Cosmos DB"", ""Power BI""]
  ]
}
```","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF attachments""],
    [""Logic App"", ""component"", ""Logic app for processing email attachments""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud storage for PDF files""],
    [""Function App"", ""component"", ""Function app for processing PDF forms""],
    [""AI Document Intelligence"", ""system"", ""Microsoft's AI-powered document processing service""],
    [""Cosmos DB"", ""system"", ""NoSQL database for storing processed data""],
    [""Power BI"", ""person"", ""Business intelligence and data analytics platform""]
  ],
  ""relationships"": [
    [""contains"", ""Logic App"", ""Outlook email account""],
    [""part-of"", ""PDF container"", ""Azure Data Lake Storage""],
    [""calls"", ""Logic App"", ""Function App""],
    [""called-from"", ""Function App"", ""AI Document Intelligence""],
    [""contains"", ""Data Lake Storage"", ""PDF container""],
    [""contains"", ""Data Lake Storage"", ""Single-page PDF container""],
    [""part-of"", ""Data Lake Storage"", ""Cosmos DB""],
    [""calls"", ""Cosmos DB"", ""Power BI""]
  ]
}
```",3.7190111950039864
87,gemma:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""Version control system""],
    [""Azure VM"", ""component"", ""Virtual Machine""],
    [""API application"", ""component"", ""Software application""],
    [""Azure API Management"", ""system"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Traffic routing""],
    [""Internal DNS"", ""system"", ""Domain name system""],
    [""Public IP"", ""component"", ""Network address""],
    [""Developers"", ""person""],
    [""Pipeline agent"", ""component""],
  ],
  ""relationships"": [
    [""Develops"", ""person"", ""Pipeline agent""],
    [""Contains"", ""Pipeline agent"", ""GitHub""],
    [""Calls"", ""Pipeline agent"", ""API application""],
    [""Consumed by"", ""Azure API Management"", ""API application""],
    [""Uses"", ""Azure API Management"", ""Internal DNS""],
    [""Exposes"", ""Application Gateway"", ""Azure API Management""],
    [""Routes traffic through"", ""Application Gateway"", ""Public IP""]
  ]
}
```","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""Version control system""],
    [""Azure VM"", ""component"", ""Virtual Machine""],
    [""API application"", ""component"", ""Software application""],
    [""Azure API Management"", ""system"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Traffic routing""],
    [""Internal DNS"", ""system"", ""Domain name system""],
    [""Public IP"", ""component"", ""Network address""],
    [""Developers"", ""person""],
    [""Pipeline agent"", ""component""],
  ],
  ""relationships"": [
    [""Develops"", ""person"", ""Pipeline agent""],
    [""Contains"", ""Pipeline agent"", ""GitHub""],
    [""Calls"", ""Pipeline agent"", ""API application""],
    [""Consumed by"", ""Azure API Management"", ""API application""],
    [""Uses"", ""Azure API Management"", ""Internal DNS""],
    [""Exposes"", ""Application Gateway"", ""Azure API Management""],
    [""Routes traffic through"", ""Application Gateway"", ""Public IP""]
  ]
}
```",3.1976742029655725
88,gemma:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Azure API Management"", ""system"", ""Custom API hosting platform""],
    [""Custom API Coordinator"", ""component"", ""Notification handler""],
    [""Power Apps"", ""system"", ""Flight assignment platform""],
    [""Microsoft Teams"", ""system"", ""Collaboration platform""],
    [""Azure Storage Account"", ""system"", ""Message queue""],
    [""Azure Functions"", ""component"", ""Graph API processor""],
    [""Azure Event Hubs"", ""system"", ""Event streaming platform""],
    [""Azure Data Lake"", ""system"", ""Data storage""],
    [""Power BI"", ""system"", ""Analytics platform""],
    [""Custom Bot Messaging Service"", ""component"", ""Bot service""],
    [""Custom Bots"", ""component"", ""Flight update sender""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API Coordinator""],
    [""Custom API Coordinator"", ""calls"", ""Power Apps""],
    [""Custom API Coordinator"", ""part-of"", ""Azure API Management""],
    [""Custom API Coordinator"", ""contains"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Graph API"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""contains"", ""Power BI""],
    [""Custom Bot Messaging Service"", ""part-of"", ""Airline System""],
    [""Custom Bot Messaging Service"", ""calls"", ""Custom Bots""],
    [""Custom Bots"", ""part-of"", ""Microsoft Teams""]
  ]
}
```","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Azure API Management"", ""system"", ""Custom API hosting platform""],
    [""Custom API Coordinator"", ""component"", ""Notification handler""],
    [""Power Apps"", ""system"", ""Flight assignment platform""],
    [""Microsoft Teams"", ""system"", ""Collaboration platform""],
    [""Azure Storage Account"", ""system"", ""Message queue""],
    [""Azure Functions"", ""component"", ""Graph API processor""],
    [""Azure Event Hubs"", ""system"", ""Event streaming platform""],
    [""Azure Data Lake"", ""system"", ""Data storage""],
    [""Power BI"", ""system"", ""Analytics platform""],
    [""Custom Bot Messaging Service"", ""component"", ""Bot service""],
    [""Custom Bots"", ""component"", ""Flight update sender""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API Coordinator""],
    [""Custom API Coordinator"", ""calls"", ""Power Apps""],
    [""Custom API Coordinator"", ""part-of"", ""Azure API Management""],
    [""Custom API Coordinator"", ""contains"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Graph API"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""contains"", ""Power BI""],
    [""Custom Bot Messaging Service"", ""part-of"", ""Airline System""],
    [""Custom Bot Messaging Service"", ""calls"", ""Custom Bots""],
    [""Custom Bots"", ""part-of"", ""Microsoft Teams""]
  ]
}
```",4.593245017051231
89,gemma:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}","```json
{
  ""entities"": [
    [""Connect Agent"", ""component"", ""Capture change logs from mainframe or midrange systems and cache them in temporary storage.""],
    [""Publisher"", ""component"", ""Manages data migration for mainframe systems.""],
    [""Listener"", ""component"", ""Manages data migration for midrange systems.""],
    [""Enhanced-security connection"", ""system"", ""Provides secure connection between on-premises and Azure""],
    [""Replicator Engine"", ""system"", ""Captures data from publisher or listener and applies it to the target""],
    [""Target"", ""system"", ""Database that receives changes via ODBC or ingests changes via Azure Event Hubs""],
    [""Azure Databricks"", ""system"", ""Consume changes from Target""],
    [""Azure Data Platform services"", ""system"", ""Receive changes from Azure Databricks""],
    [""Controller Daemon"", ""person"", ""Authenticates request and establishes socket connection""]
  ],
  ""relationships"": [
    [""Publisher"", ""calls"", ""Enhanced-security connection""],
    [""Listener"", ""calls"", ""Enhanced-security connection""],
    [""Enhanced-security connection"", ""part-of"", ""Publisher""],
    [""Enhanced-security connection"", ""part-of"", ""Listener""],
    [""Replicator Engine"", ""calls"", ""Publisher""],
    [""Replicator Engine"", ""calls"", ""Listener""],
    [""Publisher"", ""contains"", ""Data migration""],
    [""Listener"", ""contains"", ""Data migration""],
    [""Replicator Engine"", ""contains"", ""Data capture""],
    [""Target"", ""contains"", ""Changes""],
    [""Azure Data Platform services"", ""contains"", ""Changes from Azure Databricks""]
  ]
}
```","```json
{
  ""entities"": [
    [""Connect Agent"", ""component"", ""Capture change logs from mainframe or midrange systems and cache them in temporary storage.""],
    [""Publisher"", ""component"", ""Manages data migration for mainframe systems.""],
    [""Listener"", ""component"", ""Manages data migration for midrange systems.""],
    [""Enhanced-security connection"", ""system"", ""Provides secure connection between on-premises and Azure""],
    [""Replicator Engine"", ""system"", ""Captures data from publisher or listener and applies it to the target""],
    [""Target"", ""system"", ""Database that receives changes via ODBC or ingests changes via Azure Event Hubs""],
    [""Azure Databricks"", ""system"", ""Consume changes from Target""],
    [""Azure Data Platform services"", ""system"", ""Receive changes from Azure Databricks""],
    [""Controller Daemon"", ""person"", ""Authenticates request and establishes socket connection""]
  ],
  ""relationships"": [
    [""Publisher"", ""calls"", ""Enhanced-security connection""],
    [""Listener"", ""calls"", ""Enhanced-security connection""],
    [""Enhanced-security connection"", ""part-of"", ""Publisher""],
    [""Enhanced-security connection"", ""part-of"", ""Listener""],
    [""Replicator Engine"", ""calls"", ""Publisher""],
    [""Replicator Engine"", ""calls"", ""Listener""],
    [""Publisher"", ""contains"", ""Data migration""],
    [""Listener"", ""contains"", ""Data migration""],
    [""Replicator Engine"", ""contains"", ""Data capture""],
    [""Target"", ""contains"", ""Changes""],
    [""Azure Data Platform services"", ""contains"", ""Changes from Azure Databricks""]
  ]
}
```",4.747496992000379
90,gemma:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech-to-text transcription service""],
    [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
    [""Azure OpenAI"", ""system"", ""Conversational analysis and summarization service""],
    [""Ingestion Client tool"", ""component"", ""No-code tool for call center transcription""],
    [""Power BI"", ""component"", ""Data visualization tool""],
    [""App Service"", ""component"", ""Web application hosting service""],
    [""Customer"", ""person""],
    [""Agent"", ""person""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
    [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
    [""Ingestion Client tool"", ""calls"", ""Azure AI Speech""],
    [""Azure AI Speech"", ""contains"", ""Transcription results""],
    [""Azure AI Language"", ""contains"", ""Redacted personal data""],
    [""Azure OpenAI"", ""contains"", ""Processed transcript""],
    [""Azure OpenAI"", ""part-of"", ""Azure AI Speech""],
    [""Power BI"", ""calls"", ""Azure OpenAI""],
    [""App Service"", ""calls"", ""Power BI""],
    [""Azure OpenAI"", ""contains"", ""Conversation summary""],
    [""Azure OpenAI"", ""contains"", ""Sentiment analysis""],
    [""Azure OpenAI"", ""part-of"", ""Call analysis""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech-to-text transcription service""],
    [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
    [""Azure OpenAI"", ""system"", ""Conversational analysis and summarization service""],
    [""Ingestion Client tool"", ""component"", ""No-code tool for call center transcription""],
    [""Power BI"", ""component"", ""Data visualization tool""],
    [""App Service"", ""component"", ""Web application hosting service""],
    [""Customer"", ""person""],
    [""Agent"", ""person""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
    [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
    [""Ingestion Client tool"", ""calls"", ""Azure AI Speech""],
    [""Azure AI Speech"", ""contains"", ""Transcription results""],
    [""Azure AI Language"", ""contains"", ""Redacted personal data""],
    [""Azure OpenAI"", ""contains"", ""Processed transcript""],
    [""Azure OpenAI"", ""part-of"", ""Azure AI Speech""],
    [""Power BI"", ""calls"", ""Azure OpenAI""],
    [""App Service"", ""calls"", ""Power BI""],
    [""Azure OpenAI"", ""contains"", ""Conversation summary""],
    [""Azure OpenAI"", ""contains"", ""Sentiment analysis""],
    [""Azure OpenAI"", ""part-of"", ""Call analysis""]
  ]
}
```",4.968310003983788
91,gemma:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform triggered by HTTP requests""],
    [""Azure API Management"", ""system"", ""API gateway and developer portal""],
    [""Azure DNS"", ""system"", ""Name resolution service""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service""],
    [""API gateway"", ""component"", ""Accepts HTTP calls""],
    [""Developer portal"", ""component"", ""Access to documentation and code samples""],
    [""Custom domain name"", ""component"", ""Domain name associated with an IP address""],
    [""Standard or Premium tier"", ""component"", ""Tier of API Management with enhanced security""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""contains"", ""Workflow""],
    [""Azure Logic Apps"", ""calls"", ""Connectors""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure API Management"", ""part-of"", ""Microsoft Entra ID""],
    [""Azure DNS"", ""part-of"", ""Azure infrastructure""],
    [""Microsoft Entra ID"", ""contains"", ""OpenID Connect""],
    [""API gateway"", ""calls"", ""Microsoft Entra ID""],
    [""Microsoft Entra ID"", ""calls"", ""API Gateway""],
    [""API Management"", ""part-of"", ""Standard or Premium tier""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform triggered by HTTP requests""],
    [""Azure API Management"", ""system"", ""API gateway and developer portal""],
    [""Azure DNS"", ""system"", ""Name resolution service""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service""],
    [""API gateway"", ""component"", ""Accepts HTTP calls""],
    [""Developer portal"", ""component"", ""Access to documentation and code samples""],
    [""Custom domain name"", ""component"", ""Domain name associated with an IP address""],
    [""Standard or Premium tier"", ""component"", ""Tier of API Management with enhanced security""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""contains"", ""Workflow""],
    [""Azure Logic Apps"", ""calls"", ""Connectors""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure API Management"", ""part-of"", ""Microsoft Entra ID""],
    [""Azure DNS"", ""part-of"", ""Azure infrastructure""],
    [""Microsoft Entra ID"", ""contains"", ""OpenID Connect""],
    [""API gateway"", ""calls"", ""Microsoft Entra ID""],
    [""Microsoft Entra ID"", ""calls"", ""API Gateway""],
    [""API Management"", ""part-of"", ""Standard or Premium tier""]
  ]
}
```",4.276168829004746
92,gemma:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}","```json
{
  ""entities"": [
    [ ""Azure AD B2C"", ""system"", ""Azure Active Directory B2C authentication service"" ],
    [ ""Azure Front Door"", ""component"", ""Traffic routing service"" ],
    [ ""API Management"", ""component"", ""API gateway"" ],
    [ ""App Service Environment"", ""component"", ""Platform for deploying web apps"" ],
    [ ""Function Apps"", ""component"", ""Serverless computing platform"" ],
    [ ""AKS"", ""component"", ""Kubernetes cluster"" ]
  ],
  ""relationships"": [
    [ ""calls"", ""Azure Front Door"", ""Azure AD B2C"" ],
    [ ""contains"", ""Azure AD B2C"", ""JWT bearer token"" ],
    [ ""part-of"", ""API Management"", ""Azure Front Door"" ],
    [ ""contains"", ""API Management"", ""validate-jwt policy"" ],
    [ ""part-of"", ""App Service Environment"", ""backend API"" ],
    [ ""part-of"", ""Function Apps"", ""backend API"" ],
    [ ""part-of"", ""AKS"", ""backend API"" ]
  ]
}
```","```json
{
  ""entities"": [
    [ ""Azure AD B2C"", ""system"", ""Azure Active Directory B2C authentication service"" ],
    [ ""Azure Front Door"", ""component"", ""Traffic routing service"" ],
    [ ""API Management"", ""component"", ""API gateway"" ],
    [ ""App Service Environment"", ""component"", ""Platform for deploying web apps"" ],
    [ ""Function Apps"", ""component"", ""Serverless computing platform"" ],
    [ ""AKS"", ""component"", ""Kubernetes cluster"" ]
  ],
  ""relationships"": [
    [ ""calls"", ""Azure Front Door"", ""Azure AD B2C"" ],
    [ ""contains"", ""Azure AD B2C"", ""JWT bearer token"" ],
    [ ""part-of"", ""API Management"", ""Azure Front Door"" ],
    [ ""contains"", ""API Management"", ""validate-jwt policy"" ],
    [ ""part-of"", ""App Service Environment"", ""backend API"" ],
    [ ""part-of"", ""Function Apps"", ""backend API"" ],
    [ ""part-of"", ""AKS"", ""backend API"" ]
  ]
}
```",3.461353716033045
93,gemma:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}","```json
{
  ""entities"": [
    [ ""APIM"", ""system"", ""Outside services and clients make a POST request to APIM"" ],
    [ ""Patient API"", ""component"", ""APIM calls the CreatePatient function in the Patient API"" ],
    [ ""CreatePatient"", ""function"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app"" ],
    [ ""Audit API"", ""component"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB"" ],
    [ ""Azure Cosmos DB"", ""database"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB"" ],
    [ ""CreateAuditRecord"", ""function"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB"" ]
  ],
  ""relationships"": [
    [ ""calls"", ""APIM"", ""Patient API"" ],
    [ ""calls"", ""Patient API"", ""CreatePatient"" ],
    [ ""calls"", ""CreatePatient"", ""Audit API"" ],
    [ ""contains"", ""Audit API"", ""CreateAuditRecord"" ],
    [ ""part-of"", ""Patient API"", ""Audit API"" ]
  ]
}
```","```json
{
  ""entities"": [
    [ ""APIM"", ""system"", ""Outside services and clients make a POST request to APIM"" ],
    [ ""Patient API"", ""component"", ""APIM calls the CreatePatient function in the Patient API"" ],
    [ ""CreatePatient"", ""function"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app"" ],
    [ ""Audit API"", ""component"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB"" ],
    [ ""Azure Cosmos DB"", ""database"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB"" ],
    [ ""CreateAuditRecord"", ""function"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB"" ]
  ],
  ""relationships"": [
    [ ""calls"", ""APIM"", ""Patient API"" ],
    [ ""calls"", ""Patient API"", ""CreatePatient"" ],
    [ ""calls"", ""CreatePatient"", ""Audit API"" ],
    [ ""contains"", ""Audit API"", ""CreateAuditRecord"" ],
    [ ""part-of"", ""Patient API"", ""Audit API"" ]
  ]
}
```",3.663663178042043
94,gemma:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}","```json
{
""entities"": [
  [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
  [""Command Message"", ""system"", ""Message containing metadata""],
  [""Durable Functions"", ""system"", ""Orchestration framework""],
  [""Computer Vision Read API"", ""component"", ""Optical Character Recognition API""],
  [""Document Classifier Service"", ""component"", ""Classification service""],
  [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
  [""Azure Cosmos DB"", ""system"", ""Database for storing data""],
  [""Cognitive Search"", ""system"", ""Search engine""],
  [""Correlation ID"", ""component"", ""Unique identifier""]
],
""relationships"": [
  [""Command Message"", ""calls"", ""Durable Functions""],
  [""Durable Functions"", ""calls"", ""Computer Vision Read API""],
  [""Computer Vision Read API"", ""returns"", ""Durable Functions""],
  [""Durable Functions"", ""calls"", ""Classify activity""],
  [""Classify activity"", ""calls"", ""Document Classifier Service""],
  [""Document Classifier Service"", ""returns"", ""Classify activity""],
  [""Durable Functions"", ""calls"", ""Metadata Store activity""],
  [""Durable Functions"", ""calls"", ""Indexing activity""],
  [""Indexing activity"", ""creates"", ""Cognitive Search""],
  [""Indexing activity"", ""stores"", ""Correlation ID""],
  [""Correlation ID"", ""linked-to"", ""Azure Cosmos DB""]
]
}
```","```json
{
""entities"": [
  [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
  [""Command Message"", ""system"", ""Message containing metadata""],
  [""Durable Functions"", ""system"", ""Orchestration framework""],
  [""Computer Vision Read API"", ""component"", ""Optical Character Recognition API""],
  [""Document Classifier Service"", ""component"", ""Classification service""],
  [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
  [""Azure Cosmos DB"", ""system"", ""Database for storing data""],
  [""Cognitive Search"", ""system"", ""Search engine""],
  [""Correlation ID"", ""component"", ""Unique identifier""]
],
""relationships"": [
  [""Command Message"", ""calls"", ""Durable Functions""],
  [""Durable Functions"", ""calls"", ""Computer Vision Read API""],
  [""Computer Vision Read API"", ""returns"", ""Durable Functions""],
  [""Durable Functions"", ""calls"", ""Classify activity""],
  [""Classify activity"", ""calls"", ""Document Classifier Service""],
  [""Document Classifier Service"", ""returns"", ""Classify activity""],
  [""Durable Functions"", ""calls"", ""Metadata Store activity""],
  [""Durable Functions"", ""calls"", ""Indexing activity""],
  [""Indexing activity"", ""creates"", ""Cognitive Search""],
  [""Indexing activity"", ""stores"", ""Correlation ID""],
  [""Correlation ID"", ""linked-to"", ""Azure Cosmos DB""]
]
}
```",4.34766721399501
95,gemma:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system for suppliers""],
    [""Data Factory"", ""component"", ""Data extraction tool""],
    [""ERP system"", ""system"", ""Purchase order management system""],
    [""Azure SQL Database"", ""system"", ""Relational database""],
    [""Azure Functions"", ""component"", ""API creation tool""],
    [""Power Apps"", ""component"", ""Data review and update application""],
    [""Suppliers"", ""component"", ""External entities receiving purchase order updates""],
    [""PO data"", ""component"", ""Purchase order data""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""ERP system"", ""contains"", ""PO data""],
    [""Azure Functions"", ""calls"", ""API""],
    [""Azure SQL Database"", ""contains"", ""PO data""],
    [""Power Apps"", ""calls"", ""Azure SQL Database""],
    [""Power Apps"", ""part-of"", ""Review process""],
    [""Review process"", ""contains"", ""PO data""],
    [""Power Apps"", ""contains"", ""PO updates""],
    [""PO updates"", ""part-of"", ""Supplier communication""],
    [""Supplier communication"", ""contains"", ""CSV export""]
  ]
}
```","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Data storage system for suppliers""],
    [""Data Factory"", ""component"", ""Data extraction tool""],
    [""ERP system"", ""system"", ""Purchase order management system""],
    [""Azure SQL Database"", ""system"", ""Relational database""],
    [""Azure Functions"", ""component"", ""API creation tool""],
    [""Power Apps"", ""component"", ""Data review and update application""],
    [""Suppliers"", ""component"", ""External entities receiving purchase order updates""],
    [""PO data"", ""component"", ""Purchase order data""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""ERP system"", ""contains"", ""PO data""],
    [""Azure Functions"", ""calls"", ""API""],
    [""Azure SQL Database"", ""contains"", ""PO data""],
    [""Power Apps"", ""calls"", ""Azure SQL Database""],
    [""Power Apps"", ""part-of"", ""Review process""],
    [""Review process"", ""contains"", ""PO data""],
    [""Power Apps"", ""contains"", ""PO updates""],
    [""PO updates"", ""part-of"", ""Supplier communication""],
    [""Supplier communication"", ""contains"", ""CSV export""]
  ]
}
```",3.5779255039524287
96,gemma:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity provider configured with a custom domain for sign-in""],
    [""Azure Front Door"", ""component"", ""Proxy service""],
    [""API Management"", ""system"", ""API gateway""],
    [""App Service Environment"", ""component"", ""Cloud environment""],
    [""Function Apps"", ""component"", ""Cloud environment""],
    [""AKS"", ""component"", ""Cloud environment""],
    [""JWT"", ""component"", ""JSON Web Token""],
    [""Client"", ""person"", ""Application user""],
    [""Backend API"", ""component"", ""Collection of APIs""]
  ],
  ""relationships"": [
    [""Client"", ""calls"", ""Azure Front Door""],
    [""Azure Front Door"", ""contains"", ""Authentication request""],
    [""Azure Front Door"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""Bearer token validation""],
    [""API Management"", ""part-of"", ""Backend API""],
    [""Backend API"", ""contains"", ""App Service Environment""],
    [""Backend API"", ""contains"", ""Function Apps""],
    [""Backend API"", ""contains"", ""AKS""],
    [""Client"", ""calls"", ""Backend API""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity provider configured with a custom domain for sign-in""],
    [""Azure Front Door"", ""component"", ""Proxy service""],
    [""API Management"", ""system"", ""API gateway""],
    [""App Service Environment"", ""component"", ""Cloud environment""],
    [""Function Apps"", ""component"", ""Cloud environment""],
    [""AKS"", ""component"", ""Cloud environment""],
    [""JWT"", ""component"", ""JSON Web Token""],
    [""Client"", ""person"", ""Application user""],
    [""Backend API"", ""component"", ""Collection of APIs""]
  ],
  ""relationships"": [
    [""Client"", ""calls"", ""Azure Front Door""],
    [""Azure Front Door"", ""contains"", ""Authentication request""],
    [""Azure Front Door"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""API Management"", ""contains"", ""Bearer token validation""],
    [""API Management"", ""part-of"", ""Backend API""],
    [""Backend API"", ""contains"", ""App Service Environment""],
    [""Backend API"", ""contains"", ""Function Apps""],
    [""Backend API"", ""contains"", ""AKS""],
    [""Client"", ""calls"", ""Backend API""]
  ]
}
```",3.9220613480429165
97,gemma:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}","```json
{
  ""entities"": [
    [ ""Outlook email account"", ""system"", ""Designated email account for receiving PDF files"" ],
    [ ""Logic App"", ""component"", ""Logic app for processing email attachments"" ],
    [ ""Azure Data Lake Storage"", ""system"", ""Cloud storage service for storing PDF files"" ],
    [ ""PDF container"", ""component"", ""Container for storing PDF files"" ],
    [ ""Function App"", ""component"", ""Function app for processing PDF forms"" ],
    [ ""Azure AI Document Intelligence"", ""system"", ""AI service for processing PDF forms"" ],
    [ ""Cosmos DB"", ""system"", ""NoSQL database for storing processed data"" ]
  ],
  ""relationships"": [
    [ ""contains"", ""Logic App"", ""PDF container"" ],
    [ ""contains"", ""PDF container"", ""PDF files"" ],
    [ ""calls"", ""Logic App"", ""Function App"" ],
    [ ""called-from"", ""Function App"", ""Azure AI Document Intelligence"" ],
    [ ""contains"", ""Function App"", ""Split PDF files"" ],
    [ ""contains"", ""Function App"", ""Processed response data"" ],
    [ ""contains"", ""Function App"", ""JSON data file"" ],
    [ ""contains"", ""Cosmos DB"", ""Processed data"" ],
    [ ""calls"", ""Forms processing logic app"", ""Cosmos DB"" ]
  ]
}
```","```json
{
  ""entities"": [
    [ ""Outlook email account"", ""system"", ""Designated email account for receiving PDF files"" ],
    [ ""Logic App"", ""component"", ""Logic app for processing email attachments"" ],
    [ ""Azure Data Lake Storage"", ""system"", ""Cloud storage service for storing PDF files"" ],
    [ ""PDF container"", ""component"", ""Container for storing PDF files"" ],
    [ ""Function App"", ""component"", ""Function app for processing PDF forms"" ],
    [ ""Azure AI Document Intelligence"", ""system"", ""AI service for processing PDF forms"" ],
    [ ""Cosmos DB"", ""system"", ""NoSQL database for storing processed data"" ]
  ],
  ""relationships"": [
    [ ""contains"", ""Logic App"", ""PDF container"" ],
    [ ""contains"", ""PDF container"", ""PDF files"" ],
    [ ""calls"", ""Logic App"", ""Function App"" ],
    [ ""called-from"", ""Function App"", ""Azure AI Document Intelligence"" ],
    [ ""contains"", ""Function App"", ""Split PDF files"" ],
    [ ""contains"", ""Function App"", ""Processed response data"" ],
    [ ""contains"", ""Function App"", ""JSON data file"" ],
    [ ""contains"", ""Cosmos DB"", ""Processed data"" ],
    [ ""calls"", ""Forms processing logic app"", ""Cosmos DB"" ]
  ]
}
```",4.2064982560114
98,gemma:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""Git repository hosting platform""],
    [""CI/CD pipeline agent"", ""component"", ""Continuous integration and continuous delivery pipeline agent""],
    [""Azure VM"", ""component"", ""Virtual machine""],
    [""API application"", ""component"", ""Application that consumes APIs""],
    [""ILB ASE"", ""system"", ""Azure Service Environment""],
    [""Azure API Management"", ""component"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Traffic routing service""],
    [""Azure Private DNS"", ""component"", ""Internal DNS resolution service""],
    [""External users"", ""person"", ""Users accessing the API""]
  ],
  ""relationships"": [
    [""GitHub"", ""contains"", ""CI/CD pipeline agent""],
    [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
    [""Azure VM"", ""contains"", ""API application""],
    [""API application"", ""calls"", ""Azure API Management""],
    [""Azure API Management"", ""part-of"", ""ILB ASE""],
    [""Azure API Management"", ""calls"", ""Application Gateway""],
    [""Application Gateway"", ""part-of"", ""Azure Private DNS""],
    [""Azure Private DNS"", ""contains"", ""API Management""],
    [""Application Gateway"", ""exposes"", ""Azure API Management""],
    [""Azure API Management"", ""used by"", ""External users""]
  ]
}
```","```json
{
  ""entities"": [
    [""GitHub"", ""system"", ""Git repository hosting platform""],
    [""CI/CD pipeline agent"", ""component"", ""Continuous integration and continuous delivery pipeline agent""],
    [""Azure VM"", ""component"", ""Virtual machine""],
    [""API application"", ""component"", ""Application that consumes APIs""],
    [""ILB ASE"", ""system"", ""Azure Service Environment""],
    [""Azure API Management"", ""component"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Traffic routing service""],
    [""Azure Private DNS"", ""component"", ""Internal DNS resolution service""],
    [""External users"", ""person"", ""Users accessing the API""]
  ],
  ""relationships"": [
    [""GitHub"", ""contains"", ""CI/CD pipeline agent""],
    [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
    [""Azure VM"", ""contains"", ""API application""],
    [""API application"", ""calls"", ""Azure API Management""],
    [""Azure API Management"", ""part-of"", ""ILB ASE""],
    [""Azure API Management"", ""calls"", ""Application Gateway""],
    [""Application Gateway"", ""part-of"", ""Azure Private DNS""],
    [""Azure Private DNS"", ""contains"", ""API Management""],
    [""Application Gateway"", ""exposes"", ""Azure API Management""],
    [""Azure API Management"", ""used by"", ""External users""]
  ]
}
```",4.135901250003371
99,gemma:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API Management securely routes HTTP requests to Logic Apps""],
    [""Logic Apps"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""],
    [""Azure Key Vault"", ""system"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""],
    [""Transport Layer Security"", ""component"", ""Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway""],
    [""Data Gateway"", ""component"", ""The on-premises data gateway connects to a SQL Server database to run the statement""],
    [""SQL Server"", ""component"", ""SQL Server stores the data and makes it available to apps that users access""]
  ],
  ""relationships"": [
    [""API Management"", ""contains"", ""Logic Apps""],
    [""Logic Apps"", ""contains"", ""Azure Key Vault""],
    [""Logic Apps"", ""calls"", ""Data Gateway""],
    [""Data Gateway"", ""part-of"", ""SQL Server""]
  ]
}
```","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API Management securely routes HTTP requests to Logic Apps""],
    [""Logic Apps"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""],
    [""Azure Key Vault"", ""system"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""],
    [""Transport Layer Security"", ""component"", ""Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway""],
    [""Data Gateway"", ""component"", ""The on-premises data gateway connects to a SQL Server database to run the statement""],
    [""SQL Server"", ""component"", ""SQL Server stores the data and makes it available to apps that users access""]
  ],
  ""relationships"": [
    [""API Management"", ""contains"", ""Logic Apps""],
    [""Logic Apps"", ""contains"", ""Azure Key Vault""],
    [""Logic Apps"", ""calls"", ""Data Gateway""],
    [""Data Gateway"", ""part-of"", ""SQL Server""]
  ]
}
```",3.1639373549842276
100,gemma:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}","```json
{
""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform built on Azure""],
    [""Azure API Management"", ""system"", ""API management platform with gateway and developer portal""],
    [""Azure DNS"", ""system"", ""Domain name resolution service on Azure""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service for API Gateway""],
    [""API gateway"", ""component"", ""Component of Azure API Management that accepts HTTP calls""],
    [""Developer portal"", ""component"", ""Component of Azure API Management with documentation and code samples""],
    [""Custom domain name"", ""component"", ""Domain name associated with an Azure service""],
    [""Azure"", ""system"", ""Cloud computing platform""]
],
""relationships"": [
    [""Azure Logic Apps"", ""calls"", ""API gateway""],
    [""API gateway"", ""part-of"", ""Azure API Management""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure API Management"", ""part-of"", ""Azure""],
    [""Azure DNS"", ""part-of"", ""Azure""],
    [""Microsoft Entra ID"", ""part-of"", ""Azure""],
    [""Azure Logic Apps"", ""calls"", ""Microsoft Entra ID""]
]
}
```","```json
{
""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform built on Azure""],
    [""Azure API Management"", ""system"", ""API management platform with gateway and developer portal""],
    [""Azure DNS"", ""system"", ""Domain name resolution service on Azure""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service for API Gateway""],
    [""API gateway"", ""component"", ""Component of Azure API Management that accepts HTTP calls""],
    [""Developer portal"", ""component"", ""Component of Azure API Management with documentation and code samples""],
    [""Custom domain name"", ""component"", ""Domain name associated with an Azure service""],
    [""Azure"", ""system"", ""Cloud computing platform""]
],
""relationships"": [
    [""Azure Logic Apps"", ""calls"", ""API gateway""],
    [""API gateway"", ""part-of"", ""Azure API Management""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure API Management"", ""part-of"", ""Azure""],
    [""Azure DNS"", ""part-of"", ""Azure""],
    [""Microsoft Entra ID"", ""part-of"", ""Azure""],
    [""Azure Logic Apps"", ""calls"", ""Microsoft Entra ID""]
]
}
```",3.9890821949811652
101,gemma:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Mainframe"", ""system"", ""Mainframe system""],
    [""Midrange systems"", ""system"", ""Midrange systems""],
    [""Publisher"", ""component"", ""Manages data migration""],
    [""Listener"", ""component"", ""Manages data migration""],
    [""Enhanced-security connection"", ""component"", ""Data transfer""],
    [""Replicator Engine"", ""component"", ""Captures and applies data""],
    [""Target"", ""component"", ""Receives changes""],
    [""Azure Databricks"", ""system"", ""Consume changes""],
    [""Azure data platform services"", ""system"", ""Apply changes""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""part-of"", ""Mainframe"", ""Publisher""],
    [""part-of"", ""Midrange systems"", ""Listener""],
    [""contains"", ""Enhanced-security connection"", ""Publisher""],
    [""contains"", ""Enhanced-security connection"", ""Listener""],
    [""contains"", ""Replicator Engine"", ""Publisher""],
    [""contains"", ""Replicator Engine"", ""Listener""],
    [""contains"", ""Target"", ""Azure Databricks""],
    [""calls"", ""Publisher"", ""Replicator Engine""],
    [""calls"", ""Listener"", ""Replicator Engine""]
  ]
}
```","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Mainframe"", ""system"", ""Mainframe system""],
    [""Midrange systems"", ""system"", ""Midrange systems""],
    [""Publisher"", ""component"", ""Manages data migration""],
    [""Listener"", ""component"", ""Manages data migration""],
    [""Enhanced-security connection"", ""component"", ""Data transfer""],
    [""Replicator Engine"", ""component"", ""Captures and applies data""],
    [""Target"", ""component"", ""Receives changes""],
    [""Azure Databricks"", ""system"", ""Consume changes""],
    [""Azure data platform services"", ""system"", ""Apply changes""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""part-of"", ""Mainframe"", ""Publisher""],
    [""part-of"", ""Midrange systems"", ""Listener""],
    [""contains"", ""Enhanced-security connection"", ""Publisher""],
    [""contains"", ""Enhanced-security connection"", ""Listener""],
    [""contains"", ""Replicator Engine"", ""Publisher""],
    [""contains"", ""Replicator Engine"", ""Listener""],
    [""contains"", ""Target"", ""Azure Databricks""],
    [""calls"", ""Publisher"", ""Replicator Engine""],
    [""calls"", ""Listener"", ""Replicator Engine""]
  ]
}
```",4.194388067990076
102,gemma:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure Storage Account"", ""component"", ""Storage account for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech recognition service""],
    [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
    [""Ingestion Client tool"", ""component"", ""Tool for transcription and redaction""],
    [""Azure OpenAI"", ""system"", ""Natural language processing service""],
    [""Power BI"", ""component"", ""Visualization tool""],
    [""App Service"", ""component"", ""Web application hosting service""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Azure Storage Account""],
    [""Azure AI Speech"", ""calls"", ""Ingestion Client tool""],
    [""Ingestion Client tool"", ""calls"", ""Azure AI Language""],
    [""Azure AI Language"", ""calls"", ""Azure OpenAI""],
    [""Azure OpenAI"", ""calls"", ""Power BI""],
    [""Azure OpenAI"", ""contains"", ""Transcript""],
    [""Power BI"", ""part-of"", ""App Service""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure Storage Account"", ""component"", ""Storage account for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech recognition service""],
    [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
    [""Ingestion Client tool"", ""component"", ""Tool for transcription and redaction""],
    [""Azure OpenAI"", ""system"", ""Natural language processing service""],
    [""Power BI"", ""component"", ""Visualization tool""],
    [""App Service"", ""component"", ""Web application hosting service""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Azure Storage Account""],
    [""Azure AI Speech"", ""calls"", ""Ingestion Client tool""],
    [""Ingestion Client tool"", ""calls"", ""Azure AI Language""],
    [""Azure AI Language"", ""calls"", ""Azure OpenAI""],
    [""Azure OpenAI"", ""calls"", ""Power BI""],
    [""Azure OpenAI"", ""contains"", ""Transcript""],
    [""Power BI"", ""part-of"", ""App Service""]
  ]
}
```",3.654720799997449
103,gemma:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Custom API"", ""component"", ""Custom API for Azure API Management""],
    [""API Coordinator"", ""person""],
    [""Power Apps"", ""system""],
    [""Microsoft Teams"", ""system""],
    [""Graph API"", ""component""],
    [""Azure Storage Account"", ""system""],
    [""Azure Functions"", ""system""],
    [""Azure Bot Service"", ""system""],
    [""Custom Bot"", ""component""],
    [""Azure Data Lake"", ""system""],
    [""Power BI"", ""system""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API""],
    [""Custom API"", ""contains"", ""Custom API Coordinator""],
    [""Custom API"", ""calls"", ""Power Apps""],
    [""Custom API"", ""part-of"", ""Azure API Management""],
    [""Power Apps"", ""part-of"", ""Microsoft Teams""],
    [""Custom API"", ""part-of"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Graph API"", ""part-of"", ""Azure Functions""],
    [""Custom Bot"", ""calls"", ""Azure Bot Service""],
    [""Azure Bot Service"", ""part-of"", ""Custom Bot""],
    [""Custom Bot"", ""calls"", ""Microsoft Teams""],
    [""Azure Data Lake"", ""part-of"", ""Event Hubs""],
    [""Event Hubs"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""calls"", ""Power BI""]
  ]
}
```","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Custom API"", ""component"", ""Custom API for Azure API Management""],
    [""API Coordinator"", ""person""],
    [""Power Apps"", ""system""],
    [""Microsoft Teams"", ""system""],
    [""Graph API"", ""component""],
    [""Azure Storage Account"", ""system""],
    [""Azure Functions"", ""system""],
    [""Azure Bot Service"", ""system""],
    [""Custom Bot"", ""component""],
    [""Azure Data Lake"", ""system""],
    [""Power BI"", ""system""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API""],
    [""Custom API"", ""contains"", ""Custom API Coordinator""],
    [""Custom API"", ""calls"", ""Power Apps""],
    [""Custom API"", ""part-of"", ""Azure API Management""],
    [""Power Apps"", ""part-of"", ""Microsoft Teams""],
    [""Custom API"", ""part-of"", ""Azure Storage Account""],
    [""Azure Functions"", ""calls"", ""Graph API""],
    [""Graph API"", ""part-of"", ""Azure Functions""],
    [""Custom Bot"", ""calls"", ""Azure Bot Service""],
    [""Azure Bot Service"", ""part-of"", ""Custom Bot""],
    [""Custom Bot"", ""calls"", ""Microsoft Teams""],
    [""Azure Data Lake"", ""part-of"", ""Event Hubs""],
    [""Event Hubs"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""calls"", ""Power BI""]
  ]
}
```",4.692300073977094
104,gemma:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}","```json
{
""entities"": [
    [""CDS"", ""system"", ""Data storage system containing supplier data""],
    [""Data Factory"", ""component"", ""Data transformation pipeline""],
    [""ERP system"", ""system"", ""System containing purchase order data""],
    [""Azure SQL Database"", ""system"", ""Database storing purchase order data""],
    [""Azure Functions"", ""component"", ""Automated process for surfacing PO data""],
    [""API"", ""component"", ""Interface for accessing PO data""],
    [""Power Apps"", ""component"", ""Application for reviewing and updating POs""],
    [""Suppliers"", ""person"", ""Recipients of purchase order updates""]
],
""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""Data Factory"", ""part-of"", ""CDS""],
    [""ERP system"", ""contains"", ""Purchase order data""],
    [""Purchase order data"", ""part-of"", ""ERP system""],
    [""Azure Functions"", ""calls"", ""API""],
    [""API"", ""part-of"", ""Azure Functions""],
    [""Azure Functions"", ""calls"", ""Power Apps""],
    [""Power Apps"", ""part-of"", ""Azure Functions""],
    [""Power Apps"", ""calls"", ""Suppliers""],
    [""Suppliers"", ""part-of"", ""Power Apps""]
]
}
```","```json
{
""entities"": [
    [""CDS"", ""system"", ""Data storage system containing supplier data""],
    [""Data Factory"", ""component"", ""Data transformation pipeline""],
    [""ERP system"", ""system"", ""System containing purchase order data""],
    [""Azure SQL Database"", ""system"", ""Database storing purchase order data""],
    [""Azure Functions"", ""component"", ""Automated process for surfacing PO data""],
    [""API"", ""component"", ""Interface for accessing PO data""],
    [""Power Apps"", ""component"", ""Application for reviewing and updating POs""],
    [""Suppliers"", ""person"", ""Recipients of purchase order updates""]
],
""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""Data Factory"", ""part-of"", ""CDS""],
    [""ERP system"", ""contains"", ""Purchase order data""],
    [""Purchase order data"", ""part-of"", ""ERP system""],
    [""Azure Functions"", ""calls"", ""API""],
    [""API"", ""part-of"", ""Azure Functions""],
    [""Azure Functions"", ""calls"", ""Power Apps""],
    [""Power Apps"", ""part-of"", ""Azure Functions""],
    [""Power Apps"", ""calls"", ""Suppliers""],
    [""Suppliers"", ""part-of"", ""Power Apps""]
]
}
```",3.81335134600522
105,gemma:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}","```json
{
  ""entities"": [
    [ ""Azure Data Factory"", ""system"", ""Data Factory is a cloud-based data integration service."" ],
    [ ""Blob storage"", ""component"", ""A cloud storage service for storing data."" ],
    [ ""Microsoft-managed keys"", ""component"", ""A key management service for generating and managing encryption keys."" ],
    [ ""Data science VMs"", ""component"", ""Virtual machines provisioned for research work."" ],
    [ ""Azure Machine Learning"", ""system"", ""A cloud-based platform for machine learning."" ],
    [ ""Logic app"", ""component"", ""A workflow tool for automated tasks."" ],
    [ ""Data reviewers"", ""person"", ""Individuals responsible for reviewing data for sensitivity."" ]
  ],
  ""relationships"": [
    [ ""contains"", ""Azure Data Factory"", ""Blob storage"" ],
    [ ""part-of"", ""Blob storage"", ""Microsoft-managed keys"" ],
    [ ""calls"", ""Azure Data Factory"", ""Logic app"" ],
    [ ""called-from"", ""Logic app"", ""Azure Data Factory"" ],
    [ ""contains"", ""Secure environment"", ""Azure Virtual Desktop"" ],
    [ ""contains"", ""Secure environment"", ""Azure Machine Learning"" ],
    [ ""contains"", ""Secure storage account"", ""Dataset"" ],
    [ ""part-of"", ""Dataset"", ""Data science VMs"" ],
    [ ""contains"", ""Secure environment"", ""Data science VMs"" ],
    [ ""contains"", ""Secure environment"", ""Azure Machine Learning"" ],
    [ ""calls"", ""Logic app"", ""Data reviewers"" ],
    [ ""called-from"", ""Data reviewers"", ""Logic app"" ],
    [ ""contains"", ""Secure storage account"", ""Models"" ],
    [ ""part-of"", ""Models"", ""Data Factory"" ],
    [ ""contains"", ""Data Factory"", ""Public storage account"" ]
  ]
}
```","```json
{
  ""entities"": [
    [ ""Azure Data Factory"", ""system"", ""Data Factory is a cloud-based data integration service."" ],
    [ ""Blob storage"", ""component"", ""A cloud storage service for storing data."" ],
    [ ""Microsoft-managed keys"", ""component"", ""A key management service for generating and managing encryption keys."" ],
    [ ""Data science VMs"", ""component"", ""Virtual machines provisioned for research work."" ],
    [ ""Azure Machine Learning"", ""system"", ""A cloud-based platform for machine learning."" ],
    [ ""Logic app"", ""component"", ""A workflow tool for automated tasks."" ],
    [ ""Data reviewers"", ""person"", ""Individuals responsible for reviewing data for sensitivity."" ]
  ],
  ""relationships"": [
    [ ""contains"", ""Azure Data Factory"", ""Blob storage"" ],
    [ ""part-of"", ""Blob storage"", ""Microsoft-managed keys"" ],
    [ ""calls"", ""Azure Data Factory"", ""Logic app"" ],
    [ ""called-from"", ""Logic app"", ""Azure Data Factory"" ],
    [ ""contains"", ""Secure environment"", ""Azure Virtual Desktop"" ],
    [ ""contains"", ""Secure environment"", ""Azure Machine Learning"" ],
    [ ""contains"", ""Secure storage account"", ""Dataset"" ],
    [ ""part-of"", ""Dataset"", ""Data science VMs"" ],
    [ ""contains"", ""Secure environment"", ""Data science VMs"" ],
    [ ""contains"", ""Secure environment"", ""Azure Machine Learning"" ],
    [ ""calls"", ""Logic app"", ""Data reviewers"" ],
    [ ""called-from"", ""Data reviewers"", ""Logic app"" ],
    [ ""contains"", ""Secure storage account"", ""Models"" ],
    [ ""part-of"", ""Models"", ""Data Factory"" ],
    [ ""contains"", ""Data Factory"", ""Public storage account"" ]
  ]
}
```",5.647878941032104
106,gemma:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing documents""],
    [""Command Message"", ""component"", ""Message containing metadata about the document""],
    [""Durable Functions"", ""system"", ""Orchestration framework""],
    [""Computer Vision Read API"", ""component"", ""Optical character recognition service""],
    [""Document Classifier Service"", ""component"", ""Service for classifying document types""],
    [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing document metadata""],
    [""Cognitive Search"", ""system"", ""Search engine""],
    [""Correlation ID"", ""component"", ""Unique identifier for linking search results with document metadata""]
  ],
  ""relationships"": [
    [""Command Message"", ""calls"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Computer Vision Read API""],
    [""Computer Vision Read API"", ""returns"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Classify activity""],
    [""Classify activity"", ""calls"", ""Document Classifier Service""],
    [""Document Classifier Service"", ""returns"", ""Classify activity""],
    [""Durable Functions"", ""calls"", ""Metadata Store activity""],
    [""Durable Functions"", ""calls"", ""Indexing activity""],
    [""Indexing activity"", ""creates"", ""Cognitive Search""],
    [""Indexing activity"", ""uses"", ""Azure AI Search libraries""],
    [""Durable Functions"", ""returns"", ""Correlation ID""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing documents""],
    [""Command Message"", ""component"", ""Message containing metadata about the document""],
    [""Durable Functions"", ""system"", ""Orchestration framework""],
    [""Computer Vision Read API"", ""component"", ""Optical character recognition service""],
    [""Document Classifier Service"", ""component"", ""Service for classifying document types""],
    [""Azure Kubernetes Service"", ""system"", ""Container orchestration platform""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing document metadata""],
    [""Cognitive Search"", ""system"", ""Search engine""],
    [""Correlation ID"", ""component"", ""Unique identifier for linking search results with document metadata""]
  ],
  ""relationships"": [
    [""Command Message"", ""calls"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Computer Vision Read API""],
    [""Computer Vision Read API"", ""returns"", ""Durable Functions""],
    [""Durable Functions"", ""calls"", ""Classify activity""],
    [""Classify activity"", ""calls"", ""Document Classifier Service""],
    [""Document Classifier Service"", ""returns"", ""Classify activity""],
    [""Durable Functions"", ""calls"", ""Metadata Store activity""],
    [""Durable Functions"", ""calls"", ""Indexing activity""],
    [""Indexing activity"", ""creates"", ""Cognitive Search""],
    [""Indexing activity"", ""uses"", ""Azure AI Search libraries""],
    [""Durable Functions"", ""returns"", ""Correlation ID""]
  ]
}
```",4.536064365005586
107,gemma:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API gateway that receives client requests""],
    [""Patient API"", ""component"", ""API for managing patient data""],
    [""Audit API"", ""component"", ""API for managing audit records""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing patient data""],
    [""Audit Record"", ""component"", ""Record of patient data changes""]
  ],
  ""relationships"": [
    [""APIM"", ""calls"", ""Patient API"", ""Makes a POST request with patient information""],
    [""Patient API"", ""calls"", ""Audit API"", ""Creates an audit record""],
    [""Audit API"", ""calls"", ""Azure Cosmos DB"", ""Creates an audit record""],
    [""Patient API"", ""calls"", ""Azure Cosmos DB"", ""Creates a patient document""],
    [""Patient API"", ""returns"", ""APIM"", ""Success response""]
  ]
}
```","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API gateway that receives client requests""],
    [""Patient API"", ""component"", ""API for managing patient data""],
    [""Audit API"", ""component"", ""API for managing audit records""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing patient data""],
    [""Audit Record"", ""component"", ""Record of patient data changes""]
  ],
  ""relationships"": [
    [""APIM"", ""calls"", ""Patient API"", ""Makes a POST request with patient information""],
    [""Patient API"", ""calls"", ""Audit API"", ""Creates an audit record""],
    [""Audit API"", ""calls"", ""Azure Cosmos DB"", ""Creates an audit record""],
    [""Patient API"", ""calls"", ""Azure Cosmos DB"", ""Creates a patient document""],
    [""Patient API"", ""returns"", ""APIM"", ""Success response""]
  ]
}
```",2.895562724035699
108,gemma:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API gateway that receives requests from outside services""],
    [""Patient API"", ""component"", ""API for managing patients""],
    [""Audit API"", ""component"", ""API for creating audit records""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing patient data""],
    [""CreatePatient"", ""function"", ""Function in Patient API for creating patients""],
    [""CreateAuditRecord"", ""function"", ""Function in Audit API for creating audit records""]
  ],
  ""relationships"": [
    [""calls"", ""APIM"", ""Patient API""],
    [""calls"", ""Patient API"", ""CreatePatient""],
    [""calls"", ""CreatePatient"", ""Audit API""],
    [""contains"", ""Patient API"", ""CreatePatient""],
    [""contains"", ""Audit API"", ""CreateAuditRecord""]
  ]
}
```","```json
{
  ""entities"": [
    [""APIM"", ""system"", ""API gateway that receives requests from outside services""],
    [""Patient API"", ""component"", ""API for managing patients""],
    [""Audit API"", ""component"", ""API for creating audit records""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing patient data""],
    [""CreatePatient"", ""function"", ""Function in Patient API for creating patients""],
    [""CreateAuditRecord"", ""function"", ""Function in Audit API for creating audit records""]
  ],
  ""relationships"": [
    [""calls"", ""APIM"", ""Patient API""],
    [""calls"", ""Patient API"", ""CreatePatient""],
    [""calls"", ""CreatePatient"", ""Audit API""],
    [""contains"", ""Patient API"", ""CreatePatient""],
    [""contains"", ""Audit API"", ""CreateAuditRecord""]
  ]
}
```",2.74318761395989
109,gemma:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF files""],
    [""Logic App"", ""component"", ""Logic app for processing email arrivals""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud storage platform for storing PDF files""],
    [""PDF container"", ""component"", ""Container for PDF files""],
    [""Function App"", ""component"", ""Function app for processing PDF forms""],
    [""Azure AI Document Intelligence"", ""system"", ""AI-powered document processing service""],
    [""Cosmos DB"", ""system"", ""NoSQL database""],
    [""Power BI"", ""component"", ""Business intelligence and data analytics platform""]
  ],
  ""relationships"": [
    [""contains"", ""Logic App"", ""Outlook email account""],
    [""part-of"", ""PDF container"", ""Azure Data Lake Storage""],
    [""calls"", ""Logic App"", ""Function App""],
    [""called-from"", ""Function App"", ""Azure AI Document Intelligence""],
    [""contains"", ""Function App"", ""Single-page PDF files""],
    [""part-of"", ""Single-page PDF files"", ""PDF container""],
    [""contains"", ""Function App"", ""Processed response data""],
    [""part-of"", ""Processed response data"", ""Data structure""],
    [""contains"", ""Data structure"", ""JSON data file""],
    [""part-of"", ""JSON data file"", ""Data Lake Storage""],
    [""calls"", ""Forms processing logic App"", ""Cosmos DB""],
    [""part-of"", ""Cosmos DB"", ""Data from Azure Cosmos DB""],
    [""uses"", ""Power BI"", ""Cosmos DB""]
  ]
}
```","```json
{
  ""entities"": [
    [""Outlook email account"", ""system"", ""Designated email account for receiving PDF files""],
    [""Logic App"", ""component"", ""Logic app for processing email arrivals""],
    [""Azure Data Lake Storage"", ""system"", ""Cloud storage platform for storing PDF files""],
    [""PDF container"", ""component"", ""Container for PDF files""],
    [""Function App"", ""component"", ""Function app for processing PDF forms""],
    [""Azure AI Document Intelligence"", ""system"", ""AI-powered document processing service""],
    [""Cosmos DB"", ""system"", ""NoSQL database""],
    [""Power BI"", ""component"", ""Business intelligence and data analytics platform""]
  ],
  ""relationships"": [
    [""contains"", ""Logic App"", ""Outlook email account""],
    [""part-of"", ""PDF container"", ""Azure Data Lake Storage""],
    [""calls"", ""Logic App"", ""Function App""],
    [""called-from"", ""Function App"", ""Azure AI Document Intelligence""],
    [""contains"", ""Function App"", ""Single-page PDF files""],
    [""part-of"", ""Single-page PDF files"", ""PDF container""],
    [""contains"", ""Function App"", ""Processed response data""],
    [""part-of"", ""Processed response data"", ""Data structure""],
    [""contains"", ""Data structure"", ""JSON data file""],
    [""part-of"", ""JSON data file"", ""Data Lake Storage""],
    [""calls"", ""Forms processing logic App"", ""Cosmos DB""],
    [""part-of"", ""Cosmos DB"", ""Data from Azure Cosmos DB""],
    [""uses"", ""Power BI"", ""Cosmos DB""]
  ]
}
```",4.993046766961925
110,gemma:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}","```json
{
""entities"": [
    [""Azure Data Factory"", ""system"", ""Data pipeline tool""],
    [""Microsoft-managed keys"", ""component"", ""Encryption key""],
    [""Secure storage account"", ""component"", ""Storage account with security controls""],
    [""Streaming application"", ""component"", ""Application for accessing secure environment""],
    [""Data science VMs"", ""component"", ""Virtual machines for research""],
    [""Azure Machine Learning compute"", ""component"", ""Machine learning environment""],
    [""Logic app"", ""component"", ""Automated workflow""],
    [""Data Factory instance"", ""component"", ""Data pipeline tool instance""],
    [""Public storage account"", ""system"", ""Storage account for external access""]
],
""relationships"": [
    [""Azure Data Factory"", ""contains"", ""Microsoft-managed keys""],
    [""Azure Data Factory"", ""contains"", ""Secure storage account""],
    [""Secure storage account"", ""part-of"", ""Azure Data Factory""],
    [""Streaming application"", ""calls"", ""Azure Virtual Desktop""],
    [""Azure Virtual Desktop"", ""called-from"", ""Streaming application""],
    [""Secure storage account"", ""contains"", ""Data science VMs""],
    [""Data science VMs"", ""part-of"", ""Secure storage account""],
    [""Secure storage account"", ""contains"", ""Azure Machine Learning compute""],
    [""Azure Machine Learning compute"", ""part-of"", ""Secure storage account""],
    [""Logic app"", ""calls"", ""Data Factory instance""],
    [""Data Factory instance"", ""called-from"", ""Logic app""],
    [""Data Factory instance"", ""contains"", ""Public storage account""],
    [""Public storage account"", ""part-of"", ""Data Factory instance""]
]
}
```","```json
{
""entities"": [
    [""Azure Data Factory"", ""system"", ""Data pipeline tool""],
    [""Microsoft-managed keys"", ""component"", ""Encryption key""],
    [""Secure storage account"", ""component"", ""Storage account with security controls""],
    [""Streaming application"", ""component"", ""Application for accessing secure environment""],
    [""Data science VMs"", ""component"", ""Virtual machines for research""],
    [""Azure Machine Learning compute"", ""component"", ""Machine learning environment""],
    [""Logic app"", ""component"", ""Automated workflow""],
    [""Data Factory instance"", ""component"", ""Data pipeline tool instance""],
    [""Public storage account"", ""system"", ""Storage account for external access""]
],
""relationships"": [
    [""Azure Data Factory"", ""contains"", ""Microsoft-managed keys""],
    [""Azure Data Factory"", ""contains"", ""Secure storage account""],
    [""Secure storage account"", ""part-of"", ""Azure Data Factory""],
    [""Streaming application"", ""calls"", ""Azure Virtual Desktop""],
    [""Azure Virtual Desktop"", ""called-from"", ""Streaming application""],
    [""Secure storage account"", ""contains"", ""Data science VMs""],
    [""Data science VMs"", ""part-of"", ""Secure storage account""],
    [""Secure storage account"", ""contains"", ""Azure Machine Learning compute""],
    [""Azure Machine Learning compute"", ""part-of"", ""Secure storage account""],
    [""Logic app"", ""calls"", ""Data Factory instance""],
    [""Data Factory instance"", ""called-from"", ""Logic app""],
    [""Data Factory instance"", ""contains"", ""Public storage account""],
    [""Public storage account"", ""part-of"", ""Data Factory instance""]
]
}
```",5.076490391045809
111,gemma:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
    [""Document file"", ""component"", ""File containing various document types""],
    [""Command message"", ""component"", ""Message containing metadata""],
    [""Durable Functions"", ""system"", ""Orchestration framework""],
    [""Computer Vision Read API"", ""component"", ""API for optical character recognition""],
    [""AKS cluster"", ""system"", ""Kubernetes service""],
    [""Document classifier service"", ""component"", ""Service for classifying document types""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing document metadata""],
    [""Cognitive Search"", ""system"", ""Search engine for storing and retrieving documents""]
  ],
  ""relationships"": [
    [""contains"", ""Document file"", ""Azure Blob Storage""],
    [""calls"", ""Durable Functions"", ""Command message""],
    [""calls"", ""Scan activity function"", ""Computer Vision Read API""],
    [""called-from"", ""Orchestration"", ""Scan activity function""],
    [""calls"", ""Classify activity function"", ""Document classifier service""],
    [""called-from"", ""Orchestration"", ""Classify activity function""],
    [""contains"", ""Document file"", ""Document types""],
    [""part-of"", ""Document classifier service"", ""AKS cluster""],
    [""contains"", ""Document file"", ""OCR results""],
    [""part-of"", ""Document classifier service"", ""Azure AI Search libraries""],
    [""contains"", ""Document file"", ""Metadata""],
    [""part-of"", ""Metadata"", ""Azure Cosmos DB""],
    [""contains"", ""Document file"", ""Search documents""],
    [""part-of"", ""Search documents"", ""Cognitive Search""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing data""],
    [""Document file"", ""component"", ""File containing various document types""],
    [""Command message"", ""component"", ""Message containing metadata""],
    [""Durable Functions"", ""system"", ""Orchestration framework""],
    [""Computer Vision Read API"", ""component"", ""API for optical character recognition""],
    [""AKS cluster"", ""system"", ""Kubernetes service""],
    [""Document classifier service"", ""component"", ""Service for classifying document types""],
    [""Azure Cosmos DB"", ""system"", ""Database for storing document metadata""],
    [""Cognitive Search"", ""system"", ""Search engine for storing and retrieving documents""]
  ],
  ""relationships"": [
    [""contains"", ""Document file"", ""Azure Blob Storage""],
    [""calls"", ""Durable Functions"", ""Command message""],
    [""calls"", ""Scan activity function"", ""Computer Vision Read API""],
    [""called-from"", ""Orchestration"", ""Scan activity function""],
    [""calls"", ""Classify activity function"", ""Document classifier service""],
    [""called-from"", ""Orchestration"", ""Classify activity function""],
    [""contains"", ""Document file"", ""Document types""],
    [""part-of"", ""Document classifier service"", ""AKS cluster""],
    [""contains"", ""Document file"", ""OCR results""],
    [""part-of"", ""Document classifier service"", ""Azure AI Search libraries""],
    [""contains"", ""Document file"", ""Metadata""],
    [""part-of"", ""Metadata"", ""Azure Cosmos DB""],
    [""contains"", ""Document file"", ""Search documents""],
    [""part-of"", ""Search documents"", ""Cognitive Search""]
  ]
}
```",5.122229425993282
112,gemma:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API Management securely routes HTTP requests to Logic Apps.""],
    [""Logic Apps"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""Azure Key Vault"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""Data Gateway"", ""component"", ""Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
    [""SQL Server"", ""component"", ""SQL Server stores the data and makes it available to apps that users access""],
    [""Azure Monitor"", ""system"", ""Azure Monitor collects information on Logic Apps events and performance""]
  ],
  ""relationships"": [
    [""API Management"", ""contains"", ""Logic Apps""],
    [""Logic Apps"", ""part-of"", ""API Management""],
    [""Logic Apps"", ""calls"", ""Azure Key Vault""],
    [""Azure Key Vault"", ""called-from"", ""Logic Apps""],
    [""Logic Apps"", ""calls"", ""Data Gateway""],
    [""Data Gateway"", ""called-from"", ""Logic Apps""],
    [""Logic Apps"", ""contains"", ""Azure Monitor""]
  ]
}
```","```json
{
  ""entities"": [
    [""API Management"", ""system"", ""API Management securely routes HTTP requests to Logic Apps.""],
    [""Logic Apps"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""Azure Key Vault"", ""component"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.""],
    [""Data Gateway"", ""component"", ""Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
    [""SQL Server"", ""component"", ""SQL Server stores the data and makes it available to apps that users access""],
    [""Azure Monitor"", ""system"", ""Azure Monitor collects information on Logic Apps events and performance""]
  ],
  ""relationships"": [
    [""API Management"", ""contains"", ""Logic Apps""],
    [""Logic Apps"", ""part-of"", ""API Management""],
    [""Logic Apps"", ""calls"", ""Azure Key Vault""],
    [""Azure Key Vault"", ""called-from"", ""Logic Apps""],
    [""Logic Apps"", ""calls"", ""Data Gateway""],
    [""Data Gateway"", ""called-from"", ""Logic Apps""],
    [""Logic Apps"", ""contains"", ""Azure Monitor""]
  ]
}
```",3.581264843000099
113,gemma:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech-to-text transcription service""],
    [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
    [""Azure OpenAI"", ""system"", ""Conversational analysis and summarization tool""],
    [""Ingestion Client tool"", ""component"", ""No-code transcription client""],
    [""Power BI"", ""component"", ""Data visualization tool""],
    [""App Service"", ""component"", ""Web application hosting service""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
    [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
    [""Azure AI Speech"", ""part-of"", ""Azure OpenAI""],
    [""Ingestion Client tool"", ""part-of"", ""Azure OpenAI""],
    [""Azure OpenAI"", ""contains"", ""Processed transcript""],
    [""Azure OpenAI"", ""part-of"", ""Power BI""],
    [""Power BI"", ""part-of"", ""App Service""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Blob Storage"", ""system"", ""Cloud storage service for storing audio files""],
    [""Azure AI Speech"", ""system"", ""Speech-to-text transcription service""],
    [""Azure AI Language"", ""system"", ""Personal data detection and redaction service""],
    [""Azure OpenAI"", ""system"", ""Conversational analysis and summarization tool""],
    [""Ingestion Client tool"", ""component"", ""No-code transcription client""],
    [""Power BI"", ""component"", ""Data visualization tool""],
    [""App Service"", ""component"", ""Web application hosting service""]
  ],
  ""relationships"": [
    [""Azure Blob Storage"", ""contains"", ""Audio files""],
    [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
    [""Azure AI Language"", ""calls"", ""Azure AI Speech""],
    [""Azure OpenAI"", ""calls"", ""Azure AI Speech""],
    [""Azure AI Speech"", ""part-of"", ""Azure OpenAI""],
    [""Ingestion Client tool"", ""part-of"", ""Azure OpenAI""],
    [""Azure OpenAI"", ""contains"", ""Processed transcript""],
    [""Azure OpenAI"", ""part-of"", ""Power BI""],
    [""Power BI"", ""part-of"", ""App Service""]
  ]
}
```",3.9644933589734137
114,gemma:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity Provider""],
    [""Azure Front Door"", ""component"", ""Proxy""],
    [""API Management"", ""system"", ""API Gateway""],
    [""App Service Environment"", ""component"", ""Environment""],
    [""Function Apps"", ""component"", ""Environment""],
    [""AKS"", ""component"", ""Environment""],
    [""JWT"", ""component"", ""Authentication Token""]
  ],
  ""relationships"": [
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""part-of"", ""Azure AD B2C""],
    [""API Management"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""contains"", ""Azure Front Door""],
    [""API Management"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""contains"", ""API Management""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""API Management"", ""part-of"", ""Azure Front Door""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure AD B2C"", ""system"", ""Identity Provider""],
    [""Azure Front Door"", ""component"", ""Proxy""],
    [""API Management"", ""system"", ""API Gateway""],
    [""App Service Environment"", ""component"", ""Environment""],
    [""Function Apps"", ""component"", ""Environment""],
    [""AKS"", ""component"", ""Environment""],
    [""JWT"", ""component"", ""Authentication Token""]
  ],
  ""relationships"": [
    [""Azure AD B2C"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""part-of"", ""Azure AD B2C""],
    [""API Management"", ""calls"", ""Azure AD B2C""],
    [""Azure AD B2C"", ""contains"", ""Azure Front Door""],
    [""API Management"", ""contains"", ""JWT""],
    [""Azure Front Door"", ""contains"", ""API Management""],
    [""Azure Front Door"", ""part-of"", ""API Management""],
    [""API Management"", ""part-of"", ""Azure Front Door""]
  ]
}
```",3.2959917259868234
115,gemma:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform that uses connectors to integrate with various services""],
    [""Azure API Management"", ""system"", ""API management platform with gateway and developer portal""],
    [""Azure DNS"", ""system"", ""Name resolution service""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service""],
    [""API gateway"", ""component"", ""Accepts HTTP calls and routes them to backend""],
    [""Developer portal"", ""component"", ""Provides access to documentation and code samples for calling APIs""],
    [""Custom domain name"", ""component"", ""Domain name that can be mapped to IP address""],
    [""Clients"", ""person"", ""Call the API gateway""],
    [""Standard or Premium tier"", ""person"", ""Tier of API Management""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""calls"", ""API gateway""],
    [""API gateway"", ""called-from"", ""Azure Logic Apps""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure API Management"", ""part-of"", ""Azure Infrastructure""],
    [""Azure DNS"", ""part-of"", ""Azure Infrastructure""],
    [""Microsoft Entra ID"", ""part-of"", ""Azure Infrastructure""],
    [""Clients"", ""calls"", ""API gateway""],
    [""Standard or Premium tier"", ""part-of"", ""Azure API Management""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure Logic Apps"", ""system"", ""Orchestration platform that uses connectors to integrate with various services""],
    [""Azure API Management"", ""system"", ""API management platform with gateway and developer portal""],
    [""Azure DNS"", ""system"", ""Name resolution service""],
    [""Microsoft Entra ID"", ""system"", ""Authentication service""],
    [""API gateway"", ""component"", ""Accepts HTTP calls and routes them to backend""],
    [""Developer portal"", ""component"", ""Provides access to documentation and code samples for calling APIs""],
    [""Custom domain name"", ""component"", ""Domain name that can be mapped to IP address""],
    [""Clients"", ""person"", ""Call the API gateway""],
    [""Standard or Premium tier"", ""person"", ""Tier of API Management""]
  ],
  ""relationships"": [
    [""Azure Logic Apps"", ""calls"", ""API gateway""],
    [""API gateway"", ""called-from"", ""Azure Logic Apps""],
    [""Azure API Management"", ""contains"", ""API gateway""],
    [""Azure API Management"", ""contains"", ""Developer portal""],
    [""Azure API Management"", ""part-of"", ""Azure Infrastructure""],
    [""Azure DNS"", ""part-of"", ""Azure Infrastructure""],
    [""Microsoft Entra ID"", ""part-of"", ""Azure Infrastructure""],
    [""Clients"", ""calls"", ""API gateway""],
    [""Standard or Premium tier"", ""part-of"", ""Azure API Management""]
  ]
}
```",4.29687143699266
116,gemma:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Mainframe"", ""system""],
    [""Midrange systems"", ""system""],
    [""Publisher"", ""component""],
    [""Listener"", ""component""],
    [""Enhanced-security connection"", ""system""],
    [""Replicator Engine"", ""component""],
    [""Target"", ""system""],
    [""Azure Databricks"", ""system""],
    [""Azure data platform services"", ""system""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""part-of"", ""Mainframe"", ""Publisher""],
    [""contains"", ""Midrange systems"", ""Listener""],
    [""part-of"", ""Windows or Linux machine"", ""Listener""],
    [""calls"", ""Publisher or Listener"", ""Enhanced-security connection""],
    [""called-from"", ""Enhanced-security connection"", ""Publisher or Listener""],
    [""contains"", ""Publisher or Listener"", ""Replicator Engine""],
    [""part-of"", ""Replicator Engine"", ""Target""],
    [""calls"", ""Replicator Engine"", ""Azure Databricks""],
    [""called-from"", ""Azure Databricks"", ""Replicator Engine""]
  ]
}
```","```json
{
  ""entities"": [
    [""Connect agent component"", ""component"", ""Capture change logs""],
    [""Mainframe"", ""system""],
    [""Midrange systems"", ""system""],
    [""Publisher"", ""component""],
    [""Listener"", ""component""],
    [""Enhanced-security connection"", ""system""],
    [""Replicator Engine"", ""component""],
    [""Target"", ""system""],
    [""Azure Databricks"", ""system""],
    [""Azure data platform services"", ""system""]
  ],
  ""relationships"": [
    [""contains"", ""Connect agent component"", ""Publisher""],
    [""part-of"", ""Mainframe"", ""Publisher""],
    [""contains"", ""Midrange systems"", ""Listener""],
    [""part-of"", ""Windows or Linux machine"", ""Listener""],
    [""calls"", ""Publisher or Listener"", ""Enhanced-security connection""],
    [""called-from"", ""Enhanced-security connection"", ""Publisher or Listener""],
    [""contains"", ""Publisher or Listener"", ""Replicator Engine""],
    [""part-of"", ""Replicator Engine"", ""Target""],
    [""calls"", ""Replicator Engine"", ""Azure Databricks""],
    [""called-from"", ""Azure Databricks"", ""Replicator Engine""]
  ]
}
```",3.8331127199926414
117,gemma:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Custom API Coordinator"", ""person""],
    [""Azure API Management"", ""system""],
    [""Power Apps"", ""system""],
    [""Microsoft Teams"", ""system""],
    [""Azure Storage Account"", ""system""],
    [""Azure Functions"", ""system""],
    [""Azure Event Hubs"", ""system""],
    [""Azure Data Lake"", ""system""],
    [""Power BI"", ""system""],
    [""Custom Bot Messaging Service"", ""person""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API Coordinator""],
    [""Custom API Coordinator"", ""calls"", ""Power Apps""],
    [""Custom API Coordinator"", ""part-of"", ""Airline System""],
    [""Custom API Coordinator"", ""contains"", ""Azure Storage Account""],
    [""Azure Functions"", ""part-of"", ""Azure Storage Account""],
    [""Azure Storage Account"", ""contains"", ""Graph API call""],
    [""Graph API call"", ""calls"", ""Teams""],
    [""Azure Functions"", ""calls"", ""Azure Event Hubs""],
    [""Azure Event Hubs"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""contains"", ""Events""],
    [""Events"", ""calls"", ""Power BI""],
    [""Custom Bot Messaging Service"", ""part-of"", ""Airline System""]
  ]
}
```","```json
{
  ""entities"": [
    [""Airline System"", ""system"", ""Airline notification system""],
    [""Custom API Coordinator"", ""person""],
    [""Azure API Management"", ""system""],
    [""Power Apps"", ""system""],
    [""Microsoft Teams"", ""system""],
    [""Azure Storage Account"", ""system""],
    [""Azure Functions"", ""system""],
    [""Azure Event Hubs"", ""system""],
    [""Azure Data Lake"", ""system""],
    [""Power BI"", ""system""],
    [""Custom Bot Messaging Service"", ""person""]
  ],
  ""relationships"": [
    [""Airline System"", ""calls"", ""Custom API Coordinator""],
    [""Custom API Coordinator"", ""calls"", ""Power Apps""],
    [""Custom API Coordinator"", ""part-of"", ""Airline System""],
    [""Custom API Coordinator"", ""contains"", ""Azure Storage Account""],
    [""Azure Functions"", ""part-of"", ""Azure Storage Account""],
    [""Azure Storage Account"", ""contains"", ""Graph API call""],
    [""Graph API call"", ""calls"", ""Teams""],
    [""Azure Functions"", ""calls"", ""Azure Event Hubs""],
    [""Azure Event Hubs"", ""part-of"", ""Azure Data Lake""],
    [""Azure Data Lake"", ""contains"", ""Events""],
    [""Events"", ""calls"", ""Power BI""],
    [""Custom Bot Messaging Service"", ""part-of"", ""Airline System""]
  ]
}
```",4.205818094022106
118,gemma:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}","```json
{
  ""entities"": [
    [""Azure VM"", ""system"", ""Cloud-based virtual machine""],
    [""GitHub repository"", ""component"", ""Code repository""],
    [""CI/CD pipeline agent"", ""component"", ""Continuous integration and continuous delivery pipeline""],
    [""API application"", ""component"", ""Software application""],
    [""Azure API Management"", ""system"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Reverse proxy""],
    [""Internal users"", ""person"", ""Humans""],
    [""API Management policy"", ""component"", ""Configuration rule""],
    [""App Service Environment"", ""system"", ""Deployment environment""]
  ],
  ""relationships"": [
    [""GitHub repository"", ""contains"", ""CI/CD pipeline agent""],
    [""CI/CD pipeline agent"", ""calls"", ""API application""],
    [""Azure API Management"", ""contains"", ""API Management policy""],
    [""API Management"", ""part-of"", ""App Service Environment""],
    [""Application Gateway"", ""contains"", ""API Management""],
    [""App Service Environment"", ""part-of"", ""Azure Private DNS""],
    [""Azure Private DNS"", ""contains"", ""API Management""],
    [""Application Gateway"", ""calls"", ""API Management""],
    [""API Management"", ""calls"", ""API application""]
  ]
}
```","```json
{
  ""entities"": [
    [""Azure VM"", ""system"", ""Cloud-based virtual machine""],
    [""GitHub repository"", ""component"", ""Code repository""],
    [""CI/CD pipeline agent"", ""component"", ""Continuous integration and continuous delivery pipeline""],
    [""API application"", ""component"", ""Software application""],
    [""Azure API Management"", ""system"", ""API gateway""],
    [""Application Gateway"", ""component"", ""Reverse proxy""],
    [""Internal users"", ""person"", ""Humans""],
    [""API Management policy"", ""component"", ""Configuration rule""],
    [""App Service Environment"", ""system"", ""Deployment environment""]
  ],
  ""relationships"": [
    [""GitHub repository"", ""contains"", ""CI/CD pipeline agent""],
    [""CI/CD pipeline agent"", ""calls"", ""API application""],
    [""Azure API Management"", ""contains"", ""API Management policy""],
    [""API Management"", ""part-of"", ""App Service Environment""],
    [""Application Gateway"", ""contains"", ""API Management""],
    [""App Service Environment"", ""part-of"", ""Azure Private DNS""],
    [""Azure Private DNS"", ""contains"", ""API Management""],
    [""Application Gateway"", ""calls"", ""API Management""],
    [""API Management"", ""calls"", ""API application""]
  ]
}
```",3.8390654000104405
119,gemma:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Supplier data storage""],
    [""Data Factory"", ""component"", ""Data pipeline""],
    [""ERP system"", ""system"", ""Purchase order storage""],
    [""Azure SQL Database"", ""system"", ""Database""],
    [""Azure Functions"", ""component"", ""Data extraction""],
    [""API"", ""component"", ""Data access""],
    [""Power Apps"", ""component"", ""Data review""],
    [""Suppliers"", ""person"", ""Recipients""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""Data Factory"", ""part-of"", ""Azure SQL Database""],
    [""ERP system"", ""part-of"", ""Azure SQL Database""],
    [""Azure Functions"", ""calls"", ""API""],
    [""API"", ""part-of"", ""Azure SQL Database""],
    [""Power Apps"", ""calls"", ""API""],
    [""Azure SQL Database"", ""contains"", ""Purchase order data""],
    [""Power Apps"", ""contains"", ""Purchase order data""],
    [""Power Apps"", ""calls"", ""Suppliers""]
  ]
}
```","```json
{
  ""entities"": [
    [""CDS"", ""system"", ""Supplier data storage""],
    [""Data Factory"", ""component"", ""Data pipeline""],
    [""ERP system"", ""system"", ""Purchase order storage""],
    [""Azure SQL Database"", ""system"", ""Database""],
    [""Azure Functions"", ""component"", ""Data extraction""],
    [""API"", ""component"", ""Data access""],
    [""Power Apps"", ""component"", ""Data review""],
    [""Suppliers"", ""person"", ""Recipients""]
  ],
  ""relationships"": [
    [""CDS"", ""contains"", ""Data Factory""],
    [""Data Factory"", ""part-of"", ""Azure SQL Database""],
    [""ERP system"", ""part-of"", ""Azure SQL Database""],
    [""Azure Functions"", ""calls"", ""API""],
    [""API"", ""part-of"", ""Azure SQL Database""],
    [""Power Apps"", ""calls"", ""API""],
    [""Azure SQL Database"", ""contains"", ""Purchase order data""],
    [""Power Apps"", ""contains"", ""Purchase order data""],
    [""Power Apps"", ""calls"", ""Suppliers""]
  ]
}
```",3.3798310100100935
