,MODEL_NAME,filename,original text,expected_json,llm_full_response,llm_json_extracted,time_taken_seconds
0,mistral:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}"," {
      ""entities"": [
         [""Airline System"", ""system"", ""The airline system that communicates with a custom API hosted in Azure API Management.""],
         [""Custom API Hosted in Azure API Management"", ""component"", ""A component that the airline system communicates with.""],
         [""Custom API Coordinator"", ""component"", ""A component that receives notifications and handles incoming messages from the airline system, sending them to Power Apps.""],
         [""Power Apps"", ""system"", ""A system where flights are assigned to Microsoft Teams channels.""],
         [""Azure Storage Account Queue"", ""system"", ""A system used for queuing Graph API calls for further processing.""],
         [""Azure Functions"", ""component"", ""A component that runs the Graph API calls based on incoming messages in the storage queue, sending notifications to Teams and streaming all events to an Azure Event Hubs.""],
         [""Azure Event Hubs"", ""system"", ""A system for further analytics.""],
         [""Custom Bot Messaging Service"", ""component"", ""A component that employs Azure Bot Service.""],
         [""Azure Bot Service"", ""system"", ""A system that manages the airline's notification system.""],
         [""Microsoft Teams Channels"", ""system"", ""Channels in Microsoft Teams where flights are assigned.""],
         [""Graph API Calls"", ""component"", ""Calls queued in Azure Storage Account Queue for further processing.""],
         [""Azure Data Lake Storage"", ""system"", ""A system offering long-term retention and micro-batch processing of events from Event Hubs.""],
         [""Power BI"", ""system"", ""A system generating insightful reports with data from Azure Data Lake storage.""]
      ],
      ""relationships"": [
         [""Airline System"", ""part-of"", ""Custom API Hosted in Azure API Management""],
         [""Custom API Hosted in Azure API Management"", ""calls"", ""Custom API Coordinator""],
         [""Custom API Coordinator"", ""calls"", ""Power Apps""],
         [""Power Apps"", ""contains"", ""Microsoft Teams Channels""],
         [""Airline System"", ""part-of"", ""Graph API Calls""],
         [""Azure Storage Account Queue"", ""part-of"", ""Graph API Calls""],
         [""Azure Functions"", ""part-of"", ""Graph API Calls""],
         [""Graph API Calls"", ""calls"", ""Azure Event Hubs""],
         [""Azure Data Lake Storage"", ""contains"", ""Azure Event Hubs""],
         [""Custom Bots"", ""part-of"", ""Custom Bot Messaging Service""],
         [""Custom Bot Messaging Service"", ""calls"", ""Microsoft Teams""],
         [""Azure Data Lake Storage"", ""contains"", ""Power BI""]
      ]
   }"," {
      ""entities"": [
         [""Airline System"", ""system"", ""The airline system that communicates with a custom API hosted in Azure API Management.""],
         [""Custom API Hosted in Azure API Management"", ""component"", ""A component that the airline system communicates with.""],
         [""Custom API Coordinator"", ""component"", ""A component that receives notifications and handles incoming messages from the airline system, sending them to Power Apps.""],
         [""Power Apps"", ""system"", ""A system where flights are assigned to Microsoft Teams channels.""],
         [""Azure Storage Account Queue"", ""system"", ""A system used for queuing Graph API calls for further processing.""],
         [""Azure Functions"", ""component"", ""A component that runs the Graph API calls based on incoming messages in the storage queue, sending notifications to Teams and streaming all events to an Azure Event Hubs.""],
         [""Azure Event Hubs"", ""system"", ""A system for further analytics.""],
         [""Custom Bot Messaging Service"", ""component"", ""A component that employs Azure Bot Service.""],
         [""Azure Bot Service"", ""system"", ""A system that manages the airline's notification system.""],
         [""Microsoft Teams Channels"", ""system"", ""Channels in Microsoft Teams where flights are assigned.""],
         [""Graph API Calls"", ""component"", ""Calls queued in Azure Storage Account Queue for further processing.""],
         [""Azure Data Lake Storage"", ""system"", ""A system offering long-term retention and micro-batch processing of events from Event Hubs.""],
         [""Power BI"", ""system"", ""A system generating insightful reports with data from Azure Data Lake storage.""]
      ],
      ""relationships"": [
         [""Airline System"", ""part-of"", ""Custom API Hosted in Azure API Management""],
         [""Custom API Hosted in Azure API Management"", ""calls"", ""Custom API Coordinator""],
         [""Custom API Coordinator"", ""calls"", ""Power Apps""],
         [""Power Apps"", ""contains"", ""Microsoft Teams Channels""],
         [""Airline System"", ""part-of"", ""Graph API Calls""],
         [""Azure Storage Account Queue"", ""part-of"", ""Graph API Calls""],
         [""Azure Functions"", ""part-of"", ""Graph API Calls""],
         [""Graph API Calls"", ""calls"", ""Azure Event Hubs""],
         [""Azure Data Lake Storage"", ""contains"", ""Azure Event Hubs""],
         [""Custom Bots"", ""part-of"", ""Custom Bot Messaging Service""],
         [""Custom Bot Messaging Service"", ""calls"", ""Microsoft Teams""],
         [""Azure Data Lake Storage"", ""contains"", ""Power BI""]
      ]
   }",7.100425111013465
1,mistral:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}"," {
      ""entities"": [
         [""Connect Agent Component"", ""component"", ""Captures change logs using native utilities of mainframe or midrange systems, caches the logs in temporary storage.""],
         [""Publisher Component (Mainframe)"", ""component"", ""Manages data migration for mainframe systems and moves data from on-premises to Azure via an enhanced-security connection.""],
         [""Listener Component (Midrange)"", ""component"", ""Manages data migration for midrange systems, located on either a Windows or Linux machine, moves data from on-premises to Azure via an enhanced-security connection.""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target. Distributes data for parallel processing.""],
         [""Target Database"", ""system"", ""Receives changes via ODBC or ingests changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""component"", ""Consumes the changed data and applies it to Azure data platform services.""]
      ],
      ""relationships"": [
         [""Connect Agent Component"", ""part-of"", ""System"", ""It's a component that captures change logs.""],
         [""Publisher Component (Mainframe)"", ""part-of"", ""System"", ""It's a component responsible for managing data migration of mainframe systems.""],
         [""Listener Component (Midrange)"", ""part-of"", ""System"", ""It's a component responsible for managing data migration of midrange systems.""],
         [""Publisher Component (Mainframe)"", ""calls"", ""Connect Replicator Engine"", ""It moves the captured data to the Connect Replicator Engine.""],
         [""Listener Component (Midrange)"", ""calls"", ""Connect Replicator Engine"", ""It moves the captured data to the Connect Replicator Engine.""],
         [""Connect Replicator Engine"", ""part-of"", ""System"", ""It applies changes to the target database.""],
         [""Target Database"", ""called-from"", ""Connect Replicator Engine"", ""It receives and processes the changes applied by the Connect Replicator Engine.""]
      ]
   }"," {
      ""entities"": [
         [""Connect Agent Component"", ""component"", ""Captures change logs using native utilities of mainframe or midrange systems, caches the logs in temporary storage.""],
         [""Publisher Component (Mainframe)"", ""component"", ""Manages data migration for mainframe systems and moves data from on-premises to Azure via an enhanced-security connection.""],
         [""Listener Component (Midrange)"", ""component"", ""Manages data migration for midrange systems, located on either a Windows or Linux machine, moves data from on-premises to Azure via an enhanced-security connection.""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target. Distributes data for parallel processing.""],
         [""Target Database"", ""system"", ""Receives changes via ODBC or ingests changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""component"", ""Consumes the changed data and applies it to Azure data platform services.""]
      ],
      ""relationships"": [
         [""Connect Agent Component"", ""part-of"", ""System"", ""It's a component that captures change logs.""],
         [""Publisher Component (Mainframe)"", ""part-of"", ""System"", ""It's a component responsible for managing data migration of mainframe systems.""],
         [""Listener Component (Midrange)"", ""part-of"", ""System"", ""It's a component responsible for managing data migration of midrange systems.""],
         [""Publisher Component (Mainframe)"", ""calls"", ""Connect Replicator Engine"", ""It moves the captured data to the Connect Replicator Engine.""],
         [""Listener Component (Midrange)"", ""calls"", ""Connect Replicator Engine"", ""It moves the captured data to the Connect Replicator Engine.""],
         [""Connect Replicator Engine"", ""part-of"", ""System"", ""It applies changes to the target database.""],
         [""Target Database"", ""called-from"", ""Connect Replicator Engine"", ""It receives and processes the changes applied by the Connect Replicator Engine.""]
      ]
   }",5.286529517034069
2,mistral:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}"," {
      ""entities"": [
         [""CDS"", ""component"", ""Cloud Data Services""],
         [""SQL"", ""component"", ""Microsoft SQL Server""],
         [""Data Factory"", ""component"", ""Azure Data Factory""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning System""],
         [""Azure SQL database"", ""component"", ""Database on Azure Platform""],
         [""Azure Functions"", ""component"", ""Serverless computing service on Azure""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Microsoft Power Apps""],
         [""Power BI"", ""component"", ""Microsoft Power BI""]
      ],
      ""relationships"": [
         [""CDS"", ""part-of"", ""Azure SQL database"", ""Data flows through the solution and is stored in CDS, then moved to SQL via Data Factory.""],
         [""SQL"", ""part-of"", ""Azure SQL database"", """"],
         [""ERP system"", ""part-of"", ""PO data storage"", ""PO data is stored in ERP system and sent to Azure SQL database.""],
         [""Azure SQL database"", ""part-of"", ""Data Flow"", """"],
         [""Azure Functions"", ""calls"", ""API"", ""Azure Functions uses API to surface PO data monthly.""],
         [""Power Apps"", ""calls"", ""Azure SQL Database"", ""Power Apps retrieves data from Azure SQL Database through API.""],
         [""User"", ""part-of"", ""Power Apps"", ""User reviews and updates POs in Power Apps""],
         [""CSV export"", ""called-from"", ""User"", ""User sends this data to suppliers through CSV export.""],
         [""Power BI"", ""calls"", ""trends in supplier status"", ""Power BI reports trends in supplier status.""]
      ]
   }"," {
      ""entities"": [
         [""CDS"", ""component"", ""Cloud Data Services""],
         [""SQL"", ""component"", ""Microsoft SQL Server""],
         [""Data Factory"", ""component"", ""Azure Data Factory""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning System""],
         [""Azure SQL database"", ""component"", ""Database on Azure Platform""],
         [""Azure Functions"", ""component"", ""Serverless computing service on Azure""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Microsoft Power Apps""],
         [""Power BI"", ""component"", ""Microsoft Power BI""]
      ],
      ""relationships"": [
         [""CDS"", ""part-of"", ""Azure SQL database"", ""Data flows through the solution and is stored in CDS, then moved to SQL via Data Factory.""],
         [""SQL"", ""part-of"", ""Azure SQL database"", """"],
         [""ERP system"", ""part-of"", ""PO data storage"", ""PO data is stored in ERP system and sent to Azure SQL database.""],
         [""Azure SQL database"", ""part-of"", ""Data Flow"", """"],
         [""Azure Functions"", ""calls"", ""API"", ""Azure Functions uses API to surface PO data monthly.""],
         [""Power Apps"", ""calls"", ""Azure SQL Database"", ""Power Apps retrieves data from Azure SQL Database through API.""],
         [""User"", ""part-of"", ""Power Apps"", ""User reviews and updates POs in Power Apps""],
         [""CSV export"", ""called-from"", ""User"", ""User sends this data to suppliers through CSV export.""],
         [""Power BI"", ""calls"", ""trends in supplier status"", ""Power BI reports trends in supplier status.""]
      ]
   }",4.563019653025549
3,mistral:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Users who upload datasets into public blob storage""],
         [""Public Blob Storage Account"", ""system"", ""Storage account where data owners upload and store their datasets""],
         [""Microsoft-Managed Keys"", ""component"", ""Encryption keys used for encrypting the data in the public blob storage account""],
         [""Azure Data Factory"", ""component"", ""Tool that initiates the copying of the uploaded dataset from the public blob storage account to another storage account""],
         [""Trigger"", ""component"", ""Event that starts the copying process initiated by Azure Data Factory""],
         [""Specific Location (import path)"", ""system"", ""The destination for copied data in another storage account""],
         [""Private Endpoint"", ""component"", ""Endpoint through which the storage account can be accessed securely""],
         [""Service Principal with limited permissions"", ""component"", ""Principal used to access the storage account""],
         [""Another Storage Account"", ""system"", ""Storage account where the copied data is stored after the copying process""],
         [""Streaming Application"", ""component"", ""Application through which researchers access the secure environment""],
         [""Azure Virtual Desktop as a privileged jump box"", ""component"", ""Virtual desktop used as a gateway to access the streaming application and the secure environment""],
         [""Secure Network Environment"", ""system"", ""Environment in which data science VMs are provisioned for research work""],
         [""Data Science VMs"", ""component"", ""VMs provisioned in the secure network environment for data preparation and research tasks""],
         [""Azure Machine Learning Compute"", ""component"", ""Compute resource that can access the dataset through a private endpoint for machine learning tasks""],
         [""Secure Storage Account (export path)"", ""system"", ""Storage account where de-identified data is saved after the preparation process""],
         [""Logic App"", ""component"", ""Application that starts the manual approval process when new data is added to the export path""],
         [""Manual Reviewers"", ""person"", ""Individuals who review the data queued for exporting to ensure sensitive data isn't included""],
         [""De-identified Data"", ""component"", ""Data that has had personal information removed""],
         [""Secure Storage (export path)"", ""system"", ""The system where the approved de-identified data is saved""],
         [""Azure Machine Learning Capabilities"", ""component"", ""Capabilities provided by Azure Machine Learning, such as training, deploying, automating, and managing machine learning models""],
         [""Public Storage Account (separate container)"", ""system"", ""Storage account where approved de-identified data and models are stored for external researchers to access""],
         [""Lower Security Environment Storage Account"", ""component"", ""Optional alternative storage account in a lower security environment""]
      ],
      ""relationships"": [
         [""Data Owners"", ""part-of"", ""Public Blob Storage Account""],
         [""Public Blob Storage Account"", ""contains"", ""Microsoft-Managed Keys""],
         [""Azure Data Factory"", ""part-of"", ""Trigger""],
         [""Trigger"", ""calls"", ""Azure Data Factory""],
         [""Azure Data Factory"", ""part-of"", ""Specific Location (import path)""],
         [""Another Storage Account"", ""part-of"", ""Private Endpoint""],
         [""Another Storage Account"", ""contains"", ""Service Principal with limited permissions""],
         [""Streaming Application"", ""calls"", ""Azure Virtual Desktop as a privileged jump box""],
         [""Secure Network Environment"", ""contains"", ""Data Science VMs""],
         [""Secure Network Environment"", ""part-of"", ""Azure Machine Learning Compute""],
         [""Secure Storage Account (export path)"", ""contains"", ""De-identified Data""],
         [""Logic App"", ""called-from"", ""Manual Reviewers""],
         [""Logic App"", ""calls"", ""Data Factory instance""],
         [""Public Storage Account (separate container)"", ""part-of"", ""Lower Security Environment Storage Account""]
      ]
   }"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Users who upload datasets into public blob storage""],
         [""Public Blob Storage Account"", ""system"", ""Storage account where data owners upload and store their datasets""],
         [""Microsoft-Managed Keys"", ""component"", ""Encryption keys used for encrypting the data in the public blob storage account""],
         [""Azure Data Factory"", ""component"", ""Tool that initiates the copying of the uploaded dataset from the public blob storage account to another storage account""],
         [""Trigger"", ""component"", ""Event that starts the copying process initiated by Azure Data Factory""],
         [""Specific Location (import path)"", ""system"", ""The destination for copied data in another storage account""],
         [""Private Endpoint"", ""component"", ""Endpoint through which the storage account can be accessed securely""],
         [""Service Principal with limited permissions"", ""component"", ""Principal used to access the storage account""],
         [""Another Storage Account"", ""system"", ""Storage account where the copied data is stored after the copying process""],
         [""Streaming Application"", ""component"", ""Application through which researchers access the secure environment""],
         [""Azure Virtual Desktop as a privileged jump box"", ""component"", ""Virtual desktop used as a gateway to access the streaming application and the secure environment""],
         [""Secure Network Environment"", ""system"", ""Environment in which data science VMs are provisioned for research work""],
         [""Data Science VMs"", ""component"", ""VMs provisioned in the secure network environment for data preparation and research tasks""],
         [""Azure Machine Learning Compute"", ""component"", ""Compute resource that can access the dataset through a private endpoint for machine learning tasks""],
         [""Secure Storage Account (export path)"", ""system"", ""Storage account where de-identified data is saved after the preparation process""],
         [""Logic App"", ""component"", ""Application that starts the manual approval process when new data is added to the export path""],
         [""Manual Reviewers"", ""person"", ""Individuals who review the data queued for exporting to ensure sensitive data isn't included""],
         [""De-identified Data"", ""component"", ""Data that has had personal information removed""],
         [""Secure Storage (export path)"", ""system"", ""The system where the approved de-identified data is saved""],
         [""Azure Machine Learning Capabilities"", ""component"", ""Capabilities provided by Azure Machine Learning, such as training, deploying, automating, and managing machine learning models""],
         [""Public Storage Account (separate container)"", ""system"", ""Storage account where approved de-identified data and models are stored for external researchers to access""],
         [""Lower Security Environment Storage Account"", ""component"", ""Optional alternative storage account in a lower security environment""]
      ],
      ""relationships"": [
         [""Data Owners"", ""part-of"", ""Public Blob Storage Account""],
         [""Public Blob Storage Account"", ""contains"", ""Microsoft-Managed Keys""],
         [""Azure Data Factory"", ""part-of"", ""Trigger""],
         [""Trigger"", ""calls"", ""Azure Data Factory""],
         [""Azure Data Factory"", ""part-of"", ""Specific Location (import path)""],
         [""Another Storage Account"", ""part-of"", ""Private Endpoint""],
         [""Another Storage Account"", ""contains"", ""Service Principal with limited permissions""],
         [""Streaming Application"", ""calls"", ""Azure Virtual Desktop as a privileged jump box""],
         [""Secure Network Environment"", ""contains"", ""Data Science VMs""],
         [""Secure Network Environment"", ""part-of"", ""Azure Machine Learning Compute""],
         [""Secure Storage Account (export path)"", ""contains"", ""De-identified Data""],
         [""Logic App"", ""called-from"", ""Manual Reviewers""],
         [""Logic App"", ""calls"", ""Data Factory instance""],
         [""Public Storage Account (separate container)"", ""part-of"", ""Lower Security Environment Storage Account""]
      ]
   }",9.550356329011265
4,mistral:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A system triggered by HTTP requests and uses connectors to integrate with commonly used services.""],
         [""API Management"", ""system"", ""A system consisting of two related components: API gateway and developer portal.""],
         [""API Gateway"", ""component"", ""A component that accepts HTTP calls and routes them to the backend.""],
         [""Developer Portal"", ""component"", ""A component within Azure API Management providing access to documentation, code samples for calling APIs, testing APIs, etc.""],
         [""Azure DNS"", ""system"", ""A system providing name resolution using Azure infrastructure.""],
         [""Custom Domain Name"", ""component"", ""A component used to map a custom domain name to the IP address.""],
         [""Microsoft Entra ID"", ""system"", ""A system used for authenticating clients that call the API gateway, supporting OpenID Connect (OIDC) protocol.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""calls"", ""API Management"", ""Logic Apps are triggered by HTTP requests which are routed to the backend via the API gateway.""],
         [""API Management"", ""part-of"", ""Azure"", ""The Azure infrastructure is used for various components of API Management, including name resolution (Azure DNS), developer portal, and API gateway.""],
         [""Custom Domain Name"", ""called-from"", ""API Management"", ""To use a custom domain name in API Management, create DNS records that map the custom domain name to the IP address of the API gateway.""],
         [""Microsoft Entra ID"", ""calls"", ""API Gateway"", ""Microsoft Entra ID authenticates clients and provides access tokens which are used by the API Gateway to authorize requests.""]
      ]
   }"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A system triggered by HTTP requests and uses connectors to integrate with commonly used services.""],
         [""API Management"", ""system"", ""A system consisting of two related components: API gateway and developer portal.""],
         [""API Gateway"", ""component"", ""A component that accepts HTTP calls and routes them to the backend.""],
         [""Developer Portal"", ""component"", ""A component within Azure API Management providing access to documentation, code samples for calling APIs, testing APIs, etc.""],
         [""Azure DNS"", ""system"", ""A system providing name resolution using Azure infrastructure.""],
         [""Custom Domain Name"", ""component"", ""A component used to map a custom domain name to the IP address.""],
         [""Microsoft Entra ID"", ""system"", ""A system used for authenticating clients that call the API gateway, supporting OpenID Connect (OIDC) protocol.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""calls"", ""API Management"", ""Logic Apps are triggered by HTTP requests which are routed to the backend via the API gateway.""],
         [""API Management"", ""part-of"", ""Azure"", ""The Azure infrastructure is used for various components of API Management, including name resolution (Azure DNS), developer portal, and API gateway.""],
         [""Custom Domain Name"", ""called-from"", ""API Management"", ""To use a custom domain name in API Management, create DNS records that map the custom domain name to the IP address of the API gateway.""],
         [""Microsoft Entra ID"", ""calls"", ""API Gateway"", ""Microsoft Entra ID authenticates clients and provides access tokens which are used by the API Gateway to authorize requests.""]
      ]
   }",4.605603208008688
5,mistral:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A platform for version control and collaboration""],
         [""CI/CD pipeline agent"", ""component"", ""An Azure VM-installed tool that automates the build, test, and deployment of applications""],
         [""Azure VM"", ""component"", ""A virtual machine on Microsoft Azure's cloud service""],
         [""API application"", ""component"", ""The application that hosts the APIs""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment""],
         [""Azure API Management"", ""system"", ""A product from Microsoft Azure for building, publishing, and managing APIs""],
         [""HOST headers"", ""entity"", ""Information added to a request by the client to provide additional information about the request""],
         [""API Management policy"", ""entity"", ""Rules that control how requests are processed""],
         [""App Service Environment's DNS name"", ""entity"", ""The domain name of the App Service Environment""],
         [""Application Gateway"", ""system"", ""A Microsoft Azure product for managing and routing traffic to web applications""],
         [""Azure Private DNS"", ""system"", ""Microsoft Azure's private Domain Name System service""],
         [""External users"", ""person"", ""Users outside the internal network accessing the APIs""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent"", ""Developers check in code to a GitHub repository""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
         [""API application"", ""part-of"", ""ILB ASE""],
         [""Azure API Management"", ""consumes"", ""API application"", ""The agent pushes the build to the API application""],
         [""Azure API Management"", ""uses"", [""HOST headers"", ""API Management policy""], ""The App Service Environment's DNS name is used for all the APIs""],
         [""Application Gateway"", ""exposes"", ""API Management"", ""Exposes API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", [[""ASE"", ""API Management"", ""Application Gateway""], ""Internally between ASE, API Management, and Application Gateway""],
         [""External users"", ""utilizes"", ""exposed developer portal"", ""Consume the APIs via Application Gateway's public IP""]
      ]
   }"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A platform for version control and collaboration""],
         [""CI/CD pipeline agent"", ""component"", ""An Azure VM-installed tool that automates the build, test, and deployment of applications""],
         [""Azure VM"", ""component"", ""A virtual machine on Microsoft Azure's cloud service""],
         [""API application"", ""component"", ""The application that hosts the APIs""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment""],
         [""Azure API Management"", ""system"", ""A product from Microsoft Azure for building, publishing, and managing APIs""],
         [""HOST headers"", ""entity"", ""Information added to a request by the client to provide additional information about the request""],
         [""API Management policy"", ""entity"", ""Rules that control how requests are processed""],
         [""App Service Environment's DNS name"", ""entity"", ""The domain name of the App Service Environment""],
         [""Application Gateway"", ""system"", ""A Microsoft Azure product for managing and routing traffic to web applications""],
         [""Azure Private DNS"", ""system"", ""Microsoft Azure's private Domain Name System service""],
         [""External users"", ""person"", ""Users outside the internal network accessing the APIs""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent"", ""Developers check in code to a GitHub repository""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
         [""API application"", ""part-of"", ""ILB ASE""],
         [""Azure API Management"", ""consumes"", ""API application"", ""The agent pushes the build to the API application""],
         [""Azure API Management"", ""uses"", [""HOST headers"", ""API Management policy""], ""The App Service Environment's DNS name is used for all the APIs""],
         [""Application Gateway"", ""exposes"", ""API Management"", ""Exposes API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", [[""ASE"", ""API Management"", ""Application Gateway""], ""Internally between ASE, API Management, and Application Gateway""],
         [""External users"", ""utilizes"", ""exposed developer portal"", ""Consume the APIs via Application Gateway's public IP""]
      ]
   }",5.791472580982372
6,mistral:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}"," {
      ""entities"": [
         [""Web App"", ""system"", ""The web app that uploads the user-provided document file.""],
         [""Azure Blob Storage"", ""system"", ""The cloud storage service where the uploaded document files are stored.""],
         [""Storage Queue"", ""component"", ""A messaging system component used to add commands and pass data between services in Azure.""],
         [""Durable Functions Orchestration"", ""component"", ""An orchestrator that coordinates multiple functions within an Azure function app.""],
         [""Computer Vision Read API"", ""component"", ""An API service provided by Azure Cognitive Services to perform optical character recognition.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""A managed container orchestration service on Azure.""],
         [""Document Classifier Service"", ""component"", ""A service hosted in an AKS cluster that uses regular expression pattern matching to identify document types and calculate their numbers.""],
         [""Azure Cosmos DB Store"", ""component"", ""A NoSQL database service used to store the metadata of processed documents.""],
         [""Cognitive Search Service"", ""system"", ""An Azure search service that allows users to perform full-text searches on various data sources.""],
         [""Search Document"", ""component"", ""A document created in the Cognitive Search service for indexing purposes.""]
      ],
      ""relationships"": [
         [""Web App"", ""part-of"", ""system"", ""The web app is part of a larger system that includes the Azure infrastructure.""],
         [""Azure Blob Storage"", ""contains"", ""component"", ""The storage service contains the document files.""],
         [""Storage Queue"", ""part-of"", ""system"", ""The messaging component is part of the Azure messaging system.""],
         [""Durable Functions Orchestration"", ""calls"", ""component"", ""The orchestrator calls the Computer Vision Read API.""],
         [""Computer Vision Read API"", ""called-from"", ""component"", ""The API is called by the Durable Functions Orchestration.""],
         [""Durable Functions Orchestration"", ""contains"", ""component"", ""Each instance of the orchestrator processes only one document file.""],
         [""Document Classifier Service"", ""part-of"", ""system"", ""The service is part of the AKS cluster.""],
         [""Azure Cosmos DB Store"", ""contains"", ""component"", ""The database store contains metadata about processed documents.""],
         [""Cognitive Search Service"", ""calls"", ""component"", ""The search service calls the Azure AI Search libraries for .NET to index the data.""]
      ]
   }"," {
      ""entities"": [
         [""Web App"", ""system"", ""The web app that uploads the user-provided document file.""],
         [""Azure Blob Storage"", ""system"", ""The cloud storage service where the uploaded document files are stored.""],
         [""Storage Queue"", ""component"", ""A messaging system component used to add commands and pass data between services in Azure.""],
         [""Durable Functions Orchestration"", ""component"", ""An orchestrator that coordinates multiple functions within an Azure function app.""],
         [""Computer Vision Read API"", ""component"", ""An API service provided by Azure Cognitive Services to perform optical character recognition.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""A managed container orchestration service on Azure.""],
         [""Document Classifier Service"", ""component"", ""A service hosted in an AKS cluster that uses regular expression pattern matching to identify document types and calculate their numbers.""],
         [""Azure Cosmos DB Store"", ""component"", ""A NoSQL database service used to store the metadata of processed documents.""],
         [""Cognitive Search Service"", ""system"", ""An Azure search service that allows users to perform full-text searches on various data sources.""],
         [""Search Document"", ""component"", ""A document created in the Cognitive Search service for indexing purposes.""]
      ],
      ""relationships"": [
         [""Web App"", ""part-of"", ""system"", ""The web app is part of a larger system that includes the Azure infrastructure.""],
         [""Azure Blob Storage"", ""contains"", ""component"", ""The storage service contains the document files.""],
         [""Storage Queue"", ""part-of"", ""system"", ""The messaging component is part of the Azure messaging system.""],
         [""Durable Functions Orchestration"", ""calls"", ""component"", ""The orchestrator calls the Computer Vision Read API.""],
         [""Computer Vision Read API"", ""called-from"", ""component"", ""The API is called by the Durable Functions Orchestration.""],
         [""Durable Functions Orchestration"", ""contains"", ""component"", ""Each instance of the orchestrator processes only one document file.""],
         [""Document Classifier Service"", ""part-of"", ""system"", ""The service is part of the AKS cluster.""],
         [""Azure Cosmos DB Store"", ""contains"", ""component"", ""The database store contains metadata about processed documents.""],
         [""Cognitive Search Service"", ""calls"", ""component"", ""The search service calls the Azure AI Search libraries for .NET to index the data.""]
      ]
   }",6.474797201983165
7,mistral:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}"," {
      ""entities"": [
         [""Outlook Email Account"", ""person"", ""A designated email account that receives PDF files as attachments""],
         [""Logic App"", ""component"", ""A program built using Azure Logic Apps that processes emails and uploads PDF files to Azure Data Lake Storage""],
         [""Azure Data Lake Storage"", ""system"", ""A storage service provided by Microsoft Azure where the PDF files are stored""],
         [""Function App"", ""component"", ""A program built using Azure Functions that processes the single-page PDF files""],
         [""Azure AI Document Intelligence"", ""component"", ""An Azure REST API used for processing the single-page PDF files""],
         [""Azure Cosmos DB"", ""system"", ""A database service provided by Microsoft Azure where the processed data is saved""]
      ],
      ""relationships"": [
         [""Logic App"", ""part-of"", ""Azure Logic Apps"", ""The logic app is built using the capabilities of Azure Logic Apps""],
         [""Outlook Email Account"", ""calls"", ""Logic App"", ""The arrival of an email triggers the logic app to process the email""],
         [""PDF files"", ""part-of"", ""Email"", ""The PDF files are attached to the email""],
         [""PDF files"", ""uploaded-to"", ""Azure Data Lake Storage"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""],
         [""Manually or programmatically uploaded PDF files"", ""uploaded-to"", ""Azure Data Lake Storage"", ""You can manually or programmatically upload PDF files to the same container in Data Lake Storage""],
         [""PDF file in the PDF container"", ""calls"", ""Function App"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""],
         [""Function App"", ""uses"", ""HTTPS POST"", ""The function app sends the location of the single-page PDF file to AI Document Intelligence using HTTPS POST""],
         [""AI Document Intelligence"", ""called-from"", ""Function App"", ""Azure AI Document Intelligence is called from the function app for processing""],
         [""Single-page PDF files"", ""saved-to"", ""Second container in Data Lake Storage"", ""Split files are saved to a second container in Data Lake Storage""],
         [""Function App"", ""sends"", ""Location of single-page PDF file"", ""The function app sends the location of the single-page PDF file to AI Document Intelligence for processing""],
         [""AI Document Intelligence"", ""returns"", ""Response data"", ""When Azure AI Document Intelligence completes its processing, it sends a response back to the function app""],
         [""Function App"", ""receives"", ""Response data"", ""The function app places the information into a data structure based on the response from Azure AI Document Intelligence""],
         [""JSON data file"", ""created-by"", ""Function App"", ""The function app creates a JSON data file that contains the response data""],
         [""JSON data file"", ""saved-to"", ""Third container in Data Lake Storage"", ""The JSON data file is stored to a third container in Data Lake Storage""],
         [""Processed data"", ""sent-to"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""],
         [""Azure Cosmos DB"", ""stores"", ""Data"", ""Azure Cosmos DB saves the data in a database and in collections""]
      ]
   }"," {
      ""entities"": [
         [""Outlook Email Account"", ""person"", ""A designated email account that receives PDF files as attachments""],
         [""Logic App"", ""component"", ""A program built using Azure Logic Apps that processes emails and uploads PDF files to Azure Data Lake Storage""],
         [""Azure Data Lake Storage"", ""system"", ""A storage service provided by Microsoft Azure where the PDF files are stored""],
         [""Function App"", ""component"", ""A program built using Azure Functions that processes the single-page PDF files""],
         [""Azure AI Document Intelligence"", ""component"", ""An Azure REST API used for processing the single-page PDF files""],
         [""Azure Cosmos DB"", ""system"", ""A database service provided by Microsoft Azure where the processed data is saved""]
      ],
      ""relationships"": [
         [""Logic App"", ""part-of"", ""Azure Logic Apps"", ""The logic app is built using the capabilities of Azure Logic Apps""],
         [""Outlook Email Account"", ""calls"", ""Logic App"", ""The arrival of an email triggers the logic app to process the email""],
         [""PDF files"", ""part-of"", ""Email"", ""The PDF files are attached to the email""],
         [""PDF files"", ""uploaded-to"", ""Azure Data Lake Storage"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""],
         [""Manually or programmatically uploaded PDF files"", ""uploaded-to"", ""Azure Data Lake Storage"", ""You can manually or programmatically upload PDF files to the same container in Data Lake Storage""],
         [""PDF file in the PDF container"", ""calls"", ""Function App"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""],
         [""Function App"", ""uses"", ""HTTPS POST"", ""The function app sends the location of the single-page PDF file to AI Document Intelligence using HTTPS POST""],
         [""AI Document Intelligence"", ""called-from"", ""Function App"", ""Azure AI Document Intelligence is called from the function app for processing""],
         [""Single-page PDF files"", ""saved-to"", ""Second container in Data Lake Storage"", ""Split files are saved to a second container in Data Lake Storage""],
         [""Function App"", ""sends"", ""Location of single-page PDF file"", ""The function app sends the location of the single-page PDF file to AI Document Intelligence for processing""],
         [""AI Document Intelligence"", ""returns"", ""Response data"", ""When Azure AI Document Intelligence completes its processing, it sends a response back to the function app""],
         [""Function App"", ""receives"", ""Response data"", ""The function app places the information into a data structure based on the response from Azure AI Document Intelligence""],
         [""JSON data file"", ""created-by"", ""Function App"", ""The function app creates a JSON data file that contains the response data""],
         [""JSON data file"", ""saved-to"", ""Third container in Data Lake Storage"", ""The JSON data file is stored to a third container in Data Lake Storage""],
         [""Processed data"", ""sent-to"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""],
         [""Azure Cosmos DB"", ""stores"", ""Data"", ""Azure Cosmos DB saves the data in a database and in collections""]
      ]
   }",8.13487917900784
8,mistral:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Azure where audio files and transcription results are stored""],
         [""Azure Storage account"", ""component"", ""An account used for storing audio files and transcriptions in Azure Blob Storage""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool used for uploading audio files to an Azure Storage account""],
         [""Storage SDK or API"", ""component"", ""Software Development Kit (SDK) or Application Programming Interface (API) used for uploading audio files to an Azure Storage account""],
         [""Azure AI Speech"", ""system"", ""An Azure service that transcribes audio files asynchronously with speaker diarization enabled""],
         [""Batch mode transcription"", ""component"", ""A method of transcribing audio files in large quantities using Azure AI Speech""],
         [""speaker diarization"", ""component"", ""The feature in Azure AI Speech that identifies different speakers in the audio file""],
         [""Azure AI Language"", ""system"", ""An Azure service that detects and redacts personal data in transcripts""],
         [""personal data"", ""entity"", ""Sensitive information such as phone numbers, addresses, or names""],
         [""Ingestion Client tool"", ""component"", ""A no-code approach for call center transcription using the AI services provided by Azure""],
         [""Azure OpenAI"", ""system"", ""An Azure service that processes transcripts and extracts entities, summarizes conversations, and analyzes sentiments""],
         [""Power BI"", ""entity"", ""A business analytics service used to visualize data and create reports""],
         [""App Service"", ""component"", ""A platform for hosting web applications provided by Azure""],
         [""custom web application"", ""entity"", ""A web application tailored to specific needs or preferences""],
         [""Power BI or custom web application"", ""entity"", ""Visualization options for the processed output from Azure OpenAI""],
         [""Azure CRM"", ""component"", ""A customer relationship management system used for storing and managing data about customers, including transcription information""],
         [""near real-time insights"", ""component"", ""Timely information provided by Power BI or custom web application that offers contextual information about the conversation""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage account""],
         [""Azure Storage account"", ""part-of"", ""Azure Blob Storage""],
         [""Azure Storage Explorer"", ""calls"", ""Azure Storage account""],
         [""Storage SDK or API"", ""calls"", ""Azure Storage account""],
         [""Azure AI Speech"", ""contains"", ""Batch mode transcription""],
         [""Batch mode transcription"", ""part-of"", ""Azure AI Speech""],
         [""Azure AI Language"", ""called-from"", ""transcript""],
         [""Ingestion Client tool"", ""calls"", ""AI services Ingestion Client tool""],
         [""Azure OpenAI"", ""contains"", ""Power BI""],
         [""Azure OpenAI"", ""contains"", ""custom web application""],
         [""Power BI"", ""called-from"", ""transcription information""],
         [""App Service"", ""part-of"", ""custom web application""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Azure where audio files and transcription results are stored""],
         [""Azure Storage account"", ""component"", ""An account used for storing audio files and transcriptions in Azure Blob Storage""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool used for uploading audio files to an Azure Storage account""],
         [""Storage SDK or API"", ""component"", ""Software Development Kit (SDK) or Application Programming Interface (API) used for uploading audio files to an Azure Storage account""],
         [""Azure AI Speech"", ""system"", ""An Azure service that transcribes audio files asynchronously with speaker diarization enabled""],
         [""Batch mode transcription"", ""component"", ""A method of transcribing audio files in large quantities using Azure AI Speech""],
         [""speaker diarization"", ""component"", ""The feature in Azure AI Speech that identifies different speakers in the audio file""],
         [""Azure AI Language"", ""system"", ""An Azure service that detects and redacts personal data in transcripts""],
         [""personal data"", ""entity"", ""Sensitive information such as phone numbers, addresses, or names""],
         [""Ingestion Client tool"", ""component"", ""A no-code approach for call center transcription using the AI services provided by Azure""],
         [""Azure OpenAI"", ""system"", ""An Azure service that processes transcripts and extracts entities, summarizes conversations, and analyzes sentiments""],
         [""Power BI"", ""entity"", ""A business analytics service used to visualize data and create reports""],
         [""App Service"", ""component"", ""A platform for hosting web applications provided by Azure""],
         [""custom web application"", ""entity"", ""A web application tailored to specific needs or preferences""],
         [""Power BI or custom web application"", ""entity"", ""Visualization options for the processed output from Azure OpenAI""],
         [""Azure CRM"", ""component"", ""A customer relationship management system used for storing and managing data about customers, including transcription information""],
         [""near real-time insights"", ""component"", ""Timely information provided by Power BI or custom web application that offers contextual information about the conversation""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage account""],
         [""Azure Storage account"", ""part-of"", ""Azure Blob Storage""],
         [""Azure Storage Explorer"", ""calls"", ""Azure Storage account""],
         [""Storage SDK or API"", ""calls"", ""Azure Storage account""],
         [""Azure AI Speech"", ""contains"", ""Batch mode transcription""],
         [""Batch mode transcription"", ""part-of"", ""Azure AI Speech""],
         [""Azure AI Language"", ""called-from"", ""transcript""],
         [""Ingestion Client tool"", ""calls"", ""AI services Ingestion Client tool""],
         [""Azure OpenAI"", ""contains"", ""Power BI""],
         [""Azure OpenAI"", ""contains"", ""custom web application""],
         [""Power BI"", ""called-from"", ""transcription information""],
         [""App Service"", ""part-of"", ""custom web application""]
      ]
   }",7.6662285680067725
9,mistral:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""The entry point for incoming requests to the application.""],
         [""API Management"", ""component"", ""The service that validates the bearer token and forwards requests to appropriate backend APIs.""],
         [""Azure AD B2C"", ""system"", ""The Identity Provider responsible for authenticating users and issuing JWT tokens.""],
         [""Custom Domain (sign-in)"", ""component"", ""A custom configured domain used for user sign-in.""],
         [""Backend API"", ""component"", ""The service or application that is accessed after authentication.""],
         [""App Service Environment"", ""component"", ""One of the environments where backend APIs can run.""],
         [""Function Apps"", ""component"", ""Another environment where backend APIs can run.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""Yet another environment where backend APIs can run.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management""],
         [""API Management"", ""contains"", ""Custom Domain (sign-in)""],
         [""API Management"", ""calls"", ""Backend API""],
         [""Backend API"", ""called-from"", ""API Management""],
         [""API Management"", ""part-of"", ""App Service Environment""],
         [""API Management"", ""part-of"", ""Function Apps""],
         [""API Management"", ""part-of"", ""Azure Kubernetes Services (AKS)""]
      ]
   }"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""The entry point for incoming requests to the application.""],
         [""API Management"", ""component"", ""The service that validates the bearer token and forwards requests to appropriate backend APIs.""],
         [""Azure AD B2C"", ""system"", ""The Identity Provider responsible for authenticating users and issuing JWT tokens.""],
         [""Custom Domain (sign-in)"", ""component"", ""A custom configured domain used for user sign-in.""],
         [""Backend API"", ""component"", ""The service or application that is accessed after authentication.""],
         [""App Service Environment"", ""component"", ""One of the environments where backend APIs can run.""],
         [""Function Apps"", ""component"", ""Another environment where backend APIs can run.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""Yet another environment where backend APIs can run.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management""],
         [""API Management"", ""contains"", ""Custom Domain (sign-in)""],
         [""API Management"", ""calls"", ""Backend API""],
         [""Backend API"", ""called-from"", ""API Management""],
         [""API Management"", ""part-of"", ""App Service Environment""],
         [""API Management"", ""part-of"", ""Function Apps""],
         [""API Management"", ""part-of"", ""Azure Kubernetes Services (AKS)""]
      ]
   }",3.994846381014213
10,mistral:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps""],
         [""Logic Apps"", ""component"", ""Uses secured template parameters to retrieve database credentials from Azure Key Vault and uses Transport Layer Security (TLS) to send a database statement and the retrieved credentials to an on-premises data gateway.""],
         [""Azure Key Vault"", ""component"", ""Stores the database credentials""],
         [""Transport Layer Security (TLS)"", ""component"", ""Used by Logic Apps to securely send the database credentials and a database statement to the on-premises data gateway""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database to run the received statement""],
         [""SQL Server"", ""system"", ""Stores the data and makes it available to apps that users access""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance""]
      ],
      ""relationships"": [
         [""API Management"", ""calls"", ""Logic Apps""],
         [""Logic Apps"", ""part-of"", ""API Management""],
         [""Logic Apps"", ""contains"", ""Azure Key Vault""],
         [""Logic Apps"", ""uses"", ""Transport Layer Security (TLS)""],
         [""on-premises data gateway"", ""part-of"", ""SQL Server""],
         [""Logic Apps"", ""calls"", ""on-premises data gateway""]
      ]
   }"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps""],
         [""Logic Apps"", ""component"", ""Uses secured template parameters to retrieve database credentials from Azure Key Vault and uses Transport Layer Security (TLS) to send a database statement and the retrieved credentials to an on-premises data gateway.""],
         [""Azure Key Vault"", ""component"", ""Stores the database credentials""],
         [""Transport Layer Security (TLS)"", ""component"", ""Used by Logic Apps to securely send the database credentials and a database statement to the on-premises data gateway""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database to run the received statement""],
         [""SQL Server"", ""system"", ""Stores the data and makes it available to apps that users access""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance""]
      ],
      ""relationships"": [
         [""API Management"", ""calls"", ""Logic Apps""],
         [""Logic Apps"", ""part-of"", ""API Management""],
         [""Logic Apps"", ""contains"", ""Azure Key Vault""],
         [""Logic Apps"", ""uses"", ""Transport Layer Security (TLS)""],
         [""on-premises data gateway"", ""part-of"", ""SQL Server""],
         [""Logic Apps"", ""calls"", ""on-premises data gateway""]
      ]
   }",4.148232072999235
11,mistral:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway that receives POST requests""],
         [""CreatePatient function in Patient API"", ""component"", ""function called by APIM to create a patient document""],
         [""Patient API"", ""system"", ""API responsible for managing patient data""],
         [""Audit API CreateAuditRecord function"", ""component"", ""function called by the CreatePatient function in Patient API""],
         [""Audit API"", ""system"", ""API responsible for creating audit records""],
         [""CreateAuditRecord function in the Audit API function app"", ""component"", ""function that creates an audit record""],
         [""Azure Cosmos DB"", ""system"", ""Database used to store patient and audit documents""],
         [""outside services and clients"", ""person"", ""entities making POST requests to APIM""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient function in Patient API"", ""APIM calls this function with given patient information""],
         [""CreatePatient function in Patient API"", ""calls"", ""Audit API CreateAuditRecord function"", ""This function is called to create an audit record""],
         [""Audit API CreateAuditRecord function"", ""part-of"", ""Audit API"", ""The CreateAuditRecord function belongs to the Audit API""],
         [""CreateAuditRecord function in the Audit API function app"", ""part-of"", ""Audit API"", ""The CreateAuditRecord function is a part of the Audit API""],
         [""CreateAuditRecord function in the Audit API function app"", ""contains"", ""Azure Cosmos DB"", ""This function creates and stores audit records in Azure Cosmos DB""]
      ]
   }"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway that receives POST requests""],
         [""CreatePatient function in Patient API"", ""component"", ""function called by APIM to create a patient document""],
         [""Patient API"", ""system"", ""API responsible for managing patient data""],
         [""Audit API CreateAuditRecord function"", ""component"", ""function called by the CreatePatient function in Patient API""],
         [""Audit API"", ""system"", ""API responsible for creating audit records""],
         [""CreateAuditRecord function in the Audit API function app"", ""component"", ""function that creates an audit record""],
         [""Azure Cosmos DB"", ""system"", ""Database used to store patient and audit documents""],
         [""outside services and clients"", ""person"", ""entities making POST requests to APIM""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient function in Patient API"", ""APIM calls this function with given patient information""],
         [""CreatePatient function in Patient API"", ""calls"", ""Audit API CreateAuditRecord function"", ""This function is called to create an audit record""],
         [""Audit API CreateAuditRecord function"", ""part-of"", ""Audit API"", ""The CreateAuditRecord function belongs to the Audit API""],
         [""CreateAuditRecord function in the Audit API function app"", ""part-of"", ""Audit API"", ""The CreateAuditRecord function is a part of the Audit API""],
         [""CreateAuditRecord function in the Audit API function app"", ""contains"", ""Azure Cosmos DB"", ""This function creates and stores audit records in Azure Cosmos DB""]
      ]
   }",4.39269988698652
12,mistral:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway that receives POST requests from outside services and clients""],
         [""CreatePatient"", ""component"", ""Function in Patient API that processes the patient data provided by APIM""],
         [""Patient API"", ""system"", ""API that handles the creation of patients""],
         [""Audit API"", ""system"", ""API responsible for creating audit records""],
         [""CreateAuditRecord"", ""component"", ""Function in Audit API that creates an audit record""],
         [""Azure Cosmos DB"", ""system"", ""Database used to store patient documents and audit records""],
         [""outside services and clients"", ""person"", ""Entities making a POST request to APIM""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient"", ""Sends a POST request with patient information to the CreatePatient function""],
         [""CreatePatient"", ""part-of"", ""Patient API"", ""Is part of the Patient API system""],
         [""CreatePatient"", ""calls"", ""CreateAuditRecord"", ""Calls the CreateAuditRecord function in Audit API""],
         [""CreateAuditRecord"", ""part-of"", ""Audit API"", ""Is part of the Audit API system""],
         [""CreateAuditRecord"", ""contains"", ""Azure Cosmos DB"", ""Creates the audit record in Azure Cosmos DB""],
         [""Patient API"", ""contains"", ""CreatePatient"", ""Contains the CreatePatient function""],
         [""Patient API"", ""contains"", ""Azure Cosmos DB"", ""Stores patient documents in Azure Cosmos DB""]
      ]
   }"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway that receives POST requests from outside services and clients""],
         [""CreatePatient"", ""component"", ""Function in Patient API that processes the patient data provided by APIM""],
         [""Patient API"", ""system"", ""API that handles the creation of patients""],
         [""Audit API"", ""system"", ""API responsible for creating audit records""],
         [""CreateAuditRecord"", ""component"", ""Function in Audit API that creates an audit record""],
         [""Azure Cosmos DB"", ""system"", ""Database used to store patient documents and audit records""],
         [""outside services and clients"", ""person"", ""Entities making a POST request to APIM""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient"", ""Sends a POST request with patient information to the CreatePatient function""],
         [""CreatePatient"", ""part-of"", ""Patient API"", ""Is part of the Patient API system""],
         [""CreatePatient"", ""calls"", ""CreateAuditRecord"", ""Calls the CreateAuditRecord function in Audit API""],
         [""CreateAuditRecord"", ""part-of"", ""Audit API"", ""Is part of the Audit API system""],
         [""CreateAuditRecord"", ""contains"", ""Azure Cosmos DB"", ""Creates the audit record in Azure Cosmos DB""],
         [""Patient API"", ""contains"", ""CreatePatient"", ""Contains the CreatePatient function""],
         [""Patient API"", ""contains"", ""Azure Cosmos DB"", ""Stores patient documents in Azure Cosmos DB""]
      ]
   }",4.221639547031373
13,mistral:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}"," {
      ""entities"": [
         [""Connect Agent"", ""component"", ""Captures change logs using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""Publisher (Mainframe)"", ""component"", ""Manages data migration for mainframe systems. Handles commit and rollback of transactions for each unit of work, maintaining the integrity of data. Initiates a connection to the Connect Replicator Engine.""],
         [""Listener (Midrange)"", ""component"", ""Manages data migration for midrange systems. Located on either a Windows or Linux machine. Handles commit and rollback of transactions for each unit of work, maintaining the integrity of data. Initiates a connection to the Connect Replicator Engine.""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the Publisher or Listener and applies it to the target.""],
         [""Target"", ""system"", ""Receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""service"", ""Consumes the changed data and applies it to Azure data platform services.""],
         [""Connect Controller Daemon"", ""component"", ""Authenticates the request and establishes the socket connection between the Publisher or Listener and the Replicator Engine.""]
      ],
      ""relationships"": [
         [""Connect Agent"", ""part-of"", ""Publisher (Mainframe)""],
         [""Connect Agent"", ""part-of"", ""Listener (Midrange)""],
         [""Publisher (Mainframe)"", ""calls"", ""Connect Replicator Engine""],
         [""Listener (Midrange)"", ""calls"", ""Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""contains"", ""Target""],
         [""Target"", ""called-from"", ""Azure Databricks""]
      ]
   }"," {
      ""entities"": [
         [""Connect Agent"", ""component"", ""Captures change logs using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""Publisher (Mainframe)"", ""component"", ""Manages data migration for mainframe systems. Handles commit and rollback of transactions for each unit of work, maintaining the integrity of data. Initiates a connection to the Connect Replicator Engine.""],
         [""Listener (Midrange)"", ""component"", ""Manages data migration for midrange systems. Located on either a Windows or Linux machine. Handles commit and rollback of transactions for each unit of work, maintaining the integrity of data. Initiates a connection to the Connect Replicator Engine.""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the Publisher or Listener and applies it to the target.""],
         [""Target"", ""system"", ""Receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""service"", ""Consumes the changed data and applies it to Azure data platform services.""],
         [""Connect Controller Daemon"", ""component"", ""Authenticates the request and establishes the socket connection between the Publisher or Listener and the Replicator Engine.""]
      ],
      ""relationships"": [
         [""Connect Agent"", ""part-of"", ""Publisher (Mainframe)""],
         [""Connect Agent"", ""part-of"", ""Listener (Midrange)""],
         [""Publisher (Mainframe)"", ""calls"", ""Connect Replicator Engine""],
         [""Listener (Midrange)"", ""calls"", ""Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""contains"", ""Target""],
         [""Target"", ""called-from"", ""Azure Databricks""]
      ]
   }",4.7264135829755105
14,mistral:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}"," {
      ""entities"": [
         [""CDS"", ""system"", ""Cloud Data Storage (CDS)""],
         [""SQL"", ""system"", ""Structured Query Language (SQL) database""],
         [""Data Factory"", ""component"", ""Azure service that manages data movement and integration between different data stores""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning (ERP) software used to manage business operations""],
         [""Azure SQL database"", ""component"", ""SQL database hosted on Microsoft Azure cloud platform""],
         [""Azure Functions"", ""component"", ""Serverless compute service provided by Microsoft Azure for executing code in response to events or HTTP requests""],
         [""API"", ""component"", ""Application Programming Interface, a set of rules and protocols that allow programs to communicate with each other""],
         [""Power Apps"", ""component"", ""Microsoft platform for building and using custom business applications""],
         [""Power BI"", ""component"", ""Business intelligence service provided by Microsoft that enables data visualization and analysis""]
      ],
      ""relationships"": [
         [""Data Factory"", ""part-of"", ""CDS""],
         [""PO data stored in ERP system"", ""called-from"", ""Azure SQL database""],
         [""Azure Functions"", ""calls"", ""API""],
         [""Power Apps"", ""calls"", ""Azure SQL Database""],
         [""Power Apps"", ""part-of"", ""CDS""],
         [""User updates POs in Power Apps"", ""called-from"", ""Supplier data stored in CDS""],
         [""CSV export"", ""called-from"", ""Power Apps""]
      ]
   }"," {
      ""entities"": [
         [""CDS"", ""system"", ""Cloud Data Storage (CDS)""],
         [""SQL"", ""system"", ""Structured Query Language (SQL) database""],
         [""Data Factory"", ""component"", ""Azure service that manages data movement and integration between different data stores""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning (ERP) software used to manage business operations""],
         [""Azure SQL database"", ""component"", ""SQL database hosted on Microsoft Azure cloud platform""],
         [""Azure Functions"", ""component"", ""Serverless compute service provided by Microsoft Azure for executing code in response to events or HTTP requests""],
         [""API"", ""component"", ""Application Programming Interface, a set of rules and protocols that allow programs to communicate with each other""],
         [""Power Apps"", ""component"", ""Microsoft platform for building and using custom business applications""],
         [""Power BI"", ""component"", ""Business intelligence service provided by Microsoft that enables data visualization and analysis""]
      ],
      ""relationships"": [
         [""Data Factory"", ""part-of"", ""CDS""],
         [""PO data stored in ERP system"", ""called-from"", ""Azure SQL database""],
         [""Azure Functions"", ""calls"", ""API""],
         [""Power Apps"", ""calls"", ""Azure SQL Database""],
         [""Power Apps"", ""part-of"", ""CDS""],
         [""User updates POs in Power Apps"", ""called-from"", ""Supplier data stored in CDS""],
         [""CSV export"", ""called-from"", ""Power Apps""]
      ]
   }",4.131170250009745
15,mistral:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}"," {
      ""entities"": [
         [""Airline System"", ""System"", ""The system responsible for managing airline operations.""],
         [""Custom API Hosted in Azure API Management"", ""Component"", ""API that communicates with the Airline System.""],
         [""Custom API Coordinator"", ""Component"", ""Handles incoming messages from the Airline System and sends them to Power Apps.""],
         [""Power Apps"", ""System"", ""A system that assigns flights to Microsoft Teams channels.""],
         [""Microsoft Teams Channels"", ""Component"", ""Used for assigning flights in the given context.""],
         [""Azure Storage Account Queue"", ""Component"", ""Used for queuing Graph API calls for further processing.""],
         [""Azure Functions"", ""Component"", ""Responsible for running Graph API calls based on messages in the storage queue.""],
         [""Graph API"", ""Component"", ""Used for sending notifications to Teams and streaming events to Azure Event Hubs.""],
         [""Azure Event Hubs"", ""System"", ""Used for further analytics of all events streamed from the system.""],
         [""Custom Bot Messaging Service"", ""Component"", ""Manages the airline's notification system and sends flight updates through Teams.""],
         [""Azure Bot Service"", ""Component"", ""Provides a platform for creating custom bots, in this case, the messaging service.""],
         [""Azure Data Lake Storage"", ""System"", ""Offers long-term retention and micro-batch processing of events from Event Hubs.""],
         [""Power BI"", ""Component"", ""Used for generating insightful reports based on data from Azure Data Lake.""]
      ],
      ""relationships"": [
         [""Airline System"", ""communicates-with"", ""Custom API Hosted in Azure API Management""],
         [""Custom API Coordinator"", ""part-of"", ""Power Apps""],
         [""Custom API Coordinator"", ""sends-to"", ""Microsoft Teams Channels""],
         [""User"", ""queues-a-call-in"", ""Azure Storage Account Queue""],
         [""Azure Functions"", ""runs-based-on"", ""Azure Storage Account Queue""],
         [""Azure Functions"", ""calls"", ""Graph API""],
         [""Azure Functions"", ""streams-to"", ""Azure Event Hubs""],
         [""Custom Bot Messaging Service"", ""employs"", ""Azure Bot Service""],
         [""Custom Bots"", ""send-updates-to"", ""Microsoft Teams""],
         [""Azure Data Lake Storage"", ""offers-long-term-retention-and-processing-for"", ""Azure Event Hubs""],
         [""Power BI"", ""generates-reports-based-on"", ""Azure Data Lake""]
      ]
   }"," {
      ""entities"": [
         [""Airline System"", ""System"", ""The system responsible for managing airline operations.""],
         [""Custom API Hosted in Azure API Management"", ""Component"", ""API that communicates with the Airline System.""],
         [""Custom API Coordinator"", ""Component"", ""Handles incoming messages from the Airline System and sends them to Power Apps.""],
         [""Power Apps"", ""System"", ""A system that assigns flights to Microsoft Teams channels.""],
         [""Microsoft Teams Channels"", ""Component"", ""Used for assigning flights in the given context.""],
         [""Azure Storage Account Queue"", ""Component"", ""Used for queuing Graph API calls for further processing.""],
         [""Azure Functions"", ""Component"", ""Responsible for running Graph API calls based on messages in the storage queue.""],
         [""Graph API"", ""Component"", ""Used for sending notifications to Teams and streaming events to Azure Event Hubs.""],
         [""Azure Event Hubs"", ""System"", ""Used for further analytics of all events streamed from the system.""],
         [""Custom Bot Messaging Service"", ""Component"", ""Manages the airline's notification system and sends flight updates through Teams.""],
         [""Azure Bot Service"", ""Component"", ""Provides a platform for creating custom bots, in this case, the messaging service.""],
         [""Azure Data Lake Storage"", ""System"", ""Offers long-term retention and micro-batch processing of events from Event Hubs.""],
         [""Power BI"", ""Component"", ""Used for generating insightful reports based on data from Azure Data Lake.""]
      ],
      ""relationships"": [
         [""Airline System"", ""communicates-with"", ""Custom API Hosted in Azure API Management""],
         [""Custom API Coordinator"", ""part-of"", ""Power Apps""],
         [""Custom API Coordinator"", ""sends-to"", ""Microsoft Teams Channels""],
         [""User"", ""queues-a-call-in"", ""Azure Storage Account Queue""],
         [""Azure Functions"", ""runs-based-on"", ""Azure Storage Account Queue""],
         [""Azure Functions"", ""calls"", ""Graph API""],
         [""Azure Functions"", ""streams-to"", ""Azure Event Hubs""],
         [""Custom Bot Messaging Service"", ""employs"", ""Azure Bot Service""],
         [""Custom Bots"", ""send-updates-to"", ""Microsoft Teams""],
         [""Azure Data Lake Storage"", ""offers-long-term-retention-and-processing-for"", ""Azure Event Hubs""],
         [""Power BI"", ""generates-reports-based-on"", ""Azure Data Lake""]
      ]
   }",6.611503290012479
16,mistral:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}"," {
      ""entities"": [
         [""Outlook Email Account"", ""person"", ""A designated email account that receives PDF files as attachments""],
         [""Logic App"", ""component"", ""A software component built using Azure Logic Apps that processes emails and uploads PDF files to a container in Azure Data Lake Storage""],
         [""Azure Data Lake Storage"", ""system"", ""A storage system used for storing PDF files""],
         [""PDF Container"", ""component"", ""A specific location within the Azure Data Lake Storage where PDF files are stored""],
         [""Function App"", ""component"", ""A software component built using Azure Functions that processes single-page PDF files and sends them to AI Document Intelligence""],
         [""Azure REST API"", ""component"", ""A software component used by the Function App for sending requests to AI Document Intelligence""],
         [""AI Document Intelligence"", ""component"", ""An Azure service that processes single-page PDF files""],
         [""Data Structure"", ""component"", ""A data format used by the Function App for storing processed information""],
         [""JSON Data File"", ""component"", ""A specific file type containing response data""],
         [""Third Container in Data Lake Storage"", ""component"", ""A second location within the Azure Data Lake Storage where split PDF files are stored""],
         [""Azure Cosmos DB"", ""system"", ""A database system used for storing processed data from PDF forms""],
         [""Power BI"", ""component"", ""A software component that obtains data from Azure Cosmos DB and provides insights and dashboards""]
      ],
      ""relationships"": [
         [""Outlook Email Account"", ""calls"", ""Logic App"", ""The arrival of an email triggers the Logic app""],
         [""Logic App"", ""part-of"", ""Azure Data Lake Storage"", ""The logic app uploads PDF files to a container in Azure Data Lake Storage""],
         [""PDF Container"", ""part-of"", ""Azure Data Lake Storage"", ""The PDF container is a specific location within the Azure Data Lake Storage""],
         [""Function App"", ""calls"", ""AI Document Intelligence"", ""The Function app sends the location of single-page PDF files to AI Document Intelligence""],
         [""Function App"", ""part-of"", ""Third Container in Data Lake Storage"", ""Split files are saved to a second container in Data Lake Storage""],
         [""Function App"", ""calls"", ""Azure Cosmos DB"", ""The function app sends the processed data to Azure Cosmos DB""]
      ]
   }"," {
      ""entities"": [
         [""Outlook Email Account"", ""person"", ""A designated email account that receives PDF files as attachments""],
         [""Logic App"", ""component"", ""A software component built using Azure Logic Apps that processes emails and uploads PDF files to a container in Azure Data Lake Storage""],
         [""Azure Data Lake Storage"", ""system"", ""A storage system used for storing PDF files""],
         [""PDF Container"", ""component"", ""A specific location within the Azure Data Lake Storage where PDF files are stored""],
         [""Function App"", ""component"", ""A software component built using Azure Functions that processes single-page PDF files and sends them to AI Document Intelligence""],
         [""Azure REST API"", ""component"", ""A software component used by the Function App for sending requests to AI Document Intelligence""],
         [""AI Document Intelligence"", ""component"", ""An Azure service that processes single-page PDF files""],
         [""Data Structure"", ""component"", ""A data format used by the Function App for storing processed information""],
         [""JSON Data File"", ""component"", ""A specific file type containing response data""],
         [""Third Container in Data Lake Storage"", ""component"", ""A second location within the Azure Data Lake Storage where split PDF files are stored""],
         [""Azure Cosmos DB"", ""system"", ""A database system used for storing processed data from PDF forms""],
         [""Power BI"", ""component"", ""A software component that obtains data from Azure Cosmos DB and provides insights and dashboards""]
      ],
      ""relationships"": [
         [""Outlook Email Account"", ""calls"", ""Logic App"", ""The arrival of an email triggers the Logic app""],
         [""Logic App"", ""part-of"", ""Azure Data Lake Storage"", ""The logic app uploads PDF files to a container in Azure Data Lake Storage""],
         [""PDF Container"", ""part-of"", ""Azure Data Lake Storage"", ""The PDF container is a specific location within the Azure Data Lake Storage""],
         [""Function App"", ""calls"", ""AI Document Intelligence"", ""The Function app sends the location of single-page PDF files to AI Document Intelligence""],
         [""Function App"", ""part-of"", ""Third Container in Data Lake Storage"", ""Split files are saved to a second container in Data Lake Storage""],
         [""Function App"", ""calls"", ""Azure Cosmos DB"", ""The function app sends the processed data to Azure Cosmos DB""]
      ]
   }",6.030795352009591
17,mistral:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""API gateway service that handles incoming requests.""],
         [""Azure AD B2C"", ""system"", ""Identity Provider for user authentication and authorization.""],
         [""Custom Domain"", ""component"", ""Domain name configured in Azure AD B2C for sign-in.""],
         [""JSON Web Token (JWT)"", ""component"", ""Security token used to pass user identity information from the IdP to the client.""],
         [""API Management"", ""system"", ""API service that manages and secures APIs by providing features such as authentication, rate limiting, caching, etc.""],
         [""validate-jwt policy"", ""component"", ""Policy in API Management for validating JWT tokens.""],
         [""App Service Environment"", ""component"", ""Azure PaaS service for hosting web apps and APIs.""],
         [""Function Apps"", ""component"", ""Azure serverless compute service for running event-driven code.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""Managed Kubernetes service in Azure for deploying containerized applications.""],
         [""Backend API"", ""component"", ""API that provides access to an application's functionality.""],
         [""click of a button on a web application or on a mobile device"", ""event"", ""User action that triggers an event leading to the request of backend API.""],
         [""direct call to the endpoint of the backend API"", ""event"", ""Manual invocation of the backend API's endpoint.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management""],
         [""Azure AD B2C"", ""calls"", [""Custom Domain""]],
         [""Azure Front Door"", ""contains"", [""validate-jwt policy""]],
         [""API Management"", ""part-of"", [""App Service Environment"", ""Function Apps"", ""Azure Kubernetes Services (AKS)""]],
         [""API Management"", ""calls"", [""Backend API""]],
         [""Backend API"", ""called-from"", [""click of a button on a web application or on a mobile device"", ""direct call to the endpoint of the backend API""]]
      ]
   }"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""API gateway service that handles incoming requests.""],
         [""Azure AD B2C"", ""system"", ""Identity Provider for user authentication and authorization.""],
         [""Custom Domain"", ""component"", ""Domain name configured in Azure AD B2C for sign-in.""],
         [""JSON Web Token (JWT)"", ""component"", ""Security token used to pass user identity information from the IdP to the client.""],
         [""API Management"", ""system"", ""API service that manages and secures APIs by providing features such as authentication, rate limiting, caching, etc.""],
         [""validate-jwt policy"", ""component"", ""Policy in API Management for validating JWT tokens.""],
         [""App Service Environment"", ""component"", ""Azure PaaS service for hosting web apps and APIs.""],
         [""Function Apps"", ""component"", ""Azure serverless compute service for running event-driven code.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""Managed Kubernetes service in Azure for deploying containerized applications.""],
         [""Backend API"", ""component"", ""API that provides access to an application's functionality.""],
         [""click of a button on a web application or on a mobile device"", ""event"", ""User action that triggers an event leading to the request of backend API.""],
         [""direct call to the endpoint of the backend API"", ""event"", ""Manual invocation of the backend API's endpoint.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management""],
         [""Azure AD B2C"", ""calls"", [""Custom Domain""]],
         [""Azure Front Door"", ""contains"", [""validate-jwt policy""]],
         [""API Management"", ""part-of"", [""App Service Environment"", ""Function Apps"", ""Azure Kubernetes Services (AKS)""]],
         [""API Management"", ""calls"", [""Backend API""]],
         [""Backend API"", ""called-from"", [""click of a button on a web application or on a mobile device"", ""direct call to the endpoint of the backend API""]]
      ]
   }",5.500758951995522
18,mistral:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}"," {
      ""entities"": [
         [""Azure Logic Apps"", ""component"", ""A workflow automation tool in Microsoft Azure that handles HTTP requests and integrates with various services using connectors.""],
         [""API Management"", ""system"", ""A system in Microsoft Azure that consists of an API gateway for routing HTTP calls to backend services, and a developer portal for providing access to documentation, code samples, testing APIs.""],
         [""Azure API Gateway"", ""component"", ""A component within the API Management system that accepts HTTP calls and routes them to backend services.""],
         [""Developer Portal"", ""component"", ""A component within the API Management system that provides developers access to documentation, code samples, testing APIs, and other resources.""],
         [""Azure DNS"", ""system"", ""A name resolution service provided by Microsoft Azure.""],
         [""Custom Domain Name"", ""component"", ""A domain name used for mapping to an IP address in Azure DNS.""],
         [""Microsoft Entra ID"", ""system"", ""An authentication system that supports the OpenID Connect protocol and helps secure access to APIs and developer portals in Microsoft Azure.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""calls"", ""API Management""],
         [""API Management"", ""contains"", ""API Gateway""],
         [""API Management"", ""contains"", ""Developer Portal""],
         [""Azure DNS"", ""part-of"", ""Custom Domain Name""],
         [""API Management"", ""called-from"", ""Microsoft Entra ID""]
      ]
   }"," {
      ""entities"": [
         [""Azure Logic Apps"", ""component"", ""A workflow automation tool in Microsoft Azure that handles HTTP requests and integrates with various services using connectors.""],
         [""API Management"", ""system"", ""A system in Microsoft Azure that consists of an API gateway for routing HTTP calls to backend services, and a developer portal for providing access to documentation, code samples, testing APIs.""],
         [""Azure API Gateway"", ""component"", ""A component within the API Management system that accepts HTTP calls and routes them to backend services.""],
         [""Developer Portal"", ""component"", ""A component within the API Management system that provides developers access to documentation, code samples, testing APIs, and other resources.""],
         [""Azure DNS"", ""system"", ""A name resolution service provided by Microsoft Azure.""],
         [""Custom Domain Name"", ""component"", ""A domain name used for mapping to an IP address in Azure DNS.""],
         [""Microsoft Entra ID"", ""system"", ""An authentication system that supports the OpenID Connect protocol and helps secure access to APIs and developer portals in Microsoft Azure.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""calls"", ""API Management""],
         [""API Management"", ""contains"", ""API Gateway""],
         [""API Management"", ""contains"", ""Developer Portal""],
         [""Azure DNS"", ""part-of"", ""Custom Domain Name""],
         [""API Management"", ""called-from"", ""Microsoft Entra ID""]
      ]
   }",4.078515955014154
19,mistral:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""The document file is stored in this system.""],
         [""Storage Queue"", ""component"", ""The web app adds a command message to this component to initiate pipeline processing.""],
         [""Durable Functions orchestration"", ""system"", ""This system is triggered by the command message.""],
         [""Computer Vision Read API"", ""component"", ""The Scan activity function calls this component.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""The document classifier service is hosted in this system.""],
         [""Document Classifier Service"", ""component"", ""This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file.""],
         [""Azure Cosmos DB store"", ""system"", ""The Metadata Store activity function saves the document type and page range information in this system.""],
         [""Cognitive Search service"", ""system"", ""The Indexing activity function creates a new search document in this system for each identified document type.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Document File""],
         [""Document File"", ""part-of"", ""Storage Queue""],
         [""Storage Queue"", ""part-of"", ""Durable Functions orchestration""],
         [""Durable Functions orchestration"", ""calls"", ""Scan activity function""],
         [""Scan activity function"", ""calls"", ""Computer Vision Read API""],
         [""Document File"", ""part-of"", ""Classify activity function""],
         [""Classify activity function"", ""uses"", ""Document Classifier Service""],
         [""Metadata Store activity function"", ""stores in"", ""Azure Cosmos DB store""],
         [""Indexing activity function"", ""creates in"", ""Cognitive Search service""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""The document file is stored in this system.""],
         [""Storage Queue"", ""component"", ""The web app adds a command message to this component to initiate pipeline processing.""],
         [""Durable Functions orchestration"", ""system"", ""This system is triggered by the command message.""],
         [""Computer Vision Read API"", ""component"", ""The Scan activity function calls this component.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""The document classifier service is hosted in this system.""],
         [""Document Classifier Service"", ""component"", ""This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file.""],
         [""Azure Cosmos DB store"", ""system"", ""The Metadata Store activity function saves the document type and page range information in this system.""],
         [""Cognitive Search service"", ""system"", ""The Indexing activity function creates a new search document in this system for each identified document type.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Document File""],
         [""Document File"", ""part-of"", ""Storage Queue""],
         [""Storage Queue"", ""part-of"", ""Durable Functions orchestration""],
         [""Durable Functions orchestration"", ""calls"", ""Scan activity function""],
         [""Scan activity function"", ""calls"", ""Computer Vision Read API""],
         [""Document File"", ""part-of"", ""Classify activity function""],
         [""Classify activity function"", ""uses"", ""Document Classifier Service""],
         [""Metadata Store activity function"", ""stores in"", ""Azure Cosmos DB store""],
         [""Indexing activity function"", ""creates in"", ""Cognitive Search service""]
      ]
   }",4.700030068983324
20,mistral:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}"," {
      ""entities"": [
         [""Data owners"", ""person"", ""Those who upload datasets into the public blob storage account""],
         [""Public blob storage account"", ""system"", ""Storage account where data owners upload datasets""],
         [""Microsoft-managed keys"", ""component"", ""Keys used for encrypting data in the Public blob storage account""],
         [""Azure Data Factory"", ""component"", ""Tool that starts copying of the uploaded dataset to another storage account""],
         [""Trigger"", ""component"", ""Event that initiates the copying process by Azure Data Factory""],
         [""Private endpoint"", ""component"", ""Endpoint used for accessing the storage account""],
         [""Service principal"", ""component"", ""Identity used for limited permissions in accessing the storage account""],
         [""Import path"", ""component"", ""Location on another storage account where the dataset is copied to""],
         [""Secure environment"", ""system"", ""Environment where researchers access the data and perform research work""],
         [""Azure Virtual Desktop"", ""component"", ""Privileged jump box for researchers to access the secure environment""],
         [""Streaming application"", ""component"", ""Application used by researchers to access the secure environment""],
         [""Data science VMs"", ""component"", ""Virtual Machines in the secure network environment where much of the data preparation is done""],
         [""Secure network environment"", ""system"", ""Network where Data science VMs are provisioned for research work""],
         [""Azure Machine Learning compute"", ""component"", ""Compute that can access the dataset through a private endpoint""],
         [""Machine learning models"", ""component"", ""Models created in the secure environment""],
         [""De-identified data"", ""component"", ""Data with personal information removed""],
         [""Export path"", ""component"", ""Location on the secure storage account where de-identified data is saved""],
         [""Logic app"", ""component"", ""App that sends notification and starts the manual approval process""],
         [""Manual reviewers"", ""person"", ""Those who ensure sensitive data isn't exported""],
         [""Data Factory instance"", ""component"", ""Instance used for moving de-identified data to public storage account""],
         [""Public storage account"", ""system"", ""Storage account where approved data is moved to""]
      ],
      ""relationships"": [
         [""Data owners"", ""uploads"", ""Public blob storage account""],
         [""Public blob storage account"", ""is-encrypted-by"", ""Microsoft-managed keys""],
         [""Azure Data Factory"", ""starts-copying"", ""Uploaded dataset""],
         [""Uploaded dataset"", ""is-copied-to"", ""Import path""],
         [""Import path"", ""is-on"", ""Another storage account""],
         [""Another storage account"", ""can-only-be-accessed-through"", ""Private endpoint""],
         [""Another storage account"", ""has-limited-permissions"", ""Service principal""],
         [""Secure environment"", ""is-accessed-by"", ""Streaming application""],
         [""Secure environment"", ""contains"", ""Data science VMs""],
         [""Data science VMs"", ""perform"", ""Much of the data preparation""],
         [""Secure environment"", ""provides"", ""Azure Machine Learning compute""],
         [""Azure Machine Learning compute"", ""can-access"", ""Dataset""],
         [""De-identified data"", ""is-saved-to"", ""Export path""],
         [""Logic app"", ""starts"", ""Manual approval process""],
         [""Data"", ""requires-manual-review"", ""Manual reviewers""],
         [""Manual reviewers"", ""ensure"", ""Sensitive data isn't exported""],
         [""De-identified data"", ""is-approved-or-denied"", ""Manual reviewers""],
         [""Data Factory instance"", ""moves"", ""De-identified data""],
         [""De-identified data"", ""is-moved-to"", ""Public storage account""]
      ]
   }"," {
      ""entities"": [
         [""Data owners"", ""person"", ""Those who upload datasets into the public blob storage account""],
         [""Public blob storage account"", ""system"", ""Storage account where data owners upload datasets""],
         [""Microsoft-managed keys"", ""component"", ""Keys used for encrypting data in the Public blob storage account""],
         [""Azure Data Factory"", ""component"", ""Tool that starts copying of the uploaded dataset to another storage account""],
         [""Trigger"", ""component"", ""Event that initiates the copying process by Azure Data Factory""],
         [""Private endpoint"", ""component"", ""Endpoint used for accessing the storage account""],
         [""Service principal"", ""component"", ""Identity used for limited permissions in accessing the storage account""],
         [""Import path"", ""component"", ""Location on another storage account where the dataset is copied to""],
         [""Secure environment"", ""system"", ""Environment where researchers access the data and perform research work""],
         [""Azure Virtual Desktop"", ""component"", ""Privileged jump box for researchers to access the secure environment""],
         [""Streaming application"", ""component"", ""Application used by researchers to access the secure environment""],
         [""Data science VMs"", ""component"", ""Virtual Machines in the secure network environment where much of the data preparation is done""],
         [""Secure network environment"", ""system"", ""Network where Data science VMs are provisioned for research work""],
         [""Azure Machine Learning compute"", ""component"", ""Compute that can access the dataset through a private endpoint""],
         [""Machine learning models"", ""component"", ""Models created in the secure environment""],
         [""De-identified data"", ""component"", ""Data with personal information removed""],
         [""Export path"", ""component"", ""Location on the secure storage account where de-identified data is saved""],
         [""Logic app"", ""component"", ""App that sends notification and starts the manual approval process""],
         [""Manual reviewers"", ""person"", ""Those who ensure sensitive data isn't exported""],
         [""Data Factory instance"", ""component"", ""Instance used for moving de-identified data to public storage account""],
         [""Public storage account"", ""system"", ""Storage account where approved data is moved to""]
      ],
      ""relationships"": [
         [""Data owners"", ""uploads"", ""Public blob storage account""],
         [""Public blob storage account"", ""is-encrypted-by"", ""Microsoft-managed keys""],
         [""Azure Data Factory"", ""starts-copying"", ""Uploaded dataset""],
         [""Uploaded dataset"", ""is-copied-to"", ""Import path""],
         [""Import path"", ""is-on"", ""Another storage account""],
         [""Another storage account"", ""can-only-be-accessed-through"", ""Private endpoint""],
         [""Another storage account"", ""has-limited-permissions"", ""Service principal""],
         [""Secure environment"", ""is-accessed-by"", ""Streaming application""],
         [""Secure environment"", ""contains"", ""Data science VMs""],
         [""Data science VMs"", ""perform"", ""Much of the data preparation""],
         [""Secure environment"", ""provides"", ""Azure Machine Learning compute""],
         [""Azure Machine Learning compute"", ""can-access"", ""Dataset""],
         [""De-identified data"", ""is-saved-to"", ""Export path""],
         [""Logic app"", ""starts"", ""Manual approval process""],
         [""Data"", ""requires-manual-review"", ""Manual reviewers""],
         [""Manual reviewers"", ""ensure"", ""Sensitive data isn't exported""],
         [""De-identified data"", ""is-approved-or-denied"", ""Manual reviewers""],
         [""Data Factory instance"", ""moves"", ""De-identified data""],
         [""De-identified data"", ""is-moved-to"", ""Public storage account""]
      ]
   }",9.525434912007768
21,mistral:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A platform for version control and collaboration""],
         [""CI/CD pipeline agent"", ""component"", ""Software that automates continuous integration, testing, and deployment""],
         [""Azure VM"", ""component"", ""Virtual machine running on Microsoft Azure's infrastructure""],
         [""API application"", ""component"", ""An application providing APIs for other applications to interact with""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment""],
         [""Azure API Management"", ""system"", ""A platform for managing, publishing, and securing APIs""],
         [""HOST headers"", ""component"", ""Headers in HTTP requests that define the hostname and port number of the server receiving the request""],
         [""API Management policy"", ""component"", ""Configuration settings for Azure API Management""],
         [""App Service Environment's DNS name"", ""component"", ""Domain Name System (DNS) name associated with the App Service Environment""],
         [""Application Gateway"", ""system"", ""Azure service used for load balancing and web traffic management""],
         [""Azure Private DNS"", ""system"", ""A Azure-provided private DNS service for internal routing""],
         [""API Management's developer and API portal"", ""component"", ""Portal for developers to access APIs provided by Azure API Management""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
         [""API application"", ""called-from"", ""Azure API Management""],
         [""Azure API Management"", ""uses"", ""HOST headers""],
         [""Azure API Management"", ""uses"", ""App Service Environment's DNS name""],
         [""Application Gateway"", ""exposes"", ""API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", ""traffic"", ""between"", ""ASE, API Management, and Application Gateway""],
         [""external users"", ""utilize"", ""API Management's developer portal"", ""via"", ""Application Gateway's public IP""]
      ]
   }"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A platform for version control and collaboration""],
         [""CI/CD pipeline agent"", ""component"", ""Software that automates continuous integration, testing, and deployment""],
         [""Azure VM"", ""component"", ""Virtual machine running on Microsoft Azure's infrastructure""],
         [""API application"", ""component"", ""An application providing APIs for other applications to interact with""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment""],
         [""Azure API Management"", ""system"", ""A platform for managing, publishing, and securing APIs""],
         [""HOST headers"", ""component"", ""Headers in HTTP requests that define the hostname and port number of the server receiving the request""],
         [""API Management policy"", ""component"", ""Configuration settings for Azure API Management""],
         [""App Service Environment's DNS name"", ""component"", ""Domain Name System (DNS) name associated with the App Service Environment""],
         [""Application Gateway"", ""system"", ""Azure service used for load balancing and web traffic management""],
         [""Azure Private DNS"", ""system"", ""A Azure-provided private DNS service for internal routing""],
         [""API Management's developer and API portal"", ""component"", ""Portal for developers to access APIs provided by Azure API Management""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
         [""API application"", ""called-from"", ""Azure API Management""],
         [""Azure API Management"", ""uses"", ""HOST headers""],
         [""Azure API Management"", ""uses"", ""App Service Environment's DNS name""],
         [""Application Gateway"", ""exposes"", ""API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", ""traffic"", ""between"", ""ASE, API Management, and Application Gateway""],
         [""external users"", ""utilize"", ""API Management's developer portal"", ""via"", ""Application Gateway's public IP""]
      ]
   }",5.315253103966825
22,mistral:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Azure where audio files and transcription results are stored.""],
         [""Azure Storage Account"", ""component"", ""An account in Azure Blob Storage used for uploading and storing audio files.""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool provided by Azure for managing Azure Storage Accounts.""],
         [""Azure AI Speech"", ""system"", ""An Azure service used for asynchronous transcription of audio files.""],
         [""Batch mode"", ""description"", ""A method in Azure AI Speech for transcribing multiple audio files at once.""],
         [""Speaker diarization"", ""description"", ""A feature in Azure AI Speech that identifies different speakers during the transcription process.""],
         [""Azure AI Language"", ""system"", ""An Azure service used for detecting and redacting personal data in transcripts.""],
         [""Ingestion Client tool"", ""component"", ""A no-code approach tool for automating call center transcription using Azure services.""],
         [""Azure OpenAI"", ""system"", ""An Azure service used for processing transcripts, extracting entities, summarizing conversations, and analyzing sentiments.""],
         [""Power BI"", ""component"", ""A business analytics service provided by Microsoft that visualizes data and provides near real-time insights.""],
         [""App Service"", ""component"", ""A web hosting service provided by Microsoft for hosting custom web applications.""],
         [""CRM"", ""description"", ""A system used for managing customer relationships, including call center transcriptions.""],
         [""Agents"", ""person"", ""The individuals who handle customer calls in a call center.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage Account""],
         [""Azure AI Speech"", ""calls"", ""Storage SDK or API""],
         [""Azure AI Language"", ""called-from"", ""Transcription results stored in Blob Storage""],
         [""Ingestion Client tool"", ""part-of"", ""Batch mode transcription and personal data detection and redaction""],
         [""Azure OpenAI"", ""calls"", ""Stored transcription information""],
         [""Power BI"", ""called-from"", ""Output stored in Blob Storage""],
         [""Custom web application (App Service)"", ""called-from"", ""Output stored in Blob Storage""],
         [""CRM"", ""called-from"", ""Output stored in Blob Storage""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Azure where audio files and transcription results are stored.""],
         [""Azure Storage Account"", ""component"", ""An account in Azure Blob Storage used for uploading and storing audio files.""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool provided by Azure for managing Azure Storage Accounts.""],
         [""Azure AI Speech"", ""system"", ""An Azure service used for asynchronous transcription of audio files.""],
         [""Batch mode"", ""description"", ""A method in Azure AI Speech for transcribing multiple audio files at once.""],
         [""Speaker diarization"", ""description"", ""A feature in Azure AI Speech that identifies different speakers during the transcription process.""],
         [""Azure AI Language"", ""system"", ""An Azure service used for detecting and redacting personal data in transcripts.""],
         [""Ingestion Client tool"", ""component"", ""A no-code approach tool for automating call center transcription using Azure services.""],
         [""Azure OpenAI"", ""system"", ""An Azure service used for processing transcripts, extracting entities, summarizing conversations, and analyzing sentiments.""],
         [""Power BI"", ""component"", ""A business analytics service provided by Microsoft that visualizes data and provides near real-time insights.""],
         [""App Service"", ""component"", ""A web hosting service provided by Microsoft for hosting custom web applications.""],
         [""CRM"", ""description"", ""A system used for managing customer relationships, including call center transcriptions.""],
         [""Agents"", ""person"", ""The individuals who handle customer calls in a call center.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage Account""],
         [""Azure AI Speech"", ""calls"", ""Storage SDK or API""],
         [""Azure AI Language"", ""called-from"", ""Transcription results stored in Blob Storage""],
         [""Ingestion Client tool"", ""part-of"", ""Batch mode transcription and personal data detection and redaction""],
         [""Azure OpenAI"", ""calls"", ""Stored transcription information""],
         [""Power BI"", ""called-from"", ""Output stored in Blob Storage""],
         [""Custom web application (App Service)"", ""called-from"", ""Output stored in Blob Storage""],
         [""CRM"", ""called-from"", ""Output stored in Blob Storage""]
      ]
   }",6.213229109998792
23,mistral:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps.""],
         [""Logic Apps"", ""component"", ""Uses secured template parameters to retrieve database credentials from Azure Key Vault, uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
         [""Azure Key Vault"", ""component"", ""Stores the database credentials used by Logic Apps.""],
         [""Transport Layer Security (TLS)"", ""system"", ""Used by Logic Apps to securely send data.""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database to run the statement.""],
         [""SQL Server"", ""system"", ""Stores and makes data available to apps that users access.""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance.""]
      ],
      ""relationships"": [
         [""API Management"", ""calls"", ""Logic Apps""],
         [""Logic Apps"", ""part-of"", ""Transport Layer Security (TLS)""],
         [""Logic Apps"", ""part-of"", ""Azure Monitor""],
         [""Logic Apps"", ""part-of"", ""on-premises data gateway""],
         [""on-premises data gateway"", ""calls"", ""SQL Server""]
      ]
   }"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps.""],
         [""Logic Apps"", ""component"", ""Uses secured template parameters to retrieve database credentials from Azure Key Vault, uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
         [""Azure Key Vault"", ""component"", ""Stores the database credentials used by Logic Apps.""],
         [""Transport Layer Security (TLS)"", ""system"", ""Used by Logic Apps to securely send data.""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database to run the statement.""],
         [""SQL Server"", ""system"", ""Stores and makes data available to apps that users access.""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance.""]
      ],
      ""relationships"": [
         [""API Management"", ""calls"", ""Logic Apps""],
         [""Logic Apps"", ""part-of"", ""Transport Layer Security (TLS)""],
         [""Logic Apps"", ""part-of"", ""Azure Monitor""],
         [""Logic Apps"", ""part-of"", ""on-premises data gateway""],
         [""on-premises data gateway"", ""calls"", ""SQL Server""]
      ]
   }",3.928989159001503
24,mistral:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A platform for version control and collaboration""],
         [""CI/CD pipeline agent"", ""component"", ""Software that automates the continuous integration, testing, and deployment process""],
         [""Azure VM"", ""component"", ""Virtual machine on Microsoft Azure cloud infrastructure""],
         [""API application"", ""component"", ""Application that provides APIs to other components""],
         [""ILB ASE"", ""system"", ""Azure Internal Load Balancer (ILB) Application Service Environment""],
         [""Azure API Management"", ""component"", ""Microsoft Azure service for managing and publishing APIs""],
         [""HOST headers"", ""entity"", ""Additional information sent by a client to a web server to help identify the specific resource being requested""],
         [""API Management policy"", ""entity"", ""Configuration settings for API Management""],
         [""App Service Environment's DNS name"", ""entity"", ""Domain Name System (DNS) name of App Service Environment""],
         [""Application Gateway"", ""component"", ""Microsoft Azure service that acts as a reverse proxy and load balancer""],
         [""Azure Private DNS"", ""system"", ""Microsoft Azure service for private DNS resolution within virtual networks""],
         [""API Management's developer and API portal"", ""entity"", ""Portal for developers to access and manage APIs provided by API Management""],
         [""Application Gateway's public IP"", ""entity"", ""Publicly accessible IP address of Application Gateway""],
         [""External users"", ""person"", ""Users outside the organization using the API""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent"", ""Developers check in code to a GitHub repository""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM"", ""The agent is installed on an Azure VM""],
         [""API application"", ""part-of"", ""ILB ASE"", ""The API application is hosted on ILB ASE""],
         [""API application"", ""calls"", ""Azure API Management"", ""The agent pushes the build to the API application that's hosted on ILB ASE""],
         [""Azure API Management"", ""part-of"", ""API Management policy"", ""Azure API Management consumes the preceding APIs via HOST headers specified in API Management policy""],
         [""Azure API Management"", ""calls"", ""App Service Environment's DNS name"", ""API Management uses the App Service Environment's DNS name for all the APIs""],
         [""Application Gateway"", ""part-of"", ""API Management's developer and API portal"", ""Application Gateway exposes API Management's developer and API portal""],
         [""Azure Private DNS"", ""called-from"", ""Traffic"", ""Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway""],
         [""API Management's developer and API portal"", ""calls"", ""Application Gateway's public IP"", ""External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP""]
      ]
   }"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A platform for version control and collaboration""],
         [""CI/CD pipeline agent"", ""component"", ""Software that automates the continuous integration, testing, and deployment process""],
         [""Azure VM"", ""component"", ""Virtual machine on Microsoft Azure cloud infrastructure""],
         [""API application"", ""component"", ""Application that provides APIs to other components""],
         [""ILB ASE"", ""system"", ""Azure Internal Load Balancer (ILB) Application Service Environment""],
         [""Azure API Management"", ""component"", ""Microsoft Azure service for managing and publishing APIs""],
         [""HOST headers"", ""entity"", ""Additional information sent by a client to a web server to help identify the specific resource being requested""],
         [""API Management policy"", ""entity"", ""Configuration settings for API Management""],
         [""App Service Environment's DNS name"", ""entity"", ""Domain Name System (DNS) name of App Service Environment""],
         [""Application Gateway"", ""component"", ""Microsoft Azure service that acts as a reverse proxy and load balancer""],
         [""Azure Private DNS"", ""system"", ""Microsoft Azure service for private DNS resolution within virtual networks""],
         [""API Management's developer and API portal"", ""entity"", ""Portal for developers to access and manage APIs provided by API Management""],
         [""Application Gateway's public IP"", ""entity"", ""Publicly accessible IP address of Application Gateway""],
         [""External users"", ""person"", ""Users outside the organization using the API""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent"", ""Developers check in code to a GitHub repository""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM"", ""The agent is installed on an Azure VM""],
         [""API application"", ""part-of"", ""ILB ASE"", ""The API application is hosted on ILB ASE""],
         [""API application"", ""calls"", ""Azure API Management"", ""The agent pushes the build to the API application that's hosted on ILB ASE""],
         [""Azure API Management"", ""part-of"", ""API Management policy"", ""Azure API Management consumes the preceding APIs via HOST headers specified in API Management policy""],
         [""Azure API Management"", ""calls"", ""App Service Environment's DNS name"", ""API Management uses the App Service Environment's DNS name for all the APIs""],
         [""Application Gateway"", ""part-of"", ""API Management's developer and API portal"", ""Application Gateway exposes API Management's developer and API portal""],
         [""Azure Private DNS"", ""called-from"", ""Traffic"", ""Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway""],
         [""API Management's developer and API portal"", ""calls"", ""Application Gateway's public IP"", ""External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP""]
      ]
   }",7.38635596295353
25,mistral:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}"," {
      ""entities"": [
         [""Airline System"", ""system"", ""The entity that communicates with a custom API hosted in Azure API Management.""],
         [""Custom API Hosted in Azure API Management"", ""component"", ""The entity that is being communicated with by the Airline System.""],
         [""Custom API Coordinator"", ""component"", ""The entity that receives notifications and handles incoming messages from the Airline System, and sends them to Power Apps.""],
         [""Power Apps"", ""system"", ""The system where flights are assigned to Microsoft Teams channels.""],
         [""Microsoft Teams Channels"", ""system"", ""The system where flights are being assigned by Power Apps.""],
         [""Azure Storage Account Queue"", ""component"", ""The entity where Graph API calls are queued for further processing.""],
         [""Azure Functions"", ""component"", ""The entity that runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams and streaming all events to an Azure Event Hubs.""],
         [""Azure Event Hubs"", ""system"", ""The system for further analytics.""],
         [""Airline's Notification System"", ""system"", ""The entity managed by a custom bot messaging service that employs Azure Bot Service.""],
         [""Custom Bot Messaging Service"", ""component"", ""The entity that sends flight updates to users in Teams.""],
         [""Azure Bot Service"", ""component"", ""The service employed by the Custom Bot Messaging Service.""],
         [""Azure Data Lake Storage"", ""system"", ""The system that offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.""]
      ],
      ""relationships"": [
         [""Airline System"", ""communicates-with"", ""Custom API Hosted in Azure API Management""],
         [""Custom API Coordinator"", ""receives-from"", ""Airline System""],
         [""Custom API Coordinator"", ""sends-to"", ""Power Apps""],
         [""Power Apps"", ""assigns-to"", ""Microsoft Teams Channels""],
         [""System"", ""queues"", ""Graph API Call"", ""Azure Storage Account Queue""],
         [""Azure Functions"", ""runs"", ""Graph API Call""],
         [""Azure Functions"", ""sends-notifications-to"", ""Teams""],
         [""Azure Functions"", ""streams-to"", ""Azure Event Hubs""],
         [""Custom Bot Messaging Service"", ""employs"", ""Azure Bot Service""],
         [""Custom Bots"", ""send-updates-to"", ""Users in Teams""],
         [""Airline's Notification System"", ""manages"", ""Custom Bot Messaging Service""],
         [""Azure Data Lake Storage"", ""offers-long-term-retention-and-processing-for"", ""Events from Event Hubs""],
         [""Azure Data Lake Storage"", ""generates-reports-with"", ""Power BI""]
      ]
   }"," {
      ""entities"": [
         [""Airline System"", ""system"", ""The entity that communicates with a custom API hosted in Azure API Management.""],
         [""Custom API Hosted in Azure API Management"", ""component"", ""The entity that is being communicated with by the Airline System.""],
         [""Custom API Coordinator"", ""component"", ""The entity that receives notifications and handles incoming messages from the Airline System, and sends them to Power Apps.""],
         [""Power Apps"", ""system"", ""The system where flights are assigned to Microsoft Teams channels.""],
         [""Microsoft Teams Channels"", ""system"", ""The system where flights are being assigned by Power Apps.""],
         [""Azure Storage Account Queue"", ""component"", ""The entity where Graph API calls are queued for further processing.""],
         [""Azure Functions"", ""component"", ""The entity that runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams and streaming all events to an Azure Event Hubs.""],
         [""Azure Event Hubs"", ""system"", ""The system for further analytics.""],
         [""Airline's Notification System"", ""system"", ""The entity managed by a custom bot messaging service that employs Azure Bot Service.""],
         [""Custom Bot Messaging Service"", ""component"", ""The entity that sends flight updates to users in Teams.""],
         [""Azure Bot Service"", ""component"", ""The service employed by the Custom Bot Messaging Service.""],
         [""Azure Data Lake Storage"", ""system"", ""The system that offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.""]
      ],
      ""relationships"": [
         [""Airline System"", ""communicates-with"", ""Custom API Hosted in Azure API Management""],
         [""Custom API Coordinator"", ""receives-from"", ""Airline System""],
         [""Custom API Coordinator"", ""sends-to"", ""Power Apps""],
         [""Power Apps"", ""assigns-to"", ""Microsoft Teams Channels""],
         [""System"", ""queues"", ""Graph API Call"", ""Azure Storage Account Queue""],
         [""Azure Functions"", ""runs"", ""Graph API Call""],
         [""Azure Functions"", ""sends-notifications-to"", ""Teams""],
         [""Azure Functions"", ""streams-to"", ""Azure Event Hubs""],
         [""Custom Bot Messaging Service"", ""employs"", ""Azure Bot Service""],
         [""Custom Bots"", ""send-updates-to"", ""Users in Teams""],
         [""Airline's Notification System"", ""manages"", ""Custom Bot Messaging Service""],
         [""Azure Data Lake Storage"", ""offers-long-term-retention-and-processing-for"", ""Events from Event Hubs""],
         [""Azure Data Lake Storage"", ""generates-reports-with"", ""Power BI""]
      ]
   }",7.22436595504405
26,mistral:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}"," {
      ""entities"": [
        [""Outlook email account"", ""system"", ""A designated Outlook email account""],
        [""Logic app"", ""component"", ""The logic app that processes emails and triggers other actions.""],
        [""Azure Logic Apps"", ""system"", ""A system for building workflows and automating tasks.""],
        [""Azure Data Lake Storage"", ""system"", ""A system for storing large amounts of data in a scalable, secure manner.""],
        [""PDF container"", ""component"", ""A component within Azure Data Lake Storage used for storing PDF files.""],
        [""function app"", ""component"", ""The function app that processes single-page PDF files and sends them to Azure AI Document Intelligence for processing.""],
        [""Azure Functions"", ""system"", ""A system for running serverless functions in response to events.""],
        [""HTTPS POST"", ""action"", ""An action used to send a request using the HTTP protocol, secured with TLS/SSL.""],
        [""Azure REST API"", ""action"", ""An action that uses the REST architecture for accessing APIs.""],
        [""AI Document Intelligence"", ""system"", ""A system provided by Azure that processes documents and extracts meaningful information.""],
        [""data structure"", ""component"", ""A component used to store data in a specific format.""],
        [""third container in Data Lake Storage"", ""component"", ""A component within Azure Data Lake Storage used for storing processed PDF files.""],
        [""Azure Cosmos DB"", ""system"", ""A system that provides global distributed, multi-model database services.""],
        [""database"", ""component"", ""A component used for storing and organizing data in a structured manner.""],
        [""collections"", ""component"", ""A component within the Azure Cosmos DB database used for organizing and grouping documents or items.""],
        [""Power BI"", ""system"", ""A system that provides business analytics, reporting, and interactive visualizations.""]
      ],
      ""relationships"": [
        [""Outlook email account"", ""part-of"", ""system"", ""The Outlook email account is part of a larger system.""],
        [""Logic app"", ""calls"", ""Azure Logic Apps"", ""The logic app uses the capabilities provided by Azure Logic Apps.""],
        [""PDF container"", ""part-of"", ""Azure Data Lake Storage"", ""The PDF container is a component within Azure Data Lake Storage.""],
        [""function app"", ""calls"", ""HTTPS POST"", ""The function app sends an HTTPS POST request.""],
        [""function app"", ""calls"", ""Azure REST API"", ""The function app uses the Azure REST API.""],
        [""AI Document Intelligence"", ""called-from"", ""function app"", ""The AI Document Intelligence system is called from the function app.""],
        [""third container in Data Lake Storage"", ""part-of"", ""Azure Data Lake Storage"", ""The third container in Data Lake Storage is a component within Azure Data Lake Storage.""],
        [""data structure"", ""part-of"", ""function app"", ""The data structure is used by the function app.""],
        [""Azure Cosmos DB"", ""called-from"", ""forms processing logic app"", ""The forms processing logic app sends data to Azure Cosmos DB.""],
        [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains data from Azure Cosmos DB.""]
      ]
    }"," {
      ""entities"": [
        [""Outlook email account"", ""system"", ""A designated Outlook email account""],
        [""Logic app"", ""component"", ""The logic app that processes emails and triggers other actions.""],
        [""Azure Logic Apps"", ""system"", ""A system for building workflows and automating tasks.""],
        [""Azure Data Lake Storage"", ""system"", ""A system for storing large amounts of data in a scalable, secure manner.""],
        [""PDF container"", ""component"", ""A component within Azure Data Lake Storage used for storing PDF files.""],
        [""function app"", ""component"", ""The function app that processes single-page PDF files and sends them to Azure AI Document Intelligence for processing.""],
        [""Azure Functions"", ""system"", ""A system for running serverless functions in response to events.""],
        [""HTTPS POST"", ""action"", ""An action used to send a request using the HTTP protocol, secured with TLS/SSL.""],
        [""Azure REST API"", ""action"", ""An action that uses the REST architecture for accessing APIs.""],
        [""AI Document Intelligence"", ""system"", ""A system provided by Azure that processes documents and extracts meaningful information.""],
        [""data structure"", ""component"", ""A component used to store data in a specific format.""],
        [""third container in Data Lake Storage"", ""component"", ""A component within Azure Data Lake Storage used for storing processed PDF files.""],
        [""Azure Cosmos DB"", ""system"", ""A system that provides global distributed, multi-model database services.""],
        [""database"", ""component"", ""A component used for storing and organizing data in a structured manner.""],
        [""collections"", ""component"", ""A component within the Azure Cosmos DB database used for organizing and grouping documents or items.""],
        [""Power BI"", ""system"", ""A system that provides business analytics, reporting, and interactive visualizations.""]
      ],
      ""relationships"": [
        [""Outlook email account"", ""part-of"", ""system"", ""The Outlook email account is part of a larger system.""],
        [""Logic app"", ""calls"", ""Azure Logic Apps"", ""The logic app uses the capabilities provided by Azure Logic Apps.""],
        [""PDF container"", ""part-of"", ""Azure Data Lake Storage"", ""The PDF container is a component within Azure Data Lake Storage.""],
        [""function app"", ""calls"", ""HTTPS POST"", ""The function app sends an HTTPS POST request.""],
        [""function app"", ""calls"", ""Azure REST API"", ""The function app uses the Azure REST API.""],
        [""AI Document Intelligence"", ""called-from"", ""function app"", ""The AI Document Intelligence system is called from the function app.""],
        [""third container in Data Lake Storage"", ""part-of"", ""Azure Data Lake Storage"", ""The third container in Data Lake Storage is a component within Azure Data Lake Storage.""],
        [""data structure"", ""part-of"", ""function app"", ""The data structure is used by the function app.""],
        [""Azure Cosmos DB"", ""called-from"", ""forms processing logic app"", ""The forms processing logic app sends data to Azure Cosmos DB.""],
        [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains data from Azure Cosmos DB.""]
      ]
    }",8.254596750019118
27,mistral:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""A reverse proxy service provided by Azure that helps in routing requests to Azure AD B2C or other services.""],
         [""Azure AD B2C"", ""component"", ""An identity provider service offered by Microsoft Azure for consumer-facing applications and services.""],
         [""Custom domain"", ""component"", ""A specific domain configured for sign-in in Azure AD B2C.""],
         [""JSON Web Token (JWT)"", ""component"", ""A compact, self-contained data structure used to pass information between parties as a claim.""],
         [""API Management"", ""system"", ""An Azure service that manages APIs, including authentication and authorization.""],
         [""App Service Environment"", ""component"", ""A dedicated environment for hosting apps in Azure.""],
         [""Function Apps"", ""component"", ""A serverless compute solution provided by Azure to run small pieces of code.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""An managed Kubernetes platform offered by Azure.""],
         [""Backend API"", ""component"", ""API providing services for the application.""],
         [""Network connectivity"", ""component"", ""The ability of different systems to communicate with each other over a network.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management""],
         [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
         [""Azure AD B2C"", ""called-from"", ""API Management""],
         [""API Management"", ""contains"", ""App Service Environment""],
         [""API Management"", ""contains"", ""Function Apps""],
         [""API Management"", ""contains"", ""Azure Kubernetes Services (AKS)""],
         [""API Management"", ""calls"", ""Backend API""],
         [""Backend API"", ""part-of"", ""App Service Environment""],
         [""Backend API"", ""part-of"", ""Function Apps""],
         [""Backend API"", ""part-of"", ""Azure Kubernetes Services (AKS)""],
         [""APIs running on-premises and hybrid cloud environments"", ""called-from"", ""API Management""]
      ]
   }"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""A reverse proxy service provided by Azure that helps in routing requests to Azure AD B2C or other services.""],
         [""Azure AD B2C"", ""component"", ""An identity provider service offered by Microsoft Azure for consumer-facing applications and services.""],
         [""Custom domain"", ""component"", ""A specific domain configured for sign-in in Azure AD B2C.""],
         [""JSON Web Token (JWT)"", ""component"", ""A compact, self-contained data structure used to pass information between parties as a claim.""],
         [""API Management"", ""system"", ""An Azure service that manages APIs, including authentication and authorization.""],
         [""App Service Environment"", ""component"", ""A dedicated environment for hosting apps in Azure.""],
         [""Function Apps"", ""component"", ""A serverless compute solution provided by Azure to run small pieces of code.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""An managed Kubernetes platform offered by Azure.""],
         [""Backend API"", ""component"", ""API providing services for the application.""],
         [""Network connectivity"", ""component"", ""The ability of different systems to communicate with each other over a network.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management""],
         [""Azure Front Door"", ""calls"", ""Azure AD B2C""],
         [""Azure AD B2C"", ""called-from"", ""API Management""],
         [""API Management"", ""contains"", ""App Service Environment""],
         [""API Management"", ""contains"", ""Function Apps""],
         [""API Management"", ""contains"", ""Azure Kubernetes Services (AKS)""],
         [""API Management"", ""calls"", ""Backend API""],
         [""Backend API"", ""part-of"", ""App Service Environment""],
         [""Backend API"", ""part-of"", ""Function Apps""],
         [""Backend API"", ""part-of"", ""Azure Kubernetes Services (AKS)""],
         [""APIs running on-premises and hybrid cloud environments"", ""called-from"", ""API Management""]
      ]
   }",5.441906139021739
28,mistral:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}"," {
      ""entities"": [
         [""Connect Agent"", ""component"", ""Captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""Publisher (Mainframe)"", ""component"", ""A publisher component on the mainframe manages data migration.""],
         [""Listener (Midrange)"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.""],
         [""Enhanced-Security Connection"", ""system"", ""The publisher or listener moves the data from on-premises to Azure via this connection.""],
         [""Connect Replicator Engine"", ""component"", ""The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target.""],
         [""Target (Database)"", ""system"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""component"", ""The changed data is consumed by this component and applied to Azure data platform services.""]
      ],
      ""relationships"": [
         [""Connect Agent"", ""part-of"", ""Mainframe (or Midrange) System""],
         [""Publisher (Mainframe)"", ""part-of"", ""Enhanced-Security Connection""],
         [""Listener (Midrange)"", ""part-of"", ""Enhanced-Security Connection""],
         [""Enhanced-Security Connection"", ""contains"", ""Data being migrated from on-premises to Azure""],
         [""Publisher (Mainframe)"", ""calls"", ""Connect Replicator Engine""],
         [""Listener (Midrange)"", ""calls"", ""Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""part-of"", ""Target (Database)""],
         [""Target (Database)"", ""consumes"", ""Data from Connect Replicator Engine""],
         [""Azure Databricks"", ""consumes"", ""Data from Target (Database)""]
      ]
   }"," {
      ""entities"": [
         [""Connect Agent"", ""component"", ""Captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""Publisher (Mainframe)"", ""component"", ""A publisher component on the mainframe manages data migration.""],
         [""Listener (Midrange)"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.""],
         [""Enhanced-Security Connection"", ""system"", ""The publisher or listener moves the data from on-premises to Azure via this connection.""],
         [""Connect Replicator Engine"", ""component"", ""The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target.""],
         [""Target (Database)"", ""system"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""component"", ""The changed data is consumed by this component and applied to Azure data platform services.""]
      ],
      ""relationships"": [
         [""Connect Agent"", ""part-of"", ""Mainframe (or Midrange) System""],
         [""Publisher (Mainframe)"", ""part-of"", ""Enhanced-Security Connection""],
         [""Listener (Midrange)"", ""part-of"", ""Enhanced-Security Connection""],
         [""Enhanced-Security Connection"", ""contains"", ""Data being migrated from on-premises to Azure""],
         [""Publisher (Mainframe)"", ""calls"", ""Connect Replicator Engine""],
         [""Listener (Midrange)"", ""calls"", ""Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""part-of"", ""Target (Database)""],
         [""Target (Database)"", ""consumes"", ""Data from Connect Replicator Engine""],
         [""Azure Databricks"", ""consumes"", ""Data from Target (Database)""]
      ]
   }",5.2159996760310605
29,mistral:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A backend system that is triggered by HTTP requests and can nest workflows for more complex orchestration. It uses connectors to integrate with commonly used services.""],
         [""API Management"", ""system"", ""A backend system consisting of two components: API gateway and a developer portal.""],
         [""API gateway"", ""component"", ""The component that accepts HTTP calls and routes them to the backend.""],
         [""Developer portal"", ""component"", ""The component that gives developers access to documentation, code samples for calling APIs, and allows testing of APIs.""],
         [""Azure DNS"", ""system"", ""A system provided by Azure that provides name resolution using the Azure infrastructure.""],
         [""Azure"", ""person"", ""A company that provides various services including cloud computing services.""],
         [""Microsoft Entra ID"", ""system"", ""A system used for authenticating clients that call the API gateway and supports the OpenID Connect (OIDC) protocol.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""calls"", ""API Management""],
         [""API Management"", ""part-of"", ""Azure""],
         [""API Management"", ""contains"", ""API gateway""],
         [""API Management"", ""contains"", ""Developer portal""],
         [""Azure DNS"", ""part-of"", ""Azure""],
         [""Microsoft Entra ID"", ""calls"", ""API Gateway""]
      ]
   }"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A backend system that is triggered by HTTP requests and can nest workflows for more complex orchestration. It uses connectors to integrate with commonly used services.""],
         [""API Management"", ""system"", ""A backend system consisting of two components: API gateway and a developer portal.""],
         [""API gateway"", ""component"", ""The component that accepts HTTP calls and routes them to the backend.""],
         [""Developer portal"", ""component"", ""The component that gives developers access to documentation, code samples for calling APIs, and allows testing of APIs.""],
         [""Azure DNS"", ""system"", ""A system provided by Azure that provides name resolution using the Azure infrastructure.""],
         [""Azure"", ""person"", ""A company that provides various services including cloud computing services.""],
         [""Microsoft Entra ID"", ""system"", ""A system used for authenticating clients that call the API gateway and supports the OpenID Connect (OIDC) protocol.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""calls"", ""API Management""],
         [""API Management"", ""part-of"", ""Azure""],
         [""API Management"", ""contains"", ""API gateway""],
         [""API Management"", ""contains"", ""Developer portal""],
         [""Azure DNS"", ""part-of"", ""Azure""],
         [""Microsoft Entra ID"", ""calls"", ""API Gateway""]
      ]
   }",3.845520706032403
30,mistral:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}"," {
      ""entities"": [
         [""Web App"", ""System"", ""The web app that uploads and processes documents.""],
         [""Azure Blob Storage"", ""System"", ""The storage service where document files are stored.""],
         [""Storage Queue"", ""Component"", ""A queue for command messages to initiate pipeline processing.""],
         [""Durable Functions Orchestration"", ""System"", ""An Azure Functions orchestrator that manages the processing of each document file.""],
         [""Scan activity function"", ""Component"", ""Function responsible for OCR using Computer Vision API.""],
         [""Computer Vision Read API"", ""Component"", ""API used by Scan activity function for optical character recognition (OCR).""],
         [""Classify activity function"", ""Component"", ""Function that calls the document classifier service hosted in AKS.""],
         [""Azure Kubernetes Service (AKS)"", ""System"", ""A managed container orchestration service from Microsoft Azure.""],
         [""Document Classifier Service"", ""Component"", ""Service hosted in AKS for identifying document types and page ranges.""],
         [""Azure Cosmos DB Store"", ""System"", ""Database used for storing document type and page range information.""],
         [""Cognitive Search service"", ""System"", ""Azure's search service used for indexing searchable documents.""],
         [""Correlation ID"", ""String"", ""An identifier added to each search document that matches with corresponding document metadata from Azure Cosmos DB.""],
         [""Azure AI Search libraries for .NET"", ""Component"", ""Libraries used by Indexing activity function for creating and indexing search documents.""]
      ],
      ""relationships"": [
         [""Web App"", ""calls"", ""Storage Queue""],
         [""Storage Queue"", ""part-of"", ""Durable Functions Orchestration""],
         [""Durable Functions Orchestration"", ""contains"", ""Scan activity function""],
         [""Durable Functions Orchestration"", ""contains"", ""Classify activity function""],
         [""Classify activity function"", ""calls"", ""Document Classifier Service""],
         [""Durable Functions Orchestration"", ""contains"", ""Metadata Store activity function""],
         [""Metadata Store activity function"", ""part-of"", ""Azure Cosmos DB Store""],
         [""Indexing activity function"", ""part-of"", ""Cognitive Search service""],
         [""Indexing activity function"", ""uses"", ""Azure AI Search libraries for .NET""]
      ]
   }"," {
      ""entities"": [
         [""Web App"", ""System"", ""The web app that uploads and processes documents.""],
         [""Azure Blob Storage"", ""System"", ""The storage service where document files are stored.""],
         [""Storage Queue"", ""Component"", ""A queue for command messages to initiate pipeline processing.""],
         [""Durable Functions Orchestration"", ""System"", ""An Azure Functions orchestrator that manages the processing of each document file.""],
         [""Scan activity function"", ""Component"", ""Function responsible for OCR using Computer Vision API.""],
         [""Computer Vision Read API"", ""Component"", ""API used by Scan activity function for optical character recognition (OCR).""],
         [""Classify activity function"", ""Component"", ""Function that calls the document classifier service hosted in AKS.""],
         [""Azure Kubernetes Service (AKS)"", ""System"", ""A managed container orchestration service from Microsoft Azure.""],
         [""Document Classifier Service"", ""Component"", ""Service hosted in AKS for identifying document types and page ranges.""],
         [""Azure Cosmos DB Store"", ""System"", ""Database used for storing document type and page range information.""],
         [""Cognitive Search service"", ""System"", ""Azure's search service used for indexing searchable documents.""],
         [""Correlation ID"", ""String"", ""An identifier added to each search document that matches with corresponding document metadata from Azure Cosmos DB.""],
         [""Azure AI Search libraries for .NET"", ""Component"", ""Libraries used by Indexing activity function for creating and indexing search documents.""]
      ],
      ""relationships"": [
         [""Web App"", ""calls"", ""Storage Queue""],
         [""Storage Queue"", ""part-of"", ""Durable Functions Orchestration""],
         [""Durable Functions Orchestration"", ""contains"", ""Scan activity function""],
         [""Durable Functions Orchestration"", ""contains"", ""Classify activity function""],
         [""Classify activity function"", ""calls"", ""Document Classifier Service""],
         [""Durable Functions Orchestration"", ""contains"", ""Metadata Store activity function""],
         [""Metadata Store activity function"", ""part-of"", ""Azure Cosmos DB Store""],
         [""Indexing activity function"", ""part-of"", ""Cognitive Search service""],
         [""Indexing activity function"", ""uses"", ""Azure AI Search libraries for .NET""]
      ]
   }",6.092595205001999
31,mistral:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps.""],
         [""Logic Apps"", ""component"", ""Uses secured template parameters to retrieve database credentials from Azure Key Vault and uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
         [""Azure Key Vault"", ""component"", ""Used by Logic Apps to store database credentials securely.""],
         [""Transport Layer Security (TLS)"", ""component"", ""Used by Logic Apps for secure communication with the on-premises data gateway.""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database to run the provided statement.""],
         [""SQL Server"", ""system"", ""Stores data and makes it available to apps that users access.""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance.""]
      ],
      ""relationships"": [
         [""API Management"", ""part-of"", ""System""],
         [""Logic Apps"", ""part-of"", ""API Management""],
         [""Azure Key Vault"", ""called-from"", ""Logic Apps""],
         [""Transport Layer Security (TLS)"", ""calls"", ""Logic Apps""],
         [""on-premises data gateway"", ""part-of"", ""SQL Server""],
         [""Logic Apps"", ""calls"", ""on-premises data gateway""]
      ]
   }"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps.""],
         [""Logic Apps"", ""component"", ""Uses secured template parameters to retrieve database credentials from Azure Key Vault and uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
         [""Azure Key Vault"", ""component"", ""Used by Logic Apps to store database credentials securely.""],
         [""Transport Layer Security (TLS)"", ""component"", ""Used by Logic Apps for secure communication with the on-premises data gateway.""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database to run the provided statement.""],
         [""SQL Server"", ""system"", ""Stores data and makes it available to apps that users access.""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance.""]
      ],
      ""relationships"": [
         [""API Management"", ""part-of"", ""System""],
         [""Logic Apps"", ""part-of"", ""API Management""],
         [""Azure Key Vault"", ""called-from"", ""Logic Apps""],
         [""Transport Layer Security (TLS)"", ""calls"", ""Logic Apps""],
         [""on-premises data gateway"", ""part-of"", ""SQL Server""],
         [""Logic Apps"", ""calls"", ""on-premises data gateway""]
      ]
   }",4.019683822989464
32,mistral:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Individuals who upload datasets into a public blob storage account""],
         [""Public Blob Storage Account"", ""system"", ""Storage account where data owners upload their datasets""],
         [""Microsoft-managed keys"", ""component"", ""Encryption keys used to secure the data in the Public Blob Storage Account""],
         [""Azure Data Factory"", ""component"", ""Tool that starts copying of the uploaded dataset""],
         [""Trigger"", ""component"", ""Mechanism that starts the copying process of the dataset""],
         [""Import Path"", ""system"", ""Location on another storage account where the copied dataset is stored""],
         [""Private Endpoint"", ""component"", ""Method used to access the storage account""],
         [""Service Principal"", ""component"", ""Identity for limited permissions in the system""],
         [""Azure Virtual Desktop"", ""component"", ""Platform used as a privileged jump box for researchers to access the secure environment""],
         [""Secure Environment"", ""system"", ""Environment where the dataset is presented and data preparation is done""],
         [""Data Science VMs"", ""component"", ""Virtual machines provisioned in a secure network environment for research work""],
         [""Secure Network Environment"", ""system"", ""Network used to isolate the data science VMs""],
         [""Azure Machine Learning Compute"", ""component"", ""Tool for training, deploying, automating, and managing machine learning models""],
         [""Private Endpoint (for Azure Machine Learning)"", ""component"", ""Method used by Azure Machine Learning compute to access the dataset""],
         [""Regulatory Guidelines"", ""system"", ""Standards that the models must meet""],
         [""De-identified Data"", ""component"", ""Data from which personal information is removed""],
         [""Export Path"", ""system"", ""Location on the secure storage account where de-identified data is saved""],
         [""Logic App"", ""component"", ""Tool that starts the manual approval process for exported data""],
         [""Notification"", ""component"", ""Message sent by the Logic App""],
         [""Manual Approval Process"", ""system"", ""Review process to ensure sensitive data isn't exported""],
         [""Data Factory Instance"", ""component"", ""Tool that moves approved de-identified data to a public storage account""],
         [""Public Storage Account"", ""system"", ""Storage account where the approved de-identified data is stored for external researchers to access""],
         [""Lower Security Environment (Optional)"", ""system"", ""Alternate storage location for data exported in a lower security environment""]
      ],
      ""relationships"": [
         [""Data Owners"", ""part-of"", ""Public Blob Storage Account""],
         [""Public Blob Storage Account"", ""contains"", ""Microsoft-managed keys""],
         [""Azure Data Factory"", ""part-of"", ""Trigger""],
         [""Trigger"", ""calls"", ""Azure Data Factory""],
         [""Import Path"", ""part-of"", ""Another storage account""],
         [""Private Endpoint"", ""part-of"", ""Storage account""],
         [""Service Principal"", ""part-of"", ""Access controls""],
         [""Azure Virtual Desktop"", ""part-of"", ""Secure Environment""],
         [""Data Science VMs"", ""part-of"", ""Secure Network Environment""],
         [""Azure Machine Learning Compute"", ""part-of"", ""Private Endpoint (for Azure Machine Learning)""],
         [""Regulatory Guidelines"", ""applies-to"", ""Models""],
         [""De-identified Data"", ""contains"", ""Personal Information""],
         [""Export Path"", ""contains"", ""De-identified Data""],
         [""Logic App"", ""calls"", ""Manual Approval Process""],
         [""Data Factory Instance"", ""part-of"", ""Public Storage Account""],
         [""Public Storage Account"", ""contains"", ""External Researchers""]
      ]
   }"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Individuals who upload datasets into a public blob storage account""],
         [""Public Blob Storage Account"", ""system"", ""Storage account where data owners upload their datasets""],
         [""Microsoft-managed keys"", ""component"", ""Encryption keys used to secure the data in the Public Blob Storage Account""],
         [""Azure Data Factory"", ""component"", ""Tool that starts copying of the uploaded dataset""],
         [""Trigger"", ""component"", ""Mechanism that starts the copying process of the dataset""],
         [""Import Path"", ""system"", ""Location on another storage account where the copied dataset is stored""],
         [""Private Endpoint"", ""component"", ""Method used to access the storage account""],
         [""Service Principal"", ""component"", ""Identity for limited permissions in the system""],
         [""Azure Virtual Desktop"", ""component"", ""Platform used as a privileged jump box for researchers to access the secure environment""],
         [""Secure Environment"", ""system"", ""Environment where the dataset is presented and data preparation is done""],
         [""Data Science VMs"", ""component"", ""Virtual machines provisioned in a secure network environment for research work""],
         [""Secure Network Environment"", ""system"", ""Network used to isolate the data science VMs""],
         [""Azure Machine Learning Compute"", ""component"", ""Tool for training, deploying, automating, and managing machine learning models""],
         [""Private Endpoint (for Azure Machine Learning)"", ""component"", ""Method used by Azure Machine Learning compute to access the dataset""],
         [""Regulatory Guidelines"", ""system"", ""Standards that the models must meet""],
         [""De-identified Data"", ""component"", ""Data from which personal information is removed""],
         [""Export Path"", ""system"", ""Location on the secure storage account where de-identified data is saved""],
         [""Logic App"", ""component"", ""Tool that starts the manual approval process for exported data""],
         [""Notification"", ""component"", ""Message sent by the Logic App""],
         [""Manual Approval Process"", ""system"", ""Review process to ensure sensitive data isn't exported""],
         [""Data Factory Instance"", ""component"", ""Tool that moves approved de-identified data to a public storage account""],
         [""Public Storage Account"", ""system"", ""Storage account where the approved de-identified data is stored for external researchers to access""],
         [""Lower Security Environment (Optional)"", ""system"", ""Alternate storage location for data exported in a lower security environment""]
      ],
      ""relationships"": [
         [""Data Owners"", ""part-of"", ""Public Blob Storage Account""],
         [""Public Blob Storage Account"", ""contains"", ""Microsoft-managed keys""],
         [""Azure Data Factory"", ""part-of"", ""Trigger""],
         [""Trigger"", ""calls"", ""Azure Data Factory""],
         [""Import Path"", ""part-of"", ""Another storage account""],
         [""Private Endpoint"", ""part-of"", ""Storage account""],
         [""Service Principal"", ""part-of"", ""Access controls""],
         [""Azure Virtual Desktop"", ""part-of"", ""Secure Environment""],
         [""Data Science VMs"", ""part-of"", ""Secure Network Environment""],
         [""Azure Machine Learning Compute"", ""part-of"", ""Private Endpoint (for Azure Machine Learning)""],
         [""Regulatory Guidelines"", ""applies-to"", ""Models""],
         [""De-identified Data"", ""contains"", ""Personal Information""],
         [""Export Path"", ""contains"", ""De-identified Data""],
         [""Logic App"", ""calls"", ""Manual Approval Process""],
         [""Data Factory Instance"", ""part-of"", ""Public Storage Account""],
         [""Public Storage Account"", ""contains"", ""External Researchers""]
      ]
   }",9.29548008198617
33,mistral:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}"," {
      ""entities"": [
         [""APIM"", ""system"", ""The main application that receives POST requests from outside services and clients.""],
         [""CreatePatient function in the Patient API"", ""component"", ""A function within the Patient API that is responsible for creating a new patient record.""],
         [""Audit API CreateAuditRecord function"", ""component"", ""A function within the Audit API application that creates an audit record.""],
         [""Azure Cosmos DB"", ""system"", ""The database system used to store the patient and audit records.""],
         [""outside services and clients"", ""person"", ""The external entities making POST requests to APIM.""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient function in the Patient API""],
         [""CreatePatient function in the Patient API"", ""calls"", ""Audit API CreateAuditRecord function""],
         [""Audit API CreateAuditRecord function"", ""creates"", ""audit record in Azure Cosmos DB""],
         [""APIM"", ""responds-to"", ""outside services and clients""],
         [""CreatePatient function in the Patient API"", ""responds-to"", ""APIM""]
      ]
   }"," {
      ""entities"": [
         [""APIM"", ""system"", ""The main application that receives POST requests from outside services and clients.""],
         [""CreatePatient function in the Patient API"", ""component"", ""A function within the Patient API that is responsible for creating a new patient record.""],
         [""Audit API CreateAuditRecord function"", ""component"", ""A function within the Audit API application that creates an audit record.""],
         [""Azure Cosmos DB"", ""system"", ""The database system used to store the patient and audit records.""],
         [""outside services and clients"", ""person"", ""The external entities making POST requests to APIM.""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient function in the Patient API""],
         [""CreatePatient function in the Patient API"", ""calls"", ""Audit API CreateAuditRecord function""],
         [""Audit API CreateAuditRecord function"", ""creates"", ""audit record in Azure Cosmos DB""],
         [""APIM"", ""responds-to"", ""outside services and clients""],
         [""CreatePatient function in the Patient API"", ""responds-to"", ""APIM""]
      ]
   }",3.1849814879824407
34,mistral:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Azure where audio files from phone calls between an agent and a customer are uploaded and stored.""],
         [""Azure Storage account"", ""component"", ""A specific instance of the Azure Blob Storage system.""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool for interacting with the Azure Storage service.""],
         [""Azure AI Speech"", ""system"", ""An Azure service that transcribes audio files asynchronously with speaker diarization enabled.""],
         [""Batch mode transcription"", ""activity"", ""The process of transcribing audio files in a batch using Azure AI Speech.""],
         [""Azure AI Language"", ""system"", ""An Azure service used to detect and redact personal data in the transcript.""],
         [""Ingestion Client tool"", ""component"", ""A no-code approach for call center transcription.""],
         [""AI services"", ""system"", ""The set of Azure services used in this workflow, including Azure AI Speech, Azure AI Language, and Azure OpenAI.""],
         [""Azure OpenAI"", ""system"", ""An Azure service that processes the transcript, extracts entities, summarizes the conversation, analyzes sentiments, and more.""],
         [""Blob Storage"", ""component"", ""A specific instance of the Azure Blob Storage system used for storing the processed output from Azure OpenAI.""],
         [""Power BI"", ""system"", ""A business analytics service that provides visualizations of data.""],
         [""App Service"", ""system"", ""A set of products by Microsoft Azure that allows hosting web applications or APIs.""],
         [""custom web application"", ""component"", ""A specific instance of a web application hosted on App Service, used for visualizing the output.""],
         [""Power BI or custom web application"", ""OR"", ""Either Power BI or a custom web application can be used to visualize the output.""],
         [""near real-time insights"", ""result"", ""Quick and up-to-date information derived from the processed transcript.""],
         [""customer relationship management (CRM)"", ""system"", ""A software system for managing a company's interaction with customers, employees, and sales activities.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage account""],
         [""Azure Storage account"", ""part-of"", ""Azure Blob Storage""],
         [""Azure Storage Explorer"", ""calls"", ""Azure Storage account""],
         [""Azure AI Speech"", ""contains"", ""Batch mode transcription""],
         [""Batch mode transcription"", ""part-of"", ""Azure AI Speech""],
         [""Azure AI Language"", ""called-from"", ""Transcript""],
         [""Ingestion Client tool"", ""uses"", ""AI services""],
         [""AI services"", ""contains"", ""Azure AI Speech""],
         [""AI services"", ""contains"", ""Azure AI Language""],
         [""AI services"", ""contains"", ""Azure OpenAI""],
         [""Blob Storage"", ""called-from"", ""Azure OpenAI""],
         [""Power BI or custom web application"", ""visualizes"", ""Output""],
         [""Output"", ""stored"", ""Blob Storage""],
         [""CRM"", ""stores"", ""Output""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Azure where audio files from phone calls between an agent and a customer are uploaded and stored.""],
         [""Azure Storage account"", ""component"", ""A specific instance of the Azure Blob Storage system.""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool for interacting with the Azure Storage service.""],
         [""Azure AI Speech"", ""system"", ""An Azure service that transcribes audio files asynchronously with speaker diarization enabled.""],
         [""Batch mode transcription"", ""activity"", ""The process of transcribing audio files in a batch using Azure AI Speech.""],
         [""Azure AI Language"", ""system"", ""An Azure service used to detect and redact personal data in the transcript.""],
         [""Ingestion Client tool"", ""component"", ""A no-code approach for call center transcription.""],
         [""AI services"", ""system"", ""The set of Azure services used in this workflow, including Azure AI Speech, Azure AI Language, and Azure OpenAI.""],
         [""Azure OpenAI"", ""system"", ""An Azure service that processes the transcript, extracts entities, summarizes the conversation, analyzes sentiments, and more.""],
         [""Blob Storage"", ""component"", ""A specific instance of the Azure Blob Storage system used for storing the processed output from Azure OpenAI.""],
         [""Power BI"", ""system"", ""A business analytics service that provides visualizations of data.""],
         [""App Service"", ""system"", ""A set of products by Microsoft Azure that allows hosting web applications or APIs.""],
         [""custom web application"", ""component"", ""A specific instance of a web application hosted on App Service, used for visualizing the output.""],
         [""Power BI or custom web application"", ""OR"", ""Either Power BI or a custom web application can be used to visualize the output.""],
         [""near real-time insights"", ""result"", ""Quick and up-to-date information derived from the processed transcript.""],
         [""customer relationship management (CRM)"", ""system"", ""A software system for managing a company's interaction with customers, employees, and sales activities.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage account""],
         [""Azure Storage account"", ""part-of"", ""Azure Blob Storage""],
         [""Azure Storage Explorer"", ""calls"", ""Azure Storage account""],
         [""Azure AI Speech"", ""contains"", ""Batch mode transcription""],
         [""Batch mode transcription"", ""part-of"", ""Azure AI Speech""],
         [""Azure AI Language"", ""called-from"", ""Transcript""],
         [""Ingestion Client tool"", ""uses"", ""AI services""],
         [""AI services"", ""contains"", ""Azure AI Speech""],
         [""AI services"", ""contains"", ""Azure AI Language""],
         [""AI services"", ""contains"", ""Azure OpenAI""],
         [""Blob Storage"", ""called-from"", ""Azure OpenAI""],
         [""Power BI or custom web application"", ""visualizes"", ""Output""],
         [""Output"", ""stored"", ""Blob Storage""],
         [""CRM"", ""stores"", ""Output""]
      ]
   }",7.905699533002917
35,mistral:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}"," {
      ""entities"": [
         [""CDS"", ""system"", ""Cloud Data Storage""],
         [""SQL"", ""system"", ""Structured Query Language Database""],
         [""Data Factory"", ""component"", ""Data Integration Service""],
         [""ERP system"", ""system"", ""Enterprise Resource Planning System""],
         [""Azure SQL database"", ""component"", ""Azure-based Structured Query Language Database""],
         [""Azure Functions"", ""component"", ""Azure Serverless Function""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Microsoft Power Platform Application""],
         [""PO data"", ""data"", ""Purchase Order Data""],
         [""user"", ""person"", ""User performing tasks""],
         [""CSV export"", ""component"", ""Comma Separated Values Export Function""],
         [""Power BI"", ""system"", ""Business Intelligence System""]
      ],
      ""relationships"": [
         [""CDS"", ""contains"", ""Data Factory""],
         [""Data Factory"", ""part-of"", ""SQL""],
         [""ERP system"", ""calls"", ""Azure SQL database""],
         [""Azure Functions"", ""uses"", ""API""],
         [""Azure Functions"", ""creates"", [""task"", ""for users to review""]],
         [""Power Apps"", ""retrieves data from"", ""Azure SQL Database""],
         [""user"", ""updates"", ""POs in Power Apps""],
         [""Power Apps"", ""sends data to"", [""CDS"", ""through CSV export""]],
         [""Power BI"", ""reports trends in"", ""supplier status""]
      ]
   }"," {
      ""entities"": [
         [""CDS"", ""system"", ""Cloud Data Storage""],
         [""SQL"", ""system"", ""Structured Query Language Database""],
         [""Data Factory"", ""component"", ""Data Integration Service""],
         [""ERP system"", ""system"", ""Enterprise Resource Planning System""],
         [""Azure SQL database"", ""component"", ""Azure-based Structured Query Language Database""],
         [""Azure Functions"", ""component"", ""Azure Serverless Function""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Microsoft Power Platform Application""],
         [""PO data"", ""data"", ""Purchase Order Data""],
         [""user"", ""person"", ""User performing tasks""],
         [""CSV export"", ""component"", ""Comma Separated Values Export Function""],
         [""Power BI"", ""system"", ""Business Intelligence System""]
      ],
      ""relationships"": [
         [""CDS"", ""contains"", ""Data Factory""],
         [""Data Factory"", ""part-of"", ""SQL""],
         [""ERP system"", ""calls"", ""Azure SQL database""],
         [""Azure Functions"", ""uses"", ""API""],
         [""Azure Functions"", ""creates"", [""task"", ""for users to review""]],
         [""Power Apps"", ""retrieves data from"", ""Azure SQL Database""],
         [""user"", ""updates"", ""POs in Power Apps""],
         [""Power Apps"", ""sends data to"", [""CDS"", ""through CSV export""]],
         [""Power BI"", ""reports trends in"", ""supplier status""]
      ]
   }",4.245711472991388
36,mistral:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}"," {
      ""entities"": [
         [""Outlook email account"", ""person"", ""A designated Outlook email account""],
         [""Logic app"", ""component"", ""A logic app built using Azure Logic Apps that processes emails and PDF files""],
         [""Azure Data Lake Storage"", ""system"", ""A storage system used to store PDF files""],
         [""PDF container in Azure Data Lake Storage"", ""component"", ""A container in the Azure Data Lake Storage system where PDF files are stored""],
         [""function app"", ""component"", ""A function app built using Azure Functions that processes single-page PDF files""],
         [""Azure REST API"", ""component"", ""An Azure REST API used by the function app to send location of single-page PDF files for processing""],
         [""AI Document Intelligence"", ""component"", ""An Azure service used for processing single-page PDF files""],
         [""Azure Cosmos DB"", ""system"", ""A database system used to store and manage data""],
         [""Power BI"", ""person"", ""A business intelligence tool that provides insights and dashboards from the data in Azure Cosmos DB""]
      ],
      ""relationships"": [
         [""Outlook email account"", ""calls"", ""Logic app"", ""The arrival of an email in the Outlook account triggers the logic app""],
         [""Logic app"", ""contains"", ""Azure Data Lake Storage"", ""The logic app uploads PDF files to a container in Azure Data Lake Storage""],
         [""PDF container in Azure Data Lake Storage"", ""part-of"", ""Azure Data Lake Storage"", ""The PDF container is part of the larger Data Lake Storage system""],
         [""Logic app"", ""calls"", ""function app"", ""The arrival of a PDF file in the PDF container triggers another logic app that sends the location of the file to the function app""],
         [""function app"", ""part-of"", ""Azure Functions"", ""The function app is built using Azure Functions capabilities""],
         [""function app"", ""contains"", ""Azure REST API"", ""The function app uses the Azure REST API for sending the location of single-page PDF files for processing""],
         [""AI Document Intelligence"", ""part-of"", ""Azure"", ""AI Document Intelligence is an Azure service""],
         [""function app"", ""calls"", ""AI Document Intelligence"", ""The function app sends the location of the single-page PDF file to AI Document Intelligence for processing""],
         [""AI Document Intelligence"", ""called-from"", ""function app"", ""AI Document Intelligence receives a request from the function app and processes it""],
         [""function app"", ""contains"", ""second container in Data Lake Storage"", ""The split files are saved to a second container in Data Lake Storage""],
         [""function app"", ""calls"", ""third container in Data Lake Storage"", ""The JSON data file containing the response data is stored to a third container in Data Lake Storage""],
         [""forms processing logic app"", ""part-of"", ""Logic apps"", ""The forms processing logic app is another logic app built using Azure Logic Apps""],
         [""function app"", ""calls"", ""forms processing logic app"", ""The function app sends the processed data to the forms processing logic app""],
         [""forms processing logic app"", ""part-of"", ""Logic apps"", ""The forms processing logic app is another logic app built using Azure Logic Apps""],
         [""function app"", ""calls"", ""Azure Cosmos DB"", ""The function app sends the processed data to Azure Cosmos DB for saving""],
         [""Azure Cosmos DB"", ""contains"", ""data"", ""Azure Cosmos DB stores and manages the data in a database""],
         [""Power BI"", ""obtains"", ""data from Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB for providing insights and dashboards""]
      ]
   }"," {
      ""entities"": [
         [""Outlook email account"", ""person"", ""A designated Outlook email account""],
         [""Logic app"", ""component"", ""A logic app built using Azure Logic Apps that processes emails and PDF files""],
         [""Azure Data Lake Storage"", ""system"", ""A storage system used to store PDF files""],
         [""PDF container in Azure Data Lake Storage"", ""component"", ""A container in the Azure Data Lake Storage system where PDF files are stored""],
         [""function app"", ""component"", ""A function app built using Azure Functions that processes single-page PDF files""],
         [""Azure REST API"", ""component"", ""An Azure REST API used by the function app to send location of single-page PDF files for processing""],
         [""AI Document Intelligence"", ""component"", ""An Azure service used for processing single-page PDF files""],
         [""Azure Cosmos DB"", ""system"", ""A database system used to store and manage data""],
         [""Power BI"", ""person"", ""A business intelligence tool that provides insights and dashboards from the data in Azure Cosmos DB""]
      ],
      ""relationships"": [
         [""Outlook email account"", ""calls"", ""Logic app"", ""The arrival of an email in the Outlook account triggers the logic app""],
         [""Logic app"", ""contains"", ""Azure Data Lake Storage"", ""The logic app uploads PDF files to a container in Azure Data Lake Storage""],
         [""PDF container in Azure Data Lake Storage"", ""part-of"", ""Azure Data Lake Storage"", ""The PDF container is part of the larger Data Lake Storage system""],
         [""Logic app"", ""calls"", ""function app"", ""The arrival of a PDF file in the PDF container triggers another logic app that sends the location of the file to the function app""],
         [""function app"", ""part-of"", ""Azure Functions"", ""The function app is built using Azure Functions capabilities""],
         [""function app"", ""contains"", ""Azure REST API"", ""The function app uses the Azure REST API for sending the location of single-page PDF files for processing""],
         [""AI Document Intelligence"", ""part-of"", ""Azure"", ""AI Document Intelligence is an Azure service""],
         [""function app"", ""calls"", ""AI Document Intelligence"", ""The function app sends the location of the single-page PDF file to AI Document Intelligence for processing""],
         [""AI Document Intelligence"", ""called-from"", ""function app"", ""AI Document Intelligence receives a request from the function app and processes it""],
         [""function app"", ""contains"", ""second container in Data Lake Storage"", ""The split files are saved to a second container in Data Lake Storage""],
         [""function app"", ""calls"", ""third container in Data Lake Storage"", ""The JSON data file containing the response data is stored to a third container in Data Lake Storage""],
         [""forms processing logic app"", ""part-of"", ""Logic apps"", ""The forms processing logic app is another logic app built using Azure Logic Apps""],
         [""function app"", ""calls"", ""forms processing logic app"", ""The function app sends the processed data to the forms processing logic app""],
         [""forms processing logic app"", ""part-of"", ""Logic apps"", ""The forms processing logic app is another logic app built using Azure Logic Apps""],
         [""function app"", ""calls"", ""Azure Cosmos DB"", ""The function app sends the processed data to Azure Cosmos DB for saving""],
         [""Azure Cosmos DB"", ""contains"", ""data"", ""Azure Cosmos DB stores and manages the data in a database""],
         [""Power BI"", ""obtains"", ""data from Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB for providing insights and dashboards""]
      ]
   }",9.04972616903251
37,mistral:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests""],
         [""Logic Apps"", ""component"", ""Securely routes the HTTP requests to Logic Apps, uses secured template parameters to retrieve database credentials from Azure Key Vault, and uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway""],
         [""Azure Key Vault"", ""component"", ""Used by Logic Apps to retrieve database credentials""],
         [""Transport Layer Security (TLS)"", ""system"", ""Used by Logic Apps to securely send data""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database to run the statement""],
         [""SQL Server"", ""component"", ""Stores the data and makes it available to apps that users access""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance""]
      ],
      ""relationships"": [
         [""API Management"", ""calls"", ""Logic Apps""],
         [""Logic Apps"", ""part-of"", ""Securely routes the HTTP requests to Logic Apps""],
         [""Logic Apps"", ""contains"", ""uses secured template parameters""],
         [""Logic Apps"", ""contains"", ""Transport Layer Security (TLS)""],
         [""Logic Apps"", ""calls"", ""on-premises data gateway""],
         [""on-premises data gateway"", ""part-of"", ""SQL Server database""]
      ]
   }"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests""],
         [""Logic Apps"", ""component"", ""Securely routes the HTTP requests to Logic Apps, uses secured template parameters to retrieve database credentials from Azure Key Vault, and uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway""],
         [""Azure Key Vault"", ""component"", ""Used by Logic Apps to retrieve database credentials""],
         [""Transport Layer Security (TLS)"", ""system"", ""Used by Logic Apps to securely send data""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database to run the statement""],
         [""SQL Server"", ""component"", ""Stores the data and makes it available to apps that users access""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance""]
      ],
      ""relationships"": [
         [""API Management"", ""calls"", ""Logic Apps""],
         [""Logic Apps"", ""part-of"", ""Securely routes the HTTP requests to Logic Apps""],
         [""Logic Apps"", ""contains"", ""uses secured template parameters""],
         [""Logic Apps"", ""contains"", ""Transport Layer Security (TLS)""],
         [""Logic Apps"", ""calls"", ""on-premises data gateway""],
         [""on-premises data gateway"", ""part-of"", ""SQL Server database""]
      ]
   }",4.13854989700485
38,mistral:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Azure where the uploaded document files are stored.""],
         [""Web App"", ""person"", ""The web application that allows users to upload their documents.""],
         [""Document File"", ""component"", ""The file containing multiple documents of various types, such as a PDF or multipage TIFF file.""],
         [""Durable Functions Orchestration"", ""system"", ""A serverless Azure function that handles the coordination and management of multiple functions.""],
         [""Command Message"", ""component"", ""A message sent to a storage queue to initiate pipeline processing of a document file.""],
         [""Storage Queue"", ""system"", ""A messaging system used to send and receive messages in an Azure cloud service.""],
         [""Scan Activity Function"", ""component"", ""A function that calls the Computer Vision Read API to perform OCR on the uploaded document.""],
         [""Computer Vision Read API"", ""system"", ""An API provided by Microsoft's Computer Vision service for performing OCR.""],
         [""Optical Character Recognition (OCR)"", ""person"", ""A technology used to convert scanned images of text into machine-editable text.""],
         [""Classify Activity Function"", ""component"", ""A function that calls the document classifier service hosted in an Azure Kubernetes Service (AKS) cluster.""],
         [""Document Classifier Service"", ""system"", ""A service that identifies the starting page of each known document and calculates the number of document types in a document file.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""A managed container orchestration service provided by Azure for deploying, scaling, and managing containerized applications.""],
         [""Regular Expression Pattern Matching"", ""person"", ""A method used to search through a sequence of characters (such as text) according to a defined pattern.""],
         [""Azure Cosmos DB Store"", ""system"", ""An Azure database service for storing structured, semi-structured, and unstructured data.""],
         [""Metadata Store Activity Function"", ""component"", ""A function that saves the document type and page range information in an Azure Cosmos DB store.""],
         [""Cognitive Search Service"", ""system"", ""An Azure service for building, deploying, and using intelligent search solutions.""],
         [""Azure AI Search libraries for .NET"", ""person"", ""Libraries provided by Microsoft for integrating the Cognitive Search service with .NET applications.""],
         [""Correlation ID"", ""component"", ""A unique identifier added to the search document for matching search results with the corresponding document metadata.""],
         [""Cognitive Search Indexing Activity Function"", ""component"", ""A function that creates a new search document in the Cognitive Search service for each identified document type.""]
      ],
      ""relationships"": [
         [""Web App"", ""calls"", ""Command Message""],
         [""Command Message"", ""part-of"", ""Storage Queue""],
         [""Durable Functions Orchestration"", ""part-of"", ""Command Message""],
         [""Durable Functions Orchestration"", ""contains"", ""Scan Activity Function""],
         [""Durable Functions Orchestration"", ""contains"", ""Classify Activity Function""],
         [""Classify Activity Function"", ""calls"", ""Document Classifier Service""],
         [""Metadata Store Activity Function"", ""part-of"", ""Durable Functions Orchestration""],
         [""Cognitive Search Indexing Activity Function"", ""part-of"", ""Durable Functions Orchestration""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Azure where the uploaded document files are stored.""],
         [""Web App"", ""person"", ""The web application that allows users to upload their documents.""],
         [""Document File"", ""component"", ""The file containing multiple documents of various types, such as a PDF or multipage TIFF file.""],
         [""Durable Functions Orchestration"", ""system"", ""A serverless Azure function that handles the coordination and management of multiple functions.""],
         [""Command Message"", ""component"", ""A message sent to a storage queue to initiate pipeline processing of a document file.""],
         [""Storage Queue"", ""system"", ""A messaging system used to send and receive messages in an Azure cloud service.""],
         [""Scan Activity Function"", ""component"", ""A function that calls the Computer Vision Read API to perform OCR on the uploaded document.""],
         [""Computer Vision Read API"", ""system"", ""An API provided by Microsoft's Computer Vision service for performing OCR.""],
         [""Optical Character Recognition (OCR)"", ""person"", ""A technology used to convert scanned images of text into machine-editable text.""],
         [""Classify Activity Function"", ""component"", ""A function that calls the document classifier service hosted in an Azure Kubernetes Service (AKS) cluster.""],
         [""Document Classifier Service"", ""system"", ""A service that identifies the starting page of each known document and calculates the number of document types in a document file.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""A managed container orchestration service provided by Azure for deploying, scaling, and managing containerized applications.""],
         [""Regular Expression Pattern Matching"", ""person"", ""A method used to search through a sequence of characters (such as text) according to a defined pattern.""],
         [""Azure Cosmos DB Store"", ""system"", ""An Azure database service for storing structured, semi-structured, and unstructured data.""],
         [""Metadata Store Activity Function"", ""component"", ""A function that saves the document type and page range information in an Azure Cosmos DB store.""],
         [""Cognitive Search Service"", ""system"", ""An Azure service for building, deploying, and using intelligent search solutions.""],
         [""Azure AI Search libraries for .NET"", ""person"", ""Libraries provided by Microsoft for integrating the Cognitive Search service with .NET applications.""],
         [""Correlation ID"", ""component"", ""A unique identifier added to the search document for matching search results with the corresponding document metadata.""],
         [""Cognitive Search Indexing Activity Function"", ""component"", ""A function that creates a new search document in the Cognitive Search service for each identified document type.""]
      ],
      ""relationships"": [
         [""Web App"", ""calls"", ""Command Message""],
         [""Command Message"", ""part-of"", ""Storage Queue""],
         [""Durable Functions Orchestration"", ""part-of"", ""Command Message""],
         [""Durable Functions Orchestration"", ""contains"", ""Scan Activity Function""],
         [""Durable Functions Orchestration"", ""contains"", ""Classify Activity Function""],
         [""Classify Activity Function"", ""calls"", ""Document Classifier Service""],
         [""Metadata Store Activity Function"", ""part-of"", ""Durable Functions Orchestration""],
         [""Cognitive Search Indexing Activity Function"", ""part-of"", ""Durable Functions Orchestration""]
      ]
   }",8.495672345045023
39,mistral:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A web-based hosting service for version control using Git""],
         [""CI/CD pipeline agent"", ""component"", ""An Azure VM instance that executes continuous integration and continuous delivery tasks""],
         [""Azure VM"", ""component"", ""Virtual Machine hosted on Microsoft Azure infrastructure""],
         [""API application"", ""component"", ""Application that provides a specific set of APIs""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Azure App Service Environment""],
         [""Azure API Management"", ""system"", ""A service offered by Microsoft Azure for managing APIs and their access policies""],
         [""HOST headers"", ""component"", ""HTTP request headers that specify the domain name of the server to which the request should be sent""],
         [""API Management policy"", ""component"", ""Rules applied to API requests by Azure API Management""],
         [""App Service Environment's DNS name"", ""system"", ""DNS name of the App Service Environment used for all APIs""],
         [""Application Gateway"", ""system"", ""A Microsoft Azure service that acts as a reverse proxy""],
         [""Azure Private DNS"", ""system"", ""Microsoft Azure's private Domain Name System (DNS) service""],
         [""ASE, API Management, and Application Gateway"", ""system"", ""Components of the described architecture in relation to each other""],
         [""External users"", ""person"", ""Users outside of the internal network consuming APIs via the developer portal""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent"", ""Developers check in code to a GitHub repository""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM"", ""The agent is installed on an Azure VM""],
         [""Azure VM"", ""contains"", ""API application"", ""The API application is hosted on the ILB ASE""],
         [""API application"", ""calls"", ""Azure API Management"", ""The API application sends requests to Azure API Management""],
         [""Azure API Management"", ""part-of"", ""API Management policy"", ""Azure API Management applies the specified policy to incoming API requests""],
         [""Azure API Management"", ""uses"", ""App Service Environment's DNS name"", ""For all APIs, it uses the App Service Environment's DNS name""],
         [""Application Gateway"", ""exposes"", ""API Management's developer and API portal"", ""The Application Gateway exposes API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", ""traffic internally"", ""It routes the traffic between ASE, API Management, and Application Gateway""],
         [""External users"", ""utilizes"", ""Application Gateway's public IP"", ""They utilize the exposed developer portal to consume APIs via Application Gateway's public IP""]
      ]
   }"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A web-based hosting service for version control using Git""],
         [""CI/CD pipeline agent"", ""component"", ""An Azure VM instance that executes continuous integration and continuous delivery tasks""],
         [""Azure VM"", ""component"", ""Virtual Machine hosted on Microsoft Azure infrastructure""],
         [""API application"", ""component"", ""Application that provides a specific set of APIs""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Azure App Service Environment""],
         [""Azure API Management"", ""system"", ""A service offered by Microsoft Azure for managing APIs and their access policies""],
         [""HOST headers"", ""component"", ""HTTP request headers that specify the domain name of the server to which the request should be sent""],
         [""API Management policy"", ""component"", ""Rules applied to API requests by Azure API Management""],
         [""App Service Environment's DNS name"", ""system"", ""DNS name of the App Service Environment used for all APIs""],
         [""Application Gateway"", ""system"", ""A Microsoft Azure service that acts as a reverse proxy""],
         [""Azure Private DNS"", ""system"", ""Microsoft Azure's private Domain Name System (DNS) service""],
         [""ASE, API Management, and Application Gateway"", ""system"", ""Components of the described architecture in relation to each other""],
         [""External users"", ""person"", ""Users outside of the internal network consuming APIs via the developer portal""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent"", ""Developers check in code to a GitHub repository""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM"", ""The agent is installed on an Azure VM""],
         [""Azure VM"", ""contains"", ""API application"", ""The API application is hosted on the ILB ASE""],
         [""API application"", ""calls"", ""Azure API Management"", ""The API application sends requests to Azure API Management""],
         [""Azure API Management"", ""part-of"", ""API Management policy"", ""Azure API Management applies the specified policy to incoming API requests""],
         [""Azure API Management"", ""uses"", ""App Service Environment's DNS name"", ""For all APIs, it uses the App Service Environment's DNS name""],
         [""Application Gateway"", ""exposes"", ""API Management's developer and API portal"", ""The Application Gateway exposes API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", ""traffic internally"", ""It routes the traffic between ASE, API Management, and Application Gateway""],
         [""External users"", ""utilizes"", ""Application Gateway's public IP"", ""They utilize the exposed developer portal to consume APIs via Application Gateway's public IP""]
      ]
   }",6.853112775017507
40,mistral:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}"," {
      ""entities"": [
         [""Connect Agent Component"", ""component"", ""Captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""Publisher (for Mainframe Systems)"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""],
         [""Listener (for Midrange Systems)"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.""],
         [""Enhanced-Security Connection"", ""system"", ""The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection.""],
         [""Connect Replicator Engine"", ""component"", ""The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.""],
         [""Target (Database)"", ""system"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""component"", ""The changed data is consumed by Azure Databricks and applied to Azure data platform services.""]
      ],
      ""relationships"": [
         [""Connect Agent Component"", ""part-of"", ""A Connect agent component""],
         [""Publisher (for Mainframe Systems)"", ""part-of"", ""A mainframe system""],
         [""Listener (for Midrange Systems)"", ""part-of"", ""A midrange system""],
         [""Publisher (for Mainframe Systems)"", ""calls"", ""The Enhanced-Security Connection""],
         [""Listener (for Midrange Systems)"", ""calls"", ""The Enhanced-Security Connection""],
         [""Connect Replicator Engine"", ""part-of"", ""The Connect Agent Component""],
         [""Target (Database)"", ""called-from"", ""The Connect Replicator Engine""],
         [""Azure Databricks"", ""called-from"", ""The Target (Database)""]
      ]
   }"," {
      ""entities"": [
         [""Connect Agent Component"", ""component"", ""Captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""Publisher (for Mainframe Systems)"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""],
         [""Listener (for Midrange Systems)"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.""],
         [""Enhanced-Security Connection"", ""system"", ""The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection.""],
         [""Connect Replicator Engine"", ""component"", ""The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.""],
         [""Target (Database)"", ""system"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""component"", ""The changed data is consumed by Azure Databricks and applied to Azure data platform services.""]
      ],
      ""relationships"": [
         [""Connect Agent Component"", ""part-of"", ""A Connect agent component""],
         [""Publisher (for Mainframe Systems)"", ""part-of"", ""A mainframe system""],
         [""Listener (for Midrange Systems)"", ""part-of"", ""A midrange system""],
         [""Publisher (for Mainframe Systems)"", ""calls"", ""The Enhanced-Security Connection""],
         [""Listener (for Midrange Systems)"", ""calls"", ""The Enhanced-Security Connection""],
         [""Connect Replicator Engine"", ""part-of"", ""The Connect Agent Component""],
         [""Target (Database)"", ""called-from"", ""The Connect Replicator Engine""],
         [""Azure Databricks"", ""called-from"", ""The Target (Database)""]
      ]
   }",5.189899309014436
41,mistral:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A system that is triggered by HTTP requests and offers connectors to integrate with commonly used services.""],
         [""API Management"", ""system"", ""A system consisting of two components: API gateway and developer portal.""],
         [""API Gateway"", ""component"", ""A component of API Management that accepts HTTP calls and routes them to the backend.""],
         [""Developer Portal"", ""component"", ""A component of API Management providing access to documentation, code samples for calling APIs, and testing APIs.""],
         [""Azure DNS"", ""system"", ""A system that provides name resolution using Azure infrastructure.""],
         [""Custom Domain Name"", ""entity"", ""A custom domain name such as contoso.com.""],
         [""Microsoft Entra ID"", ""system"", ""A system used for authenticating clients that call the API gateway, supporting OpenID Connect protocol.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""contains"", ""API Management""],
         [""API Management"", ""part-of"", ""Azure API Management""],
         [""API Gateway"", ""part-of"", ""API Management""],
         [""Developer Portal"", ""part-of"", ""API Management""],
         [""Azure DNS"", ""calls-from"", ""Custom Domain Name""],
         [""Microsoft Entra ID"", ""called-from"", ""API Gateway""]
      ]
   }"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A system that is triggered by HTTP requests and offers connectors to integrate with commonly used services.""],
         [""API Management"", ""system"", ""A system consisting of two components: API gateway and developer portal.""],
         [""API Gateway"", ""component"", ""A component of API Management that accepts HTTP calls and routes them to the backend.""],
         [""Developer Portal"", ""component"", ""A component of API Management providing access to documentation, code samples for calling APIs, and testing APIs.""],
         [""Azure DNS"", ""system"", ""A system that provides name resolution using Azure infrastructure.""],
         [""Custom Domain Name"", ""entity"", ""A custom domain name such as contoso.com.""],
         [""Microsoft Entra ID"", ""system"", ""A system used for authenticating clients that call the API gateway, supporting OpenID Connect protocol.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""contains"", ""API Management""],
         [""API Management"", ""part-of"", ""Azure API Management""],
         [""API Gateway"", ""part-of"", ""API Management""],
         [""Developer Portal"", ""part-of"", ""API Management""],
         [""Azure DNS"", ""calls-from"", ""Custom Domain Name""],
         [""Microsoft Entra ID"", ""called-from"", ""API Gateway""]
      ]
   }",3.883898804022465
42,mistral:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""A reverse proxy service that routes network traffic to backend services""],
         [""API Management"", ""component"", ""A product for managing, securing, and scaling APIs""],
         [""Azure AD B2C"", ""component"", ""An identity management service for consumer applications""],
         [""App Service Environment"", ""component"", ""A dedicated environment for running web apps and APIs""],
         [""Function Apps"", ""component"", ""A serverless compute service for executing code""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""A managed container orchestration service""],
         [""Backend API"", ""component"", ""An application programming interface that provides functionalities to other applications""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""calls"", ""API Management"", ""The request goes via Azure Front Door to API Management""],
         [""API Management"", ""part-of"", ""App Service Environment (if running in App Service Environment)""],
         [""API Management"", ""part-of"", ""Function Apps (if running in Function Apps)""],
         [""API Management"", ""part-of"", ""Azure Kubernetes Services (AKS) (if running in AKS)""],
         [""API Management"", ""calls"", ""Backend API"", ""If the token is valid, API Management forwards the request to the appropriate backend API""],
         [""Backend API"", ""called-from"", ""Client"", ""The client triggers an event that accesses a backend API""]
      ]
   }"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""A reverse proxy service that routes network traffic to backend services""],
         [""API Management"", ""component"", ""A product for managing, securing, and scaling APIs""],
         [""Azure AD B2C"", ""component"", ""An identity management service for consumer applications""],
         [""App Service Environment"", ""component"", ""A dedicated environment for running web apps and APIs""],
         [""Function Apps"", ""component"", ""A serverless compute service for executing code""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""A managed container orchestration service""],
         [""Backend API"", ""component"", ""An application programming interface that provides functionalities to other applications""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""calls"", ""API Management"", ""The request goes via Azure Front Door to API Management""],
         [""API Management"", ""part-of"", ""App Service Environment (if running in App Service Environment)""],
         [""API Management"", ""part-of"", ""Function Apps (if running in Function Apps)""],
         [""API Management"", ""part-of"", ""Azure Kubernetes Services (AKS) (if running in AKS)""],
         [""API Management"", ""calls"", ""Backend API"", ""If the token is valid, API Management forwards the request to the appropriate backend API""],
         [""Backend API"", ""called-from"", ""Client"", ""The client triggers an event that accesses a backend API""]
      ]
   }",3.890891710994765
43,mistral:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway for handling requests""],
         [""CreatePatient function in Patient API"", ""component"", ""Function responsible for creating a new patient""],
         [""Audit API"", ""system"", ""API for managing audit records""],
         [""CreateAuditRecord function in Audit API function app"", ""component"", ""Function responsible for creating an audit record""],
         [""Azure Cosmos DB"", ""system"", ""NoSQL database for data storage""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient function in Patient API"", ""Makes a POST request to the CreatePatient function with patient information""],
         [""CreatePatient function in Patient API"", ""calls"", ""CreateAuditRecord function in Audit API function app"", ""Calls the CreateAuditRecord function to create an audit record""],
         [""CreateAuditRecord function in Audit API function app"", ""part-of"", ""Azure Cosmos DB"", ""Creates the audit record in Azure Cosmos DB, effectively being stored within it""],
         [""CreatePatient function in Patient API"", ""part-of"", ""Azure Cosmos DB"", ""Creates the patient document in Azure Cosmos DB""]
      ]
   }"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway for handling requests""],
         [""CreatePatient function in Patient API"", ""component"", ""Function responsible for creating a new patient""],
         [""Audit API"", ""system"", ""API for managing audit records""],
         [""CreateAuditRecord function in Audit API function app"", ""component"", ""Function responsible for creating an audit record""],
         [""Azure Cosmos DB"", ""system"", ""NoSQL database for data storage""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient function in Patient API"", ""Makes a POST request to the CreatePatient function with patient information""],
         [""CreatePatient function in Patient API"", ""calls"", ""CreateAuditRecord function in Audit API function app"", ""Calls the CreateAuditRecord function to create an audit record""],
         [""CreateAuditRecord function in Audit API function app"", ""part-of"", ""Azure Cosmos DB"", ""Creates the audit record in Azure Cosmos DB, effectively being stored within it""],
         [""CreatePatient function in Patient API"", ""part-of"", ""Azure Cosmos DB"", ""Creates the patient document in Azure Cosmos DB""]
      ]
   }",3.3864876980078407
44,mistral:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""Storage service provided by Azure""],
         [""Azure Storage Account"", ""component"", ""Account used for storing data in Azure Blob Storage""],
         [""UI-based tool, Azure Storage Explorer"", ""component"", ""Graphical user interface tools for managing Azure Storage Account""],
         [""Storage SDK or API"", ""component"", ""Software development kits or application programming interfaces for interacting with Azure Storage Account""],
         [""Azure AI Speech"", ""component"", ""Speech recognition service provided by Azure for transcribing audio files""],
         [""Batch mode asynchronously with speaker diarization enabled"", ""functionality"", ""Method of processing multiple audio files simultaneously and identifying speakers within each file""],
         [""Azure AI Language"", ""component"", ""Natural language understanding service provided by Azure for detecting and redacting personal data""],
         [""Ingestion Client tool"", ""component"", ""No-code approach tool for call center transcription in batch mode""],
         [""Azure OpenAI"", ""component"", ""Service provided by Azure for processing, summarizing, analyzing sentiments of text and extracting entities""],
         [""Power BI"", ""component"", ""Business analytics service for visualizing data""],
         [""App Service"", ""component"", ""Platform as a Service offering from Azure for hosting web applications""],
         [""CRM"", ""component"", ""Customer relationship management system for storing customer interactions and metadata""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage Account""],
         [""Azure AI Speech"", ""calls"", ""Ingestion Client tool""],
         [""Ingestion Client tool"", ""part-of"", ""Batch mode transcription and personal data detection and redaction process""],
         [""Azure AI Language"", ""called-from"", ""Ingestion Client tool""],
         [""Azure OpenAI"", ""calls"", ""Processed transcript in Blob Storage""],
         [""Power BI"", ""visualizes"", ""Output from Azure OpenAI or custom web application hosted by App Service""],
         [""CRM"", ""stores"", ""Output from Power BI or custom web application hosted by App Service""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""Storage service provided by Azure""],
         [""Azure Storage Account"", ""component"", ""Account used for storing data in Azure Blob Storage""],
         [""UI-based tool, Azure Storage Explorer"", ""component"", ""Graphical user interface tools for managing Azure Storage Account""],
         [""Storage SDK or API"", ""component"", ""Software development kits or application programming interfaces for interacting with Azure Storage Account""],
         [""Azure AI Speech"", ""component"", ""Speech recognition service provided by Azure for transcribing audio files""],
         [""Batch mode asynchronously with speaker diarization enabled"", ""functionality"", ""Method of processing multiple audio files simultaneously and identifying speakers within each file""],
         [""Azure AI Language"", ""component"", ""Natural language understanding service provided by Azure for detecting and redacting personal data""],
         [""Ingestion Client tool"", ""component"", ""No-code approach tool for call center transcription in batch mode""],
         [""Azure OpenAI"", ""component"", ""Service provided by Azure for processing, summarizing, analyzing sentiments of text and extracting entities""],
         [""Power BI"", ""component"", ""Business analytics service for visualizing data""],
         [""App Service"", ""component"", ""Platform as a Service offering from Azure for hosting web applications""],
         [""CRM"", ""component"", ""Customer relationship management system for storing customer interactions and metadata""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage Account""],
         [""Azure AI Speech"", ""calls"", ""Ingestion Client tool""],
         [""Ingestion Client tool"", ""part-of"", ""Batch mode transcription and personal data detection and redaction process""],
         [""Azure AI Language"", ""called-from"", ""Ingestion Client tool""],
         [""Azure OpenAI"", ""calls"", ""Processed transcript in Blob Storage""],
         [""Power BI"", ""visualizes"", ""Output from Azure OpenAI or custom web application hosted by App Service""],
         [""CRM"", ""stores"", ""Output from Power BI or custom web application hosted by App Service""]
      ]
   }",5.493197885050904
45,mistral:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Individuals or organizations that upload datasets""],
         [""Datasets"", ""component"", ""Files containing data uploaded by Data Owners""],
         [""Public Blob Storage Account"", ""system"", ""Azure storage account used for public data storage""],
         [""Microsoft-managed keys"", ""component"", ""Encryption keys provided and managed by Microsoft""],
         [""Azure Data Factory"", ""component"", ""Data integration service provided by Azure""],
         [""Trigger"", ""component"", ""Event that starts copying of datasets in Azure Data Factory""],
         [""Private Endpoint"", ""component"", ""Network interface to access the storage account securely""],
         [""Service Principal"", ""component"", ""An application identity representing a set of access rights in Azure Active Directory""],
         [""Import Path"", ""component"", ""Location on another storage account where datasets are copied""],
         [""Azure Virtual Desktop"", ""component"", ""Virtual desktop and application delivery service""],
         [""Privileged Jump Box"", ""component"", ""A device that provides secure access to a network, especially for system administrators""],
         [""Streaming Application"", ""component"", ""Application used by researchers to access the secure environment""],
         [""Secure Network Environment"", ""system"", ""Isolated network where data science VMs are provisioned for research work""],
         [""Data Science VMs"", ""component"", ""Virtual machines used for data preparation and analysis""],
         [""Azure Machine Learning Compute"", ""component"", ""Azure service for machine learning model training, deployment, automation, and management""],
         [""Export Path"", ""component"", ""Location on the secure storage account where de-identified data is saved""],
         [""Logic App"", ""component"", ""Microsoft Logic App Service used to trigger notifications""],
         [""Manual Reviewers"", ""person"", ""Individuals who ensure that sensitive data isn't exported""],
         [""Data Factory Instance"", ""component"", ""A specific instance of Azure Data Factory""],
         [""Public Storage Account"", ""system"", ""Azure storage account used for making datasets accessible to external researchers""]
      ],
      ""relationships"": [
         [""Datasets"", ""part-of"", ""Public Blob Storage Account"", ""Datasets are stored in the public blob storage account""],
         [""Datasets"", ""calls"", ""Azure Data Factory"", ""Datasets initiate a connection with Azure Data Factory for copying""],
         [""Azure Data Factory"", ""part-of"", ""Secure Network Environment"", ""Azure Data Factory is a component of the secure network environment""],
         [""Private Endpoint"", ""part-of"", ""Storage Account"", ""Private endpoint provides secure access to the storage account""],
         [""Service Principal"", ""part-of"", ""Access Control"", ""Service principal represents access rights in Azure Active Directory""],
         [""Import Path"", ""part-of"", ""Another Storage Account"", ""Import path is a location on another storage account""],
         [""Azure Virtual Desktop"", ""part-of"", ""Streaming Application"", ""Azure Virtual Desktop is a component of the streaming application""],
         [""Data Science VMs"", ""part-of"", ""Secure Network Environment"", ""Data Science VMs are provisioned in the secure network environment""],
         [""Azure Machine Learning Compute"", ""part-of"", ""Secure Network Environment"", ""Azure Machine Learning Compute is a component of the secure network environment""],
         [""Export Path"", ""part-of"", ""Secure Storage Account"", ""Export path is a location on the secure storage account""],
         [""Logic App"", ""calls"", ""Manual Reviewers"", ""Logic app initiates a manual review process with Manual Reviewers""],
         [""Data Factory Instance"", ""part-of"", ""Public Storage Account"", ""Data Factory instance is a component of the public storage account""]
      ]
   }"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Individuals or organizations that upload datasets""],
         [""Datasets"", ""component"", ""Files containing data uploaded by Data Owners""],
         [""Public Blob Storage Account"", ""system"", ""Azure storage account used for public data storage""],
         [""Microsoft-managed keys"", ""component"", ""Encryption keys provided and managed by Microsoft""],
         [""Azure Data Factory"", ""component"", ""Data integration service provided by Azure""],
         [""Trigger"", ""component"", ""Event that starts copying of datasets in Azure Data Factory""],
         [""Private Endpoint"", ""component"", ""Network interface to access the storage account securely""],
         [""Service Principal"", ""component"", ""An application identity representing a set of access rights in Azure Active Directory""],
         [""Import Path"", ""component"", ""Location on another storage account where datasets are copied""],
         [""Azure Virtual Desktop"", ""component"", ""Virtual desktop and application delivery service""],
         [""Privileged Jump Box"", ""component"", ""A device that provides secure access to a network, especially for system administrators""],
         [""Streaming Application"", ""component"", ""Application used by researchers to access the secure environment""],
         [""Secure Network Environment"", ""system"", ""Isolated network where data science VMs are provisioned for research work""],
         [""Data Science VMs"", ""component"", ""Virtual machines used for data preparation and analysis""],
         [""Azure Machine Learning Compute"", ""component"", ""Azure service for machine learning model training, deployment, automation, and management""],
         [""Export Path"", ""component"", ""Location on the secure storage account where de-identified data is saved""],
         [""Logic App"", ""component"", ""Microsoft Logic App Service used to trigger notifications""],
         [""Manual Reviewers"", ""person"", ""Individuals who ensure that sensitive data isn't exported""],
         [""Data Factory Instance"", ""component"", ""A specific instance of Azure Data Factory""],
         [""Public Storage Account"", ""system"", ""Azure storage account used for making datasets accessible to external researchers""]
      ],
      ""relationships"": [
         [""Datasets"", ""part-of"", ""Public Blob Storage Account"", ""Datasets are stored in the public blob storage account""],
         [""Datasets"", ""calls"", ""Azure Data Factory"", ""Datasets initiate a connection with Azure Data Factory for copying""],
         [""Azure Data Factory"", ""part-of"", ""Secure Network Environment"", ""Azure Data Factory is a component of the secure network environment""],
         [""Private Endpoint"", ""part-of"", ""Storage Account"", ""Private endpoint provides secure access to the storage account""],
         [""Service Principal"", ""part-of"", ""Access Control"", ""Service principal represents access rights in Azure Active Directory""],
         [""Import Path"", ""part-of"", ""Another Storage Account"", ""Import path is a location on another storage account""],
         [""Azure Virtual Desktop"", ""part-of"", ""Streaming Application"", ""Azure Virtual Desktop is a component of the streaming application""],
         [""Data Science VMs"", ""part-of"", ""Secure Network Environment"", ""Data Science VMs are provisioned in the secure network environment""],
         [""Azure Machine Learning Compute"", ""part-of"", ""Secure Network Environment"", ""Azure Machine Learning Compute is a component of the secure network environment""],
         [""Export Path"", ""part-of"", ""Secure Storage Account"", ""Export path is a location on the secure storage account""],
         [""Logic App"", ""calls"", ""Manual Reviewers"", ""Logic app initiates a manual review process with Manual Reviewers""],
         [""Data Factory Instance"", ""part-of"", ""Public Storage Account"", ""Data Factory instance is a component of the public storage account""]
      ]
   }",9.172718874993734
46,mistral:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}"," {
      ""entities"": [
         [""Airline System"", ""System"", ""A system that communicates with a custom API hosted in Azure API Management.""],
         [""Custom API Hosted in Azure API Management"", ""Component"", ""A component that is used by the Airline System for communication.""],
         [""Custom API Coordinator"", ""Component"", ""A component that receives notifications and handles incoming messages from the Airline System, sending them to Power Apps.""],
         [""Power Apps"", ""System"", ""A system where flights are assigned to Microsoft Teams channels.""],
         [""Microsoft Teams Channels"", ""System"", ""A system in which flights are assigned.""],
         [""Azure Storage Account queue"", ""Component"", ""A component that queues a Graph API call for further processing.""],
         [""Azure Functions"", ""Component"", ""A component that runs the Graph API calls based on incoming messages in the storage queue, sending notifications to Teams and streaming all events to an Azure Event Hubs.""],
         [""Azure Event Hubs"", ""System"", ""A system for further analytics of the streamed events.""],
         [""Custom bot messaging service that employs Azure Bot Service"", ""Component"", ""A component responsible for managing the airline's notification system and sending flight updates to users in Teams.""],
         [""Azure Bot Service"", ""System"", ""The underlying system utilized by the Custom bot messaging service.""],
         [""Custom bots"", ""Component"", ""Components that send flight updates to users in Teams.""],
         [""Airline's notification system"", ""System"", ""The overall system managed by the custom bot messaging service and Azure Bot Service for notifications.""],
         [""Azure Data Lake storage"", ""System"", ""A system offering long-term retention and micro-batch processing of events from Event Hubs.""],
         [""Power BI"", ""System"", ""A business intelligence tool generating insightful reports with data processed from the Azure Data Lake.""]
      ],
      ""relationships"": [
         [""Airline System"", ""calls"", ""Custom API Hosted in Azure API Management""],
         [""Custom API Coordinator"", ""part-of"", ""Power Apps""],
         [""Azure Storage Account queue"", ""part-of"", ""Airline System""],
         [""Azure Functions"", ""part-of"", ""Airline System""],
         [""Azure Event Hubs"", ""called-from"", ""Azure Functions""],
         [""Custom bot messaging service that employs Azure Bot Service"", ""part-of"", ""Airline's notification system""],
         [""Custom bots"", ""employed-by"", ""Custom bot messaging service that employs Azure Bot Service""],
         [""Azure Data Lake storage"", ""called-from"", ""Azure Event Hubs""],
         [""Power BI"", ""processes"", ""Data from Azure Data Lake""]
      ]
   }"," {
      ""entities"": [
         [""Airline System"", ""System"", ""A system that communicates with a custom API hosted in Azure API Management.""],
         [""Custom API Hosted in Azure API Management"", ""Component"", ""A component that is used by the Airline System for communication.""],
         [""Custom API Coordinator"", ""Component"", ""A component that receives notifications and handles incoming messages from the Airline System, sending them to Power Apps.""],
         [""Power Apps"", ""System"", ""A system where flights are assigned to Microsoft Teams channels.""],
         [""Microsoft Teams Channels"", ""System"", ""A system in which flights are assigned.""],
         [""Azure Storage Account queue"", ""Component"", ""A component that queues a Graph API call for further processing.""],
         [""Azure Functions"", ""Component"", ""A component that runs the Graph API calls based on incoming messages in the storage queue, sending notifications to Teams and streaming all events to an Azure Event Hubs.""],
         [""Azure Event Hubs"", ""System"", ""A system for further analytics of the streamed events.""],
         [""Custom bot messaging service that employs Azure Bot Service"", ""Component"", ""A component responsible for managing the airline's notification system and sending flight updates to users in Teams.""],
         [""Azure Bot Service"", ""System"", ""The underlying system utilized by the Custom bot messaging service.""],
         [""Custom bots"", ""Component"", ""Components that send flight updates to users in Teams.""],
         [""Airline's notification system"", ""System"", ""The overall system managed by the custom bot messaging service and Azure Bot Service for notifications.""],
         [""Azure Data Lake storage"", ""System"", ""A system offering long-term retention and micro-batch processing of events from Event Hubs.""],
         [""Power BI"", ""System"", ""A business intelligence tool generating insightful reports with data processed from the Azure Data Lake.""]
      ],
      ""relationships"": [
         [""Airline System"", ""calls"", ""Custom API Hosted in Azure API Management""],
         [""Custom API Coordinator"", ""part-of"", ""Power Apps""],
         [""Azure Storage Account queue"", ""part-of"", ""Airline System""],
         [""Azure Functions"", ""part-of"", ""Airline System""],
         [""Azure Event Hubs"", ""called-from"", ""Azure Functions""],
         [""Custom bot messaging service that employs Azure Bot Service"", ""part-of"", ""Airline's notification system""],
         [""Custom bots"", ""employed-by"", ""Custom bot messaging service that employs Azure Bot Service""],
         [""Azure Data Lake storage"", ""called-from"", ""Azure Event Hubs""],
         [""Power BI"", ""processes"", ""Data from Azure Data Lake""]
      ]
   }",6.885282072995324
47,mistral:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}"," {
      ""entities"": [
         [""CDS"", ""component"", ""Cloud Data Storage (CDS)""],
         [""Data Factory"", ""component"", ""Data Factory service""],
         [""SQL"", ""component"", ""Azure SQL Database""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning (ERP) system""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Azure Functions"", ""component"", ""Azure Functions service""],
         [""Power Apps"", ""component"", ""Microsoft Power Apps""],
         [""Azure SQL Database API"", ""component"", ""API for accessing Azure SQL Database""],
         [""Power BI"", ""component"", ""Microsoft Power BI""],
         [""Users"", ""person"", ""Human users who review and update PO data""]
      ],
      ""relationships"": [
         [""CDS"", ""contains"", ""Data Factory""],
         [""Data Factory"", ""calls"", ""SQL""],
         [""ERP system"", ""part-of"", ""PO data""],
         [""PO data"", ""part-of"", ""Azure SQL database""],
         [""Azure Functions"", ""uses"", ""API""],
         [""Azure Functions"", ""creates"", [""task"", ""for users to review""]],
         [""Power Apps"", ""retrieves"", [""data"", ""from Azure SQL Database via API""]],
         [""users"", ""updates"", ""POs""],
         [""Power Apps"", ""sends"", [""data"", ""to suppliers through CSV export""]],
         [""Power BI"", ""reports trends"", ""supplier status""]
      ]
   }"," {
      ""entities"": [
         [""CDS"", ""component"", ""Cloud Data Storage (CDS)""],
         [""Data Factory"", ""component"", ""Data Factory service""],
         [""SQL"", ""component"", ""Azure SQL Database""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning (ERP) system""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Azure Functions"", ""component"", ""Azure Functions service""],
         [""Power Apps"", ""component"", ""Microsoft Power Apps""],
         [""Azure SQL Database API"", ""component"", ""API for accessing Azure SQL Database""],
         [""Power BI"", ""component"", ""Microsoft Power BI""],
         [""Users"", ""person"", ""Human users who review and update PO data""]
      ],
      ""relationships"": [
         [""CDS"", ""contains"", ""Data Factory""],
         [""Data Factory"", ""calls"", ""SQL""],
         [""ERP system"", ""part-of"", ""PO data""],
         [""PO data"", ""part-of"", ""Azure SQL database""],
         [""Azure Functions"", ""uses"", ""API""],
         [""Azure Functions"", ""creates"", [""task"", ""for users to review""]],
         [""Power Apps"", ""retrieves"", [""data"", ""from Azure SQL Database via API""]],
         [""users"", ""updates"", ""POs""],
         [""Power Apps"", ""sends"", [""data"", ""to suppliers through CSV export""]],
         [""Power BI"", ""reports trends"", ""supplier status""]
      ]
   }",4.139161271974444
48,mistral:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}"," {
      ""entities"": [
         [""Outlook email account"", ""person"", ""A designated email account that receives PDF files as attachments.""],
         [""Logic app"", ""component"", ""A piece of software built using the capabilities of Azure Logic Apps, responsible for processing incoming emails and uploading PDF files to an Azure Data Lake Storage container.""],
         [""Azure Data Lake Storage"", ""system"", ""An Azure service where PDF files are stored.""],
         [""PDF container"", ""component"", ""A section within Azure Data Lake Storage where PDF files are stored.""],
         [""Function app"", ""component"", ""A piece of software built using the capabilities of Azure Functions, responsible for processing single-page PDF files after they have been split by the logic app.""],
         [""HTTPS POST"", ""relationship"", ""A method used to send a location of a file to an Azure REST API.""],
         [""Azure REST API"", ""component"", ""An interface offered by Azure for programmatic interactions with its services.""],
         [""AI Document Intelligence"", ""component"", ""An Azure service that processes PDF files and returns response data.""],
         [""data structure"", ""component"", ""A format used to store the processed information returned by AI Document Intelligence.""],
         [""third container in Data Lake Storage"", ""component"", ""Another section within Azure Data Lake Storage where JSON data files containing response data are stored.""],
         [""Azure Cosmos DB"", ""system"", ""An Azure service that saves the processed data in a database and collections.""],
         [""Power BI"", ""component"", ""A business analytics tool that obtains data from Azure Cosmos DB, providing insights and dashboards.""]
      ],
      ""relationships"": [
         [""Logic app"", ""part-of"", ""Azure Logic Apps"", ""The logic app is built using the capabilities of Azure Logic Apps.""],
         [""PDF container"", ""part-of"", ""Azure Data Lake Storage"", ""The PDF container is a section within Azure Data Lake Storage.""],
         [""Function app"", ""part-of"", ""Azure Functions"", ""The function app is built using the capabilities of Azure Functions.""],
         [""AI Document Intelligence"", ""calls"", ""Function app"", ""The function app sends the location of single-page PDF files to AI Document Intelligence for processing.""],
         [""Function app"", ""called-from"", ""forms processing logic app"", ""The forms processing logic app sends processed response data to the function app.""],
         [""Azure Cosmos DB"", ""part-of"", ""Azure services"", ""Azure Cosmos DB is an Azure service.""]
      ]
   }"," {
      ""entities"": [
         [""Outlook email account"", ""person"", ""A designated email account that receives PDF files as attachments.""],
         [""Logic app"", ""component"", ""A piece of software built using the capabilities of Azure Logic Apps, responsible for processing incoming emails and uploading PDF files to an Azure Data Lake Storage container.""],
         [""Azure Data Lake Storage"", ""system"", ""An Azure service where PDF files are stored.""],
         [""PDF container"", ""component"", ""A section within Azure Data Lake Storage where PDF files are stored.""],
         [""Function app"", ""component"", ""A piece of software built using the capabilities of Azure Functions, responsible for processing single-page PDF files after they have been split by the logic app.""],
         [""HTTPS POST"", ""relationship"", ""A method used to send a location of a file to an Azure REST API.""],
         [""Azure REST API"", ""component"", ""An interface offered by Azure for programmatic interactions with its services.""],
         [""AI Document Intelligence"", ""component"", ""An Azure service that processes PDF files and returns response data.""],
         [""data structure"", ""component"", ""A format used to store the processed information returned by AI Document Intelligence.""],
         [""third container in Data Lake Storage"", ""component"", ""Another section within Azure Data Lake Storage where JSON data files containing response data are stored.""],
         [""Azure Cosmos DB"", ""system"", ""An Azure service that saves the processed data in a database and collections.""],
         [""Power BI"", ""component"", ""A business analytics tool that obtains data from Azure Cosmos DB, providing insights and dashboards.""]
      ],
      ""relationships"": [
         [""Logic app"", ""part-of"", ""Azure Logic Apps"", ""The logic app is built using the capabilities of Azure Logic Apps.""],
         [""PDF container"", ""part-of"", ""Azure Data Lake Storage"", ""The PDF container is a section within Azure Data Lake Storage.""],
         [""Function app"", ""part-of"", ""Azure Functions"", ""The function app is built using the capabilities of Azure Functions.""],
         [""AI Document Intelligence"", ""calls"", ""Function app"", ""The function app sends the location of single-page PDF files to AI Document Intelligence for processing.""],
         [""Function app"", ""called-from"", ""forms processing logic app"", ""The forms processing logic app sends processed response data to the function app.""],
         [""Azure Cosmos DB"", ""part-of"", ""Azure services"", ""Azure Cosmos DB is an Azure service.""]
      ]
   }",6.395063363015652
49,mistral:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A service for storing files.""],
         [""command message"", ""component"", ""A message that initiates pipeline processing.""],
         [""storage queue"", ""component"", ""A data structure in Azure where messages can be stored.""],
         [""Durable Functions orchestration"", ""component"", ""An Azure event-driven architecture for composing microservices.""],
         [""Computer Vision Read API"", ""component"", ""An API provided by Microsoft Cognitive Services for Optical Character Recognition (OCR).""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""A managed container orchestration service by Microsoft Azure.""],
         [""document classifier service"", ""component"", ""A service hosted in AKS that identifies document types and calculates page ranges.""],
         [""Azure Cosmos DB store"", ""component"", ""An Azure NoSQL database for storing structured data.""],
         [""Cognitive Search service"", ""system"", ""An Azure search service using AI capabilities.""],
         [""Azure AI Search libraries for .NET"", ""component"", ""Libraries for integrating Azure Cognitive Search with .NET applications.""],
         [""Optical character recognition (OCR) results"", ""data"", ""Text extracted from the document by OCR.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""command message""],
         [""Durable Functions orchestration"", ""part-of"", ""command message""],
         [""Durable Functions orchestration"", ""calls"", ""Computer Vision Read API""],
         [""Durable Functions orchestration"", ""calls"", ""document classifier service""],
         [""Metadata Store activity function"", ""part-of"", ""Durable Functions orchestration""],
         [""Metadata Store activity function"", ""contains"", ""document type and page range information""],
         [""Metadata Store activity function"", ""part-of"", ""Azure Cosmos DB store""],
         [""Indexing activity function"", ""part-of"", ""Durable Functions orchestration""],
         [""Indexing activity function"", ""calls"", ""Cognitive Search service""],
         [""Indexing activity function"", ""contains"", ""full OCR results and document information""],
         [""end users"", ""interacts with"", ""search result set""],
         [""search result set"", ""correlates with"", ""Azure Cosmos DB store""],
         [""search result set"", ""contains"", ""Correlation IDs""],
         [""search result set"", ""contains"", ""links to the original document file in Blob Storage""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A service for storing files.""],
         [""command message"", ""component"", ""A message that initiates pipeline processing.""],
         [""storage queue"", ""component"", ""A data structure in Azure where messages can be stored.""],
         [""Durable Functions orchestration"", ""component"", ""An Azure event-driven architecture for composing microservices.""],
         [""Computer Vision Read API"", ""component"", ""An API provided by Microsoft Cognitive Services for Optical Character Recognition (OCR).""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""A managed container orchestration service by Microsoft Azure.""],
         [""document classifier service"", ""component"", ""A service hosted in AKS that identifies document types and calculates page ranges.""],
         [""Azure Cosmos DB store"", ""component"", ""An Azure NoSQL database for storing structured data.""],
         [""Cognitive Search service"", ""system"", ""An Azure search service using AI capabilities.""],
         [""Azure AI Search libraries for .NET"", ""component"", ""Libraries for integrating Azure Cognitive Search with .NET applications.""],
         [""Optical character recognition (OCR) results"", ""data"", ""Text extracted from the document by OCR.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""command message""],
         [""Durable Functions orchestration"", ""part-of"", ""command message""],
         [""Durable Functions orchestration"", ""calls"", ""Computer Vision Read API""],
         [""Durable Functions orchestration"", ""calls"", ""document classifier service""],
         [""Metadata Store activity function"", ""part-of"", ""Durable Functions orchestration""],
         [""Metadata Store activity function"", ""contains"", ""document type and page range information""],
         [""Metadata Store activity function"", ""part-of"", ""Azure Cosmos DB store""],
         [""Indexing activity function"", ""part-of"", ""Durable Functions orchestration""],
         [""Indexing activity function"", ""calls"", ""Cognitive Search service""],
         [""Indexing activity function"", ""contains"", ""full OCR results and document information""],
         [""end users"", ""interacts with"", ""search result set""],
         [""search result set"", ""correlates with"", ""Azure Cosmos DB store""],
         [""search result set"", ""contains"", ""Correlation IDs""],
         [""search result set"", ""contains"", ""links to the original document file in Blob Storage""]
      ]
   }",6.516149835020769
50,mistral:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""An application that acts as a reverse proxy for Azure AD B2C and forwards authentication requests to it.""],
         [""API Management"", ""component"", ""A service that intercepts requests to backend APIs, validates the bearer token against Azure AD B2C, and forwards the request to the appropriate backend API if the token is valid.""],
         [""Azure AD B2C"", ""system"", ""An identity provider used for authentication in this solution.""],
         [""custom domain"", ""component"", ""A specific domain configured for sign-in in Azure AD B2C.""],
         [""backend API"", ""component"", ""The application or service that the client is trying to access.""],
         [""App Service Environment"", ""component"", ""An environment where some backend APIs are running.""],
         [""Function Apps"", ""component"", ""An environment where some backend APIs are running.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""An environment where some backend APIs are running.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management"", ""The Azure Front Door is a component of API Management.""],
         [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""Azure Front Door initiates authentication requests to Azure AD B2C.""],
         [""API Management"", ""part-of"", ""Azure AD B2C"", ""API Management uses the validate-jwt policy to validate the bearer token against Azure AD B2C.""],
         [""API Management"", ""calls"", ""backend API"", ""If the token is valid, API Management forwards the request to the appropriate backend API.""],
         [""App Service Environment"", ""part-of"", ""backend API"", ""Some backend APIs are running in App Service Environment.""],
         [""Function Apps"", ""part-of"", ""backend API"", ""Some backend APIs are running in Function Apps.""],
         [""Azure Kubernetes Services (AKS)"", ""part-of"", ""backend API"", ""Some backend APIs are running in Azure Kubernetes Services (AKS).""]
      ]
   }"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""An application that acts as a reverse proxy for Azure AD B2C and forwards authentication requests to it.""],
         [""API Management"", ""component"", ""A service that intercepts requests to backend APIs, validates the bearer token against Azure AD B2C, and forwards the request to the appropriate backend API if the token is valid.""],
         [""Azure AD B2C"", ""system"", ""An identity provider used for authentication in this solution.""],
         [""custom domain"", ""component"", ""A specific domain configured for sign-in in Azure AD B2C.""],
         [""backend API"", ""component"", ""The application or service that the client is trying to access.""],
         [""App Service Environment"", ""component"", ""An environment where some backend APIs are running.""],
         [""Function Apps"", ""component"", ""An environment where some backend APIs are running.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""An environment where some backend APIs are running.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management"", ""The Azure Front Door is a component of API Management.""],
         [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""Azure Front Door initiates authentication requests to Azure AD B2C.""],
         [""API Management"", ""part-of"", ""Azure AD B2C"", ""API Management uses the validate-jwt policy to validate the bearer token against Azure AD B2C.""],
         [""API Management"", ""calls"", ""backend API"", ""If the token is valid, API Management forwards the request to the appropriate backend API.""],
         [""App Service Environment"", ""part-of"", ""backend API"", ""Some backend APIs are running in App Service Environment.""],
         [""Function Apps"", ""part-of"", ""backend API"", ""Some backend APIs are running in Function Apps.""],
         [""Azure Kubernetes Services (AKS)"", ""part-of"", ""backend API"", ""Some backend APIs are running in Azure Kubernetes Services (AKS).""]
      ]
   }",5.4507342030410655
51,mistral:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}"," {
      ""entities"": [
         [""Connect agent component"", ""component"", ""Captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""A publisher component"", ""component"", ""For mainframe systems, manages data migration.""],
         [""A listener component"", ""component"", ""For midrange systems, manages data migration. It's located on either a Windows or Linux machine.""],
         [""The Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target.""],
         [""The target"", ""system"", ""Receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""system"", ""Consumes the changed data and applies it to Azure data platform services.""],
         [""The Connect Controller Daemon"", ""component"", ""Authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]
      ],
      ""relationships"": [
         [""Connect agent component"", ""part-of"", ""A publisher component""],
         [""Connect agent component"", ""part-of"", ""A listener component""],
         [""A publisher component"", ""contains"", ""The Connect Replicator Engine""],
         [""A listener component"", ""contains"", ""The Connect Replicator Engine""],
         [""A publisher component"", ""calls"", ""The Connect Controller Daemon""],
         [""A listener component"", ""calls"", ""The Connect Controller Daemon""],
         [""The Connect Replicator Engine"", ""part-of"", ""The target""],
         [""The target"", ""called-from"", ""Azure Databricks""]
      ]
   }"," {
      ""entities"": [
         [""Connect agent component"", ""component"", ""Captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""A publisher component"", ""component"", ""For mainframe systems, manages data migration.""],
         [""A listener component"", ""component"", ""For midrange systems, manages data migration. It's located on either a Windows or Linux machine.""],
         [""The Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target.""],
         [""The target"", ""system"", ""Receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""system"", ""Consumes the changed data and applies it to Azure data platform services.""],
         [""The Connect Controller Daemon"", ""component"", ""Authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]
      ],
      ""relationships"": [
         [""Connect agent component"", ""part-of"", ""A publisher component""],
         [""Connect agent component"", ""part-of"", ""A listener component""],
         [""A publisher component"", ""contains"", ""The Connect Replicator Engine""],
         [""A listener component"", ""contains"", ""The Connect Replicator Engine""],
         [""A publisher component"", ""calls"", ""The Connect Controller Daemon""],
         [""A listener component"", ""calls"", ""The Connect Controller Daemon""],
         [""The Connect Replicator Engine"", ""part-of"", ""The target""],
         [""The target"", ""called-from"", ""Azure Databricks""]
      ]
   }",4.408054556988645
52,mistral:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A web-based hosting service for version control using Git""],
         [""CI/CD pipeline agent"", ""component"", ""An Azure VM-installed tool that manages continuous integration and delivery pipelines""],
         [""Azure VM"", ""component"", ""A virtual machine provided by Microsoft's Azure cloud platform""],
         [""API application"", ""component"", ""An application providing APIs to be consumed""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment""],
         [""Azure API Management"", ""system"", ""Microsoft's cloud-based solution for publishing, managing, and securing APIs""],
         [""HOST headers"", ""component"", ""A method used to specify the domain name of a host in an HTTP request header""],
         [""API Management policy"", ""component"", ""Configurations set by the API Management system to manage its behavior""],
         [""App Service Environment's DNS name"", ""entity"", ""The Domain Name System (DNS) name of the App Service Environment""],
         [""Application Gateway"", ""system"", ""A Microsoft Azure web traffic management service for exposing internal applications publicly""],
         [""Azure Private DNS"", ""system"", ""Microsoft Azure's private managed DNS service to route traffic internally""],
         [""External users"", ""person"", ""Users outside the internal network consuming APIs via Application Gateway""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent"", ""Developers check in code to a GitHub repository""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM"", ""The CI/CD pipeline agent is installed on an Azure VM""],
         [""CI/CD pipeline agent"", ""calls"", ""API application"", ""The agent pushes the build to the API application""],
         [""API application"", ""part-of"", ""ILB ASE"", ""The API application is hosted on ILB ASE""],
         [""Azure API Management"", ""consumes"", ""preceding APIs"", ""Azure API Management consumes the preceding APIs via HOST headers""],
         [""Azure API Management"", ""uses"", ""App Service Environment's DNS name"", ""API Management uses the App Service Environment's DNS name for all the APIs""],
         [""Application Gateway"", ""exposes"", ""API Management"", ""Application Gateway exposes API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", ""traffic internally"", ""Azure Private DNS is used to route traffic internally between ASE, API Management, and Application Gateway""],
         [""External users"", ""utilizes"", ""exposed developer portal"", ""External users utilize the exposed developer portal to consume APIs via Application Gateway's public IP""]
      ]
   }"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A web-based hosting service for version control using Git""],
         [""CI/CD pipeline agent"", ""component"", ""An Azure VM-installed tool that manages continuous integration and delivery pipelines""],
         [""Azure VM"", ""component"", ""A virtual machine provided by Microsoft's Azure cloud platform""],
         [""API application"", ""component"", ""An application providing APIs to be consumed""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment""],
         [""Azure API Management"", ""system"", ""Microsoft's cloud-based solution for publishing, managing, and securing APIs""],
         [""HOST headers"", ""component"", ""A method used to specify the domain name of a host in an HTTP request header""],
         [""API Management policy"", ""component"", ""Configurations set by the API Management system to manage its behavior""],
         [""App Service Environment's DNS name"", ""entity"", ""The Domain Name System (DNS) name of the App Service Environment""],
         [""Application Gateway"", ""system"", ""A Microsoft Azure web traffic management service for exposing internal applications publicly""],
         [""Azure Private DNS"", ""system"", ""Microsoft Azure's private managed DNS service to route traffic internally""],
         [""External users"", ""person"", ""Users outside the internal network consuming APIs via Application Gateway""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent"", ""Developers check in code to a GitHub repository""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM"", ""The CI/CD pipeline agent is installed on an Azure VM""],
         [""CI/CD pipeline agent"", ""calls"", ""API application"", ""The agent pushes the build to the API application""],
         [""API application"", ""part-of"", ""ILB ASE"", ""The API application is hosted on ILB ASE""],
         [""Azure API Management"", ""consumes"", ""preceding APIs"", ""Azure API Management consumes the preceding APIs via HOST headers""],
         [""Azure API Management"", ""uses"", ""App Service Environment's DNS name"", ""API Management uses the App Service Environment's DNS name for all the APIs""],
         [""Application Gateway"", ""exposes"", ""API Management"", ""Application Gateway exposes API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", ""traffic internally"", ""Azure Private DNS is used to route traffic internally between ASE, API Management, and Application Gateway""],
         [""External users"", ""utilizes"", ""exposed developer portal"", ""External users utilize the exposed developer portal to consume APIs via Application Gateway's public IP""]
      ]
   }",6.853541246964596
53,mistral:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}"," {
      ""entities"": [
         [""Airline System"", ""System"", ""The data flows through the solution as follows.""],
         [""Custom API"", ""Component"", ""The airline system communicates with a custom API hosted in Azure API Management.""],
         [""Azure API Management"", ""System"", """"],
         [""Custom API Coordinator"", ""Component"", ""A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps,""],
         [""Power Apps"", ""System"", ""It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.""],
         [""Microsoft Teams Channels"", ""Component"", ""where flights are assigned to Microsoft Teams channels""],
         [""Azure Storage Account queue"", ""Component"", ""When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.""],
         [""Azure Functions"", ""System"", """"],
         [""Graph API"", ""Component"", ""Azure Functions runs the Graph API calls based on the incoming messages in the storage queue,""],
         [""Azure Event Hubs"", ""System"", ""sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.""],
         [""Custom Bot Messaging Service"", ""Component"", ""The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.""],
         [""Azure Bot Service"", ""System"", """"],
         [""Custom Bots"", ""Component"", ""Custom bots send flight updates to users in Teams.""],
         [""Azure Data Lake storage"", ""System"", ""An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.""],
         [""Power BI"", ""System"", """"]
      ],
      ""relationships"": [
         [""Airline System"", ""contains"", ""Custom API""],
         [""Azure API Management"", ""part-of"", ""Custom API""],
         [""Custom API Coordinator"", ""part-of"", ""Power Apps""],
         [""Power Apps"", ""part-of"", ""Microsoft Teams Channels""],
         [""Airline System"", ""calls"", ""Azure Storage Account queue""],
         [""Azure Functions"", ""part-of"", ""Graph API""],
         [""Azure Functions"", ""part-of"", ""Azure Event Hubs""],
         [""Custom Bot Messaging Service"", ""part-of"", ""Azure Bot Service""],
         [""Custom Bots"", ""part-of"", ""Azure Bot Service""]
      ]
   }"," {
      ""entities"": [
         [""Airline System"", ""System"", ""The data flows through the solution as follows.""],
         [""Custom API"", ""Component"", ""The airline system communicates with a custom API hosted in Azure API Management.""],
         [""Azure API Management"", ""System"", """"],
         [""Custom API Coordinator"", ""Component"", ""A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps,""],
         [""Power Apps"", ""System"", ""It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.""],
         [""Microsoft Teams Channels"", ""Component"", ""where flights are assigned to Microsoft Teams channels""],
         [""Azure Storage Account queue"", ""Component"", ""When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.""],
         [""Azure Functions"", ""System"", """"],
         [""Graph API"", ""Component"", ""Azure Functions runs the Graph API calls based on the incoming messages in the storage queue,""],
         [""Azure Event Hubs"", ""System"", ""sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.""],
         [""Custom Bot Messaging Service"", ""Component"", ""The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.""],
         [""Azure Bot Service"", ""System"", """"],
         [""Custom Bots"", ""Component"", ""Custom bots send flight updates to users in Teams.""],
         [""Azure Data Lake storage"", ""System"", ""An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.""],
         [""Power BI"", ""System"", """"]
      ],
      ""relationships"": [
         [""Airline System"", ""contains"", ""Custom API""],
         [""Azure API Management"", ""part-of"", ""Custom API""],
         [""Custom API Coordinator"", ""part-of"", ""Power Apps""],
         [""Power Apps"", ""part-of"", ""Microsoft Teams Channels""],
         [""Airline System"", ""calls"", ""Azure Storage Account queue""],
         [""Azure Functions"", ""part-of"", ""Graph API""],
         [""Azure Functions"", ""part-of"", ""Azure Event Hubs""],
         [""Custom Bot Messaging Service"", ""part-of"", ""Azure Bot Service""],
         [""Custom Bots"", ""part-of"", ""Azure Bot Service""]
      ]
   }",6.356744203018025
54,mistral:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""Storage service provided by Azure""],
         [""Azure Storage Account"", ""component"", ""Account in Azure Blob Storage""],
         [""Azure Storage Explorer"", ""component"", ""UI-based tool for managing Azure Storage""],
         [""Storage SDK or API"", ""component"", ""Software Development Kits (SDK) and Application Programming Interfaces (API) provided by Azure for storage operations""],
         [""Azure AI Speech"", ""system"", ""Speech recognition service provided by Azure""],
         [""Batch mode asynchronously"", ""component"", ""Method of processing multiple audio files simultaneously""],
         [""speaker diarization enabled"", ""component"", ""Feature that identifies speakers in the audio file""],
         [""Azure AI Language"", ""system"", ""Natural Language Processing service provided by Azure""],
         [""personal data"", ""component"", ""Data related to individuals, such as names or addresses""],
         [""Ingestion Client tool"", ""component"", ""No-code approach for call center transcription provided by Azure""],
         [""AI services"", ""system"", "" collectively refers to Azure AI Speech, Azure AI Language and Ingestion Client tool""],
         [""Azure OpenAI"", ""system"", ""Service provided by Azure for processing natural language text""],
         [""Power BI"", ""component"", ""Business analytics service for visualizing data""],
         [""custom web application"", ""component"", ""Web application customized for specific requirements""],
         [""App Service"", ""component"", ""Microsoft's cloud computing platform for hosting web applications""],
         [""customer relationship management (CRM)"", ""system"", ""System used by call centers to manage and analyze customer interactions""],
         [""Power BI or a custom web application"", ""component"", ""Options for visualizing the processed output""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", [""Azure Storage Account""]],
         [""Azure AI Speech"", ""calls"", [""Azure Blob Storage""]],
         [""Azure AI Language"", ""called-from"", [""Azure AI Speech""]],
         [""Ingestion Client tool"", ""part-of"", [""AI services""]],
         [""Azure OpenAI"", ""processes"", [""transcript""]],
         [""Power BI or a custom web application"", ""visualizes"", [""output""]]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""Storage service provided by Azure""],
         [""Azure Storage Account"", ""component"", ""Account in Azure Blob Storage""],
         [""Azure Storage Explorer"", ""component"", ""UI-based tool for managing Azure Storage""],
         [""Storage SDK or API"", ""component"", ""Software Development Kits (SDK) and Application Programming Interfaces (API) provided by Azure for storage operations""],
         [""Azure AI Speech"", ""system"", ""Speech recognition service provided by Azure""],
         [""Batch mode asynchronously"", ""component"", ""Method of processing multiple audio files simultaneously""],
         [""speaker diarization enabled"", ""component"", ""Feature that identifies speakers in the audio file""],
         [""Azure AI Language"", ""system"", ""Natural Language Processing service provided by Azure""],
         [""personal data"", ""component"", ""Data related to individuals, such as names or addresses""],
         [""Ingestion Client tool"", ""component"", ""No-code approach for call center transcription provided by Azure""],
         [""AI services"", ""system"", "" collectively refers to Azure AI Speech, Azure AI Language and Ingestion Client tool""],
         [""Azure OpenAI"", ""system"", ""Service provided by Azure for processing natural language text""],
         [""Power BI"", ""component"", ""Business analytics service for visualizing data""],
         [""custom web application"", ""component"", ""Web application customized for specific requirements""],
         [""App Service"", ""component"", ""Microsoft's cloud computing platform for hosting web applications""],
         [""customer relationship management (CRM)"", ""system"", ""System used by call centers to manage and analyze customer interactions""],
         [""Power BI or a custom web application"", ""component"", ""Options for visualizing the processed output""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", [""Azure Storage Account""]],
         [""Azure AI Speech"", ""calls"", [""Azure Blob Storage""]],
         [""Azure AI Language"", ""called-from"", [""Azure AI Speech""]],
         [""Ingestion Client tool"", ""part-of"", [""AI services""]],
         [""Azure OpenAI"", ""processes"", [""transcript""]],
         [""Power BI or a custom web application"", ""visualizes"", [""output""]]
      ]
   }",5.868211290042382
55,mistral:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A system that is triggered by HTTP requests, allows nesting workflows for complex orchestration and uses connectors to integrate with commonly used services.""],
         [""Azure API Management"", ""system"", ""A system consisting of two related components: an API gateway and a developer portal.""],
         [""API Gateway"", ""component"", ""Accepts HTTP calls and routes them to the backend.""],
         [""Developer Portal"", ""component"", ""Provides access to documentation, code samples for calling APIs and allows testing of APIs.""],
         [""Azure DNS"", ""system"", ""A system that provides name resolution by using the Azure infrastructure.""],
         [""Microsoft Entra ID"", ""person"", ""Used to authenticate clients that call the API gateway.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""calls"", ""API Gateway""],
         [""API Gateway"", ""part-of"", ""Azure API Management""]
      ]
   }"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A system that is triggered by HTTP requests, allows nesting workflows for complex orchestration and uses connectors to integrate with commonly used services.""],
         [""Azure API Management"", ""system"", ""A system consisting of two related components: an API gateway and a developer portal.""],
         [""API Gateway"", ""component"", ""Accepts HTTP calls and routes them to the backend.""],
         [""Developer Portal"", ""component"", ""Provides access to documentation, code samples for calling APIs and allows testing of APIs.""],
         [""Azure DNS"", ""system"", ""A system that provides name resolution by using the Azure infrastructure.""],
         [""Microsoft Entra ID"", ""person"", ""Used to authenticate clients that call the API gateway.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""calls"", ""API Gateway""],
         [""API Gateway"", ""part-of"", ""Azure API Management""]
      ]
   }",2.991925205977168
56,mistral:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}"," {
      ""entities"": [
         [""Data owners"", ""person"", ""Individuals who upload datasets into a public blob storage account.""],
         [""Blob storage account"", ""system"", ""A cloud-based storage service where data is temporarily stored before processing and analysis.""],
         [""Microsoft-managed keys"", ""component"", ""Encryption keys provided by Microsoft for secure storage of data.""],
         [""Azure Data Factory"", ""component"", ""An Azure service that manages data pipelines and workflows, used in this case to copy data from one storage account to another.""],
         [""trigger"", ""component"", ""An event that initiates the execution of a specified action or process in Azure Data Factory.""],
         [""import path"", ""component"", ""The specific location on another storage account where the copied dataset is stored after being triggered by Azure Data Factory.""],
         [""private endpoint"", ""component"", ""A networking feature in Azure that allows access to resources privately, without exposing them to the public internet.""],
         [""service principal"", ""component"", ""An identity representing a service or application in Azure Active Directory. It has limited permissions and is used here for secure data access.""],
         [""Azure Virtual Desktop"", ""component"", ""A cloud-based desktop and app virtualization technology, functioning as a privileged jump box to the secure environment.""],
         [""streaming application"", ""component"", ""The software through which researchers access the secure environment.""],
         [""Azure Machine Learning compute"", ""component"", ""An Azure service used for training, deploying, automating, and managing machine learning models within the secure environment.""],
         [""regulatory guidelines"", ""component"", ""Standards set by regulatory bodies that must be adhered to in handling data and creating models.""],
         [""personal information"", ""component"", ""Data related to individuals that requires protection under privacy laws.""],
         [""de-identified data"", ""component"", ""Data from which personal information has been removed, making it compliant with regulatory guidelines.""],
         [""export path"", ""component"", ""The location on the secure storage account where de-identified data is stored for export.""],
         [""Logic App"", ""component"", ""An Azure service that triggers a manual review process when new data is added to the export path.""],
         [""manual reviewers"", ""person"", ""Individuals responsible for reviewing data before it is approved or denied for export.""],
         [""Data Factory instance"", ""component"", ""A specific instance of Azure Data Factory used for moving data from the secure storage account to a public storage account.""],
         [""public storage account"", ""system"", ""A cloud-based storage service where external researchers can access their exported data and models.""]
      ],
      ""relationships"": [
         [""Data owners"", ""calls"", ""Blob storage account""],
         [""Blob storage account"", ""part-of"", ""Microsoft-managed keys""],
         [""Azure Data Factory"", ""part-of"", ""trigger""],
         [""trigger"", ""calls"", ""import path""],
         [""import path"", ""part-of"", ""private endpoint""],
         [""import path"", ""part-of"", ""service principal""],
         [""Data Factory"", ""calls"", ""Azure Virtual Desktop""],
         [""Azure Virtual Desktop"", ""part-of"", ""streaming application""],
         [""secure storage account"", ""contains"", ""dataset""],
         [""secure storage account"", ""accessed-by"", ""Azure Machine Learning compute""],
         [""de-identified data"", ""saved-to"", ""export path""],
         [""export path"", ""called-from"", ""Logic App""],
         [""Data Factory instance"", ""calls"", ""public storage account""]
      ]
   }"," {
      ""entities"": [
         [""Data owners"", ""person"", ""Individuals who upload datasets into a public blob storage account.""],
         [""Blob storage account"", ""system"", ""A cloud-based storage service where data is temporarily stored before processing and analysis.""],
         [""Microsoft-managed keys"", ""component"", ""Encryption keys provided by Microsoft for secure storage of data.""],
         [""Azure Data Factory"", ""component"", ""An Azure service that manages data pipelines and workflows, used in this case to copy data from one storage account to another.""],
         [""trigger"", ""component"", ""An event that initiates the execution of a specified action or process in Azure Data Factory.""],
         [""import path"", ""component"", ""The specific location on another storage account where the copied dataset is stored after being triggered by Azure Data Factory.""],
         [""private endpoint"", ""component"", ""A networking feature in Azure that allows access to resources privately, without exposing them to the public internet.""],
         [""service principal"", ""component"", ""An identity representing a service or application in Azure Active Directory. It has limited permissions and is used here for secure data access.""],
         [""Azure Virtual Desktop"", ""component"", ""A cloud-based desktop and app virtualization technology, functioning as a privileged jump box to the secure environment.""],
         [""streaming application"", ""component"", ""The software through which researchers access the secure environment.""],
         [""Azure Machine Learning compute"", ""component"", ""An Azure service used for training, deploying, automating, and managing machine learning models within the secure environment.""],
         [""regulatory guidelines"", ""component"", ""Standards set by regulatory bodies that must be adhered to in handling data and creating models.""],
         [""personal information"", ""component"", ""Data related to individuals that requires protection under privacy laws.""],
         [""de-identified data"", ""component"", ""Data from which personal information has been removed, making it compliant with regulatory guidelines.""],
         [""export path"", ""component"", ""The location on the secure storage account where de-identified data is stored for export.""],
         [""Logic App"", ""component"", ""An Azure service that triggers a manual review process when new data is added to the export path.""],
         [""manual reviewers"", ""person"", ""Individuals responsible for reviewing data before it is approved or denied for export.""],
         [""Data Factory instance"", ""component"", ""A specific instance of Azure Data Factory used for moving data from the secure storage account to a public storage account.""],
         [""public storage account"", ""system"", ""A cloud-based storage service where external researchers can access their exported data and models.""]
      ],
      ""relationships"": [
         [""Data owners"", ""calls"", ""Blob storage account""],
         [""Blob storage account"", ""part-of"", ""Microsoft-managed keys""],
         [""Azure Data Factory"", ""part-of"", ""trigger""],
         [""trigger"", ""calls"", ""import path""],
         [""import path"", ""part-of"", ""private endpoint""],
         [""import path"", ""part-of"", ""service principal""],
         [""Data Factory"", ""calls"", ""Azure Virtual Desktop""],
         [""Azure Virtual Desktop"", ""part-of"", ""streaming application""],
         [""secure storage account"", ""contains"", ""dataset""],
         [""secure storage account"", ""accessed-by"", ""Azure Machine Learning compute""],
         [""de-identified data"", ""saved-to"", ""export path""],
         [""export path"", ""called-from"", ""Logic App""],
         [""Data Factory instance"", ""calls"", ""public storage account""]
      ]
   }",8.856837529980112
57,mistral:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests""],
         [""Logic Apps"", ""component"", ""Securely routes the HTTP requests and triggers a run""],
         [""Azure Key Vault"", ""component"", ""Stores database credentials""],
         [""Transport Layer Security (TLS)"", ""system"", ""Used by Logic Apps to send data securely""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database and runs the statement""],
         [""SQL Server database"", ""system"", ""Stores data and makes it available to apps""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance""]
      ],
      ""relationships"": [
         [""API Management"", ""part-of"", ""Logic Apps"", ""Securely routes the HTTP requests""],
         [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Retrieves database credentials""],
         [""Logic Apps"", ""calls"", ""Transport Layer Security (TLS)"", ""Sends data securely""],
         [""Logic Apps"", ""part-of"", ""on-premises data gateway"", ""Runs the statement""],
         [""on-premises data gateway"", ""part-of"", ""SQL Server database"", ""Connects to and runs statements""],
         [""Azure Monitor"", ""calls-from"", ""Logic Apps"", ""Collects information on Logic Apps events and performance""]
      ]
   }"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests""],
         [""Logic Apps"", ""component"", ""Securely routes the HTTP requests and triggers a run""],
         [""Azure Key Vault"", ""component"", ""Stores database credentials""],
         [""Transport Layer Security (TLS)"", ""system"", ""Used by Logic Apps to send data securely""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database and runs the statement""],
         [""SQL Server database"", ""system"", ""Stores data and makes it available to apps""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance""]
      ],
      ""relationships"": [
         [""API Management"", ""part-of"", ""Logic Apps"", ""Securely routes the HTTP requests""],
         [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Retrieves database credentials""],
         [""Logic Apps"", ""calls"", ""Transport Layer Security (TLS)"", ""Sends data securely""],
         [""Logic Apps"", ""part-of"", ""on-premises data gateway"", ""Runs the statement""],
         [""on-premises data gateway"", ""part-of"", ""SQL Server database"", ""Connects to and runs statements""],
         [""Azure Monitor"", ""calls-from"", ""Logic Apps"", ""Collects information on Logic Apps events and performance""]
      ]
   }",3.9196299070026726
58,mistral:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway that receives POST requests from outside services and clients""],
         [""Patient API"", ""system"", ""API responsible for handling patient-related operations""],
         [""CreatePatient function"", ""component"", ""function within Patient API that creates a new patient document in Azure Cosmos DB""],
         [""Audit API"", ""system"", ""API responsible for creating audit records""],
         [""CreateAuditRecord function"", ""component"", ""function within Audit API that creates an audit record""],
         [""Azure Cosmos DB"", ""system"", ""NoSQL database service provided by Microsoft Azure""],
         [""outside services and clients"", ""person"", ""external software or systems making requests to the APIs""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""Patient API"", ""APIM sends a request to Patient API with patient information""],
         [""Patient API"", ""part-of"", ""CreatePatient function"", ""The CreatePatient function is a component within the Patient API""],
         [""Patient API"", ""calls"", ""Audit API"", ""The CreatePatient function calls the Audit API's CreateAuditRecord function""],
         [""Audit API"", ""part-of"", ""CreateAuditRecord function"", ""The CreateAuditRecord function is a component within the Audit API""],
         [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""The CreateAuditRecord function creates an audit record in Azure Cosmos DB""],
         [""Patient API"", ""part-of"", ""Azure Cosmos DB"", ""The Patient API creates a patient document in Azure Cosmos DB""]
      ]
   }"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway that receives POST requests from outside services and clients""],
         [""Patient API"", ""system"", ""API responsible for handling patient-related operations""],
         [""CreatePatient function"", ""component"", ""function within Patient API that creates a new patient document in Azure Cosmos DB""],
         [""Audit API"", ""system"", ""API responsible for creating audit records""],
         [""CreateAuditRecord function"", ""component"", ""function within Audit API that creates an audit record""],
         [""Azure Cosmos DB"", ""system"", ""NoSQL database service provided by Microsoft Azure""],
         [""outside services and clients"", ""person"", ""external software or systems making requests to the APIs""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""Patient API"", ""APIM sends a request to Patient API with patient information""],
         [""Patient API"", ""part-of"", ""CreatePatient function"", ""The CreatePatient function is a component within the Patient API""],
         [""Patient API"", ""calls"", ""Audit API"", ""The CreatePatient function calls the Audit API's CreateAuditRecord function""],
         [""Audit API"", ""part-of"", ""CreateAuditRecord function"", ""The CreateAuditRecord function is a component within the Audit API""],
         [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""The CreateAuditRecord function creates an audit record in Azure Cosmos DB""],
         [""Patient API"", ""part-of"", ""Azure Cosmos DB"", ""The Patient API creates a patient document in Azure Cosmos DB""]
      ]
   }",4.233441018965095
59,mistral:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}"," {
      ""entities"": [
         [""CDS"", ""component"", ""Cloud Data Storage (CDS)""],
         [""SQL"", ""component"", ""SQL Database""],
         [""Data Factory"", ""component"", ""Azure Data Factory""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning (ERP) system""],
         [""Azure SQL database"", ""component"", ""Database in Azure""],
         [""Azure Functions"", ""component"", ""Azure Function""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Microsoft Power Apps""],
         [""Power BI"", ""component"", ""Microsoft Power BI""]
      ],
      ""relationships"": [
         [""CDS"", ""contains"", ""Supplier data""],
         [""CDS"", ""part-of"", ""Data Factory""],
         [""Data Factory"", ""uses"", ""API""],
         [""Data Factory"", ""moves"", ""CDS""],
         [""SQL"", ""part-of"", ""Azure SQL database""],
         [""ERP system"", ""stores"", ""PO data""],
         [""ERP system"", ""sends"", ""PO data""],
         [""Azure SQL database"", ""receives"", ""PO data""],
         [""Azure Functions"", ""uses"", ""API""],
         [""Azure Functions"", ""creates"", ""task""],
         [""Power Apps"", ""retrieves"", ""data from Azure SQL Database""],
         [""Power Apps"", ""updates"", ""POs""],
         [""Power Apps"", ""sends"", ""data to CSV export""],
         [""Supplier"", ""receives"", ""data from CSV export""]
      ]
   }"," {
      ""entities"": [
         [""CDS"", ""component"", ""Cloud Data Storage (CDS)""],
         [""SQL"", ""component"", ""SQL Database""],
         [""Data Factory"", ""component"", ""Azure Data Factory""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning (ERP) system""],
         [""Azure SQL database"", ""component"", ""Database in Azure""],
         [""Azure Functions"", ""component"", ""Azure Function""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Microsoft Power Apps""],
         [""Power BI"", ""component"", ""Microsoft Power BI""]
      ],
      ""relationships"": [
         [""CDS"", ""contains"", ""Supplier data""],
         [""CDS"", ""part-of"", ""Data Factory""],
         [""Data Factory"", ""uses"", ""API""],
         [""Data Factory"", ""moves"", ""CDS""],
         [""SQL"", ""part-of"", ""Azure SQL database""],
         [""ERP system"", ""stores"", ""PO data""],
         [""ERP system"", ""sends"", ""PO data""],
         [""Azure SQL database"", ""receives"", ""PO data""],
         [""Azure Functions"", ""uses"", ""API""],
         [""Azure Functions"", ""creates"", ""task""],
         [""Power Apps"", ""retrieves"", ""data from Azure SQL Database""],
         [""Power Apps"", ""updates"", ""POs""],
         [""Power Apps"", ""sends"", ""data to CSV export""],
         [""Supplier"", ""receives"", ""data from CSV export""]
      ]
   }",4.307590743002947
60,mistral:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service for storing audio files.""],
         [""Azure Storage SDK or API"", ""component"", ""A method for uploading audio files to Azure Blob Storage.""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool for managing Azure Storage accounts and their contents.""],
         [""Azure AI Speech"", ""component"", ""An AI service for transcribing audio files asynchronously with speaker diarization enabled.""],
         [""Batch mode transcription"", ""activity"", ""The process of transcribing audio files in bulk using Azure AI Speech.""],
         [""speaker diarization"", ""activity"", ""The process of identifying different speakers in an audio recording.""],
         [""Azure AI Language"", ""component"", ""An AI service for detecting and redacting personal data from transcripts.""],
         [""Ingestion Client tool"", ""tool"", ""A no-code approach for batch mode transcription and personal data detection and redaction using Azure services.""],
         [""Azure OpenAI"", ""component"", ""An AI service for processing transcripts, extracting entities, summarizing conversations, analyzing sentiments, and more.""],
         [""Power BI or custom web application (App Service)"", ""system"", ""A system for visualizing output. Power BI is a business analytics tool set, while App Service is a platform for building cloud-based web applications.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Audio files""],
         [""Azure Storage SDK or API"", ""part-of"", ""Uploading audio files to Azure Blob Storage""],
         [""Azure Storage Explorer"", ""part-of"", ""Managing Azure Storage accounts and their contents""],
         [""Batch mode transcription"", ""calls"", ""Azure AI Speech""],
         [""speaker diarization"", ""part-of"", ""Batch mode transcription""],
         [""Azure AI Language"", ""called-from"", ""Transcripts with personal data""],
         [""Ingestion Client tool"", ""contains"", ""No-code approach for batch mode transcription and personal data detection and redaction""],
         [""Azure OpenAI"", ""calls"", ""Transcripts from Azure Blob Storage""],
         [""Power BI or custom web application (App Service)"", ""called-from"", ""Output from Azure OpenAI""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service for storing audio files.""],
         [""Azure Storage SDK or API"", ""component"", ""A method for uploading audio files to Azure Blob Storage.""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool for managing Azure Storage accounts and their contents.""],
         [""Azure AI Speech"", ""component"", ""An AI service for transcribing audio files asynchronously with speaker diarization enabled.""],
         [""Batch mode transcription"", ""activity"", ""The process of transcribing audio files in bulk using Azure AI Speech.""],
         [""speaker diarization"", ""activity"", ""The process of identifying different speakers in an audio recording.""],
         [""Azure AI Language"", ""component"", ""An AI service for detecting and redacting personal data from transcripts.""],
         [""Ingestion Client tool"", ""tool"", ""A no-code approach for batch mode transcription and personal data detection and redaction using Azure services.""],
         [""Azure OpenAI"", ""component"", ""An AI service for processing transcripts, extracting entities, summarizing conversations, analyzing sentiments, and more.""],
         [""Power BI or custom web application (App Service)"", ""system"", ""A system for visualizing output. Power BI is a business analytics tool set, while App Service is a platform for building cloud-based web applications.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Audio files""],
         [""Azure Storage SDK or API"", ""part-of"", ""Uploading audio files to Azure Blob Storage""],
         [""Azure Storage Explorer"", ""part-of"", ""Managing Azure Storage accounts and their contents""],
         [""Batch mode transcription"", ""calls"", ""Azure AI Speech""],
         [""speaker diarization"", ""part-of"", ""Batch mode transcription""],
         [""Azure AI Language"", ""called-from"", ""Transcripts with personal data""],
         [""Ingestion Client tool"", ""contains"", ""No-code approach for batch mode transcription and personal data detection and redaction""],
         [""Azure OpenAI"", ""calls"", ""Transcripts from Azure Blob Storage""],
         [""Power BI or custom web application (App Service)"", ""called-from"", ""Output from Azure OpenAI""]
      ]
   }",5.981654791045003
61,mistral:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway that receives POST requests from outside services and clients""],
         [""CreatePatient"", ""component"", ""Function in Patient API that creates a patient document in Azure Cosmos DB""],
         [""Patient API"", ""system"", ""API responsible for handling patient-related operations""],
         [""Audit API CreateAuditRecord function"", ""component"", ""Function within the Audit API application that creates an audit record""],
         [""CreateAuditRecord"", ""component"", ""Function in the Audit API that creates an audit record in Azure Cosmos DB""],
         [""Azure Cosmos DB"", ""system"", ""Database service used for storing patient and audit records""],
         [""outside services and clients"", ""person"", ""External software or systems making POST requests to APIM""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient"", ""Initiates a connection to the CreatePatient function in Patient API with given patient information""],
         [""CreatePatient"", ""calls"", ""CreateAuditRecord"", ""Calls the CreateAuditRecord function in the Audit API application to create an audit record""],
         [""CreateAuditRecord"", ""part-of"", ""Audit API CreateAuditRecord function"", ""Is a part of the Audit API function that creates an audit record""],
         [""Audit API CreateAuditRecord function"", ""contains"", ""Create record in Azure Cosmos DB"", ""Creates the audit record in Azure Cosmos DB""],
         [""CreatePatient"", ""contains"", ""patient document in Azure Cosmos DB"", ""Creates the patient document in Azure Cosmos DB""]
      ]
   }"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway that receives POST requests from outside services and clients""],
         [""CreatePatient"", ""component"", ""Function in Patient API that creates a patient document in Azure Cosmos DB""],
         [""Patient API"", ""system"", ""API responsible for handling patient-related operations""],
         [""Audit API CreateAuditRecord function"", ""component"", ""Function within the Audit API application that creates an audit record""],
         [""CreateAuditRecord"", ""component"", ""Function in the Audit API that creates an audit record in Azure Cosmos DB""],
         [""Azure Cosmos DB"", ""system"", ""Database service used for storing patient and audit records""],
         [""outside services and clients"", ""person"", ""External software or systems making POST requests to APIM""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient"", ""Initiates a connection to the CreatePatient function in Patient API with given patient information""],
         [""CreatePatient"", ""calls"", ""CreateAuditRecord"", ""Calls the CreateAuditRecord function in the Audit API application to create an audit record""],
         [""CreateAuditRecord"", ""part-of"", ""Audit API CreateAuditRecord function"", ""Is a part of the Audit API function that creates an audit record""],
         [""Audit API CreateAuditRecord function"", ""contains"", ""Create record in Azure Cosmos DB"", ""Creates the audit record in Azure Cosmos DB""],
         [""CreatePatient"", ""contains"", ""patient document in Azure Cosmos DB"", ""Creates the patient document in Azure Cosmos DB""]
      ]
   }",4.33463976503117
62,mistral:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}"," {
      ""entities"": [
         [""CDS"", ""component"", ""Cloud Data Storage""],
         [""SQL"", ""component"", ""Structured Query Language Database""],
         [""Data Factory"", ""component"", ""ETL Tool""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning System""],
         [""Azure SQL database"", ""component"", ""Azure's structured query language database service""],
         [""Azure Functions"", ""component"", ""Azure serverless compute service""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Microsoft low-code application platform""],
         [""Power BI"", ""component"", ""Microsoft business analytics service""]
      ],
      ""relationships"": [
         [""CDS"", ""contains"", ""SQL""],
         [""Data Factory"", ""calls"", ""SQL""],
         [""ERP system"", ""calls"", ""Azure SQL database""],
         [""Azure Functions"", ""calls"", ""API""],
         [""Power Apps"", ""calls"", ""Azure SQL Database""],
         [""Power Apps"", ""called-from"", ""User""],
         [""Power Apps"", ""contains"", ""CSV export""],
         [""Power BI"", ""calls"", ""Power Apps""]
      ]
   }"," {
      ""entities"": [
         [""CDS"", ""component"", ""Cloud Data Storage""],
         [""SQL"", ""component"", ""Structured Query Language Database""],
         [""Data Factory"", ""component"", ""ETL Tool""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning System""],
         [""Azure SQL database"", ""component"", ""Azure's structured query language database service""],
         [""Azure Functions"", ""component"", ""Azure serverless compute service""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Microsoft low-code application platform""],
         [""Power BI"", ""component"", ""Microsoft business analytics service""]
      ],
      ""relationships"": [
         [""CDS"", ""contains"", ""SQL""],
         [""Data Factory"", ""calls"", ""SQL""],
         [""ERP system"", ""calls"", ""Azure SQL database""],
         [""Azure Functions"", ""calls"", ""API""],
         [""Power Apps"", ""calls"", ""Azure SQL Database""],
         [""Power Apps"", ""called-from"", ""User""],
         [""Power Apps"", ""contains"", ""CSV export""],
         [""Power BI"", ""calls"", ""Power Apps""]
      ]
   }",3.379546965006739
63,mistral:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""Backend system that is triggered by HTTP requests, allows for nesting workflows, and integrates with commonly used services through connectors.""],
         [""Azure API Management"", ""system"", ""Backend system consisting of two components: API gateway and developer portal.""],
         [""API gateway"", ""component"", ""Component of Azure API Management that accepts HTTP calls and routes them to the backend.""],
         [""Developer portal"", ""component"", ""Component of Azure API Management that provides access to documentation, code samples for calling APIs, and allows testing of APIs.""],
         [""Azure DNS"", ""system"", ""Backend system that provides name resolution using the Azure infrastructure.""],
         [""Microsoft Entra ID"", ""system"", ""System used for authenticating clients that call the API gateway using the OpenID Connect (OIDC) protocol.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""part-of"", ""Backend systems""],
         [""Azure API Management"", ""part-of"", ""Backend systems""],
         [""API gateway"", ""part-of"", ""Azure API Management""],
         [""Developer portal"", ""part-of"", ""Azure API Management""],
         [""Azure DNS"", ""part-of"", ""Backend systems""],
         [""Microsoft Entra ID"", ""called-from"", ""Clients that call the API gateway""],
         [""API Gateway"", ""calls"", ""Microsoft Entra ID""]
      ]
   }"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""Backend system that is triggered by HTTP requests, allows for nesting workflows, and integrates with commonly used services through connectors.""],
         [""Azure API Management"", ""system"", ""Backend system consisting of two components: API gateway and developer portal.""],
         [""API gateway"", ""component"", ""Component of Azure API Management that accepts HTTP calls and routes them to the backend.""],
         [""Developer portal"", ""component"", ""Component of Azure API Management that provides access to documentation, code samples for calling APIs, and allows testing of APIs.""],
         [""Azure DNS"", ""system"", ""Backend system that provides name resolution using the Azure infrastructure.""],
         [""Microsoft Entra ID"", ""system"", ""System used for authenticating clients that call the API gateway using the OpenID Connect (OIDC) protocol.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""part-of"", ""Backend systems""],
         [""Azure API Management"", ""part-of"", ""Backend systems""],
         [""API gateway"", ""part-of"", ""Azure API Management""],
         [""Developer portal"", ""part-of"", ""Azure API Management""],
         [""Azure DNS"", ""part-of"", ""Backend systems""],
         [""Microsoft Entra ID"", ""called-from"", ""Clients that call the API gateway""],
         [""API Gateway"", ""calls"", ""Microsoft Entra ID""]
      ]
   }",3.923728902009316
64,mistral:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A cloud storage service offered by Microsoft Azure where the uploaded document file is stored.""],
         [""web app"", ""person"", ""The application that initiates the process by uploading the document and adding a command message to a storage queue.""],
         [""storage queue"", ""component"", ""A component in Azure that holds messages sent by applications or services to trigger workflows.""],
         [""Durable Functions orchestration"", ""component"", ""A serverless, stateful, event-driven programming model for building scalable and resilient distributed applications on Azure.""],
         [""Command message"", ""component"", ""The message sent to the storage queue to initiate pipeline processing.""],
         [""Scan activity function"", ""component"", ""A function that calls the Computer Vision Read API, passing in the location in storage of the document to be processed for OCR processing.""],
         [""Computer Vision Read API"", ""component"", ""An API provided by Microsoft Azure for Optical Character Recognition (OCR) processing.""],
         [""Optical character recognition (OCR)"", ""system"", ""A technology that enables machines to recognize and extract text from images or scanned documents.""],
         [""Classify activity function"", ""component"", ""A function that calls the document classifier service hosted in an Azure Kubernetes Service (AKS) cluster.""],
         [""document classifier service"", ""component"", ""A service that identifies the starting page of each known document and calculates the number of document types contained in the document file using regular expression pattern matching.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""A managed container orchestration service provided by Microsoft Azure for deploying, scaling, and managing containerized applications.""],
         [""Azure Cosmos DB"", ""system"", ""A cloud-based, NoSQL database service offered by Microsoft Azure used to store metadata about the documents.""],
         [""Metadata Store activity function"", ""component"", ""A function that saves the document type and page range information in an Azure Cosmos DB store.""],
         [""Cognitive Search service"", ""system"", ""A search service provided by Microsoft Azure for building intelligent search solutions.""],
         [""Azure AI Search libraries for .NET"", ""component"", ""Libraries used to create a new search document in the Cognitive Search service.""],
         [""correlation ID"", ""entity"", ""A unique identifier added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.""]
      ],
      ""relationships"": [
         [""web app"", ""part-of"", ""Azure Blob Storage""],
         [""storage queue"", ""part-of"", ""Durable Functions orchestration""],
         [""Command message"", ""part-of"", ""storage queue""],
         [""Scan activity function"", ""calls"", ""Computer Vision Read API""],
         [""Classify activity function"", ""part-of"", ""Durable Functions orchestration""],
         [""Classify activity function"", ""calls"", ""document classifier service""],
         [""Metadata Store activity function"", ""contains"", ""Azure Cosmos DB""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A cloud storage service offered by Microsoft Azure where the uploaded document file is stored.""],
         [""web app"", ""person"", ""The application that initiates the process by uploading the document and adding a command message to a storage queue.""],
         [""storage queue"", ""component"", ""A component in Azure that holds messages sent by applications or services to trigger workflows.""],
         [""Durable Functions orchestration"", ""component"", ""A serverless, stateful, event-driven programming model for building scalable and resilient distributed applications on Azure.""],
         [""Command message"", ""component"", ""The message sent to the storage queue to initiate pipeline processing.""],
         [""Scan activity function"", ""component"", ""A function that calls the Computer Vision Read API, passing in the location in storage of the document to be processed for OCR processing.""],
         [""Computer Vision Read API"", ""component"", ""An API provided by Microsoft Azure for Optical Character Recognition (OCR) processing.""],
         [""Optical character recognition (OCR)"", ""system"", ""A technology that enables machines to recognize and extract text from images or scanned documents.""],
         [""Classify activity function"", ""component"", ""A function that calls the document classifier service hosted in an Azure Kubernetes Service (AKS) cluster.""],
         [""document classifier service"", ""component"", ""A service that identifies the starting page of each known document and calculates the number of document types contained in the document file using regular expression pattern matching.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""A managed container orchestration service provided by Microsoft Azure for deploying, scaling, and managing containerized applications.""],
         [""Azure Cosmos DB"", ""system"", ""A cloud-based, NoSQL database service offered by Microsoft Azure used to store metadata about the documents.""],
         [""Metadata Store activity function"", ""component"", ""A function that saves the document type and page range information in an Azure Cosmos DB store.""],
         [""Cognitive Search service"", ""system"", ""A search service provided by Microsoft Azure for building intelligent search solutions.""],
         [""Azure AI Search libraries for .NET"", ""component"", ""Libraries used to create a new search document in the Cognitive Search service.""],
         [""correlation ID"", ""entity"", ""A unique identifier added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.""]
      ],
      ""relationships"": [
         [""web app"", ""part-of"", ""Azure Blob Storage""],
         [""storage queue"", ""part-of"", ""Durable Functions orchestration""],
         [""Command message"", ""part-of"", ""storage queue""],
         [""Scan activity function"", ""calls"", ""Computer Vision Read API""],
         [""Classify activity function"", ""part-of"", ""Durable Functions orchestration""],
         [""Classify activity function"", ""calls"", ""document classifier service""],
         [""Metadata Store activity function"", ""contains"", ""Azure Cosmos DB""]
      ]
   }",7.673501713026781
65,mistral:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Individuals who upload datasets""],
         [""Datasets"", ""component"", ""Data being uploaded by Data Owners""],
         [""Blob Storage Account"", ""system"", ""Public storage account used for data uploads""],
         [""Microsoft-managed keys"", ""component"", ""Keys used for encrypting the datasets""],
         [""Azure Data Factory"", ""component"", ""Tool that starts copying of the dataset""],
         [""Trigger"", ""component"", ""Mechanism that initiates the action in Azure Data Factory""],
         [""Import Path"", ""component"", ""Specific location on another storage account where data is copied to""],
         [""Private Endpoint"", ""component"", ""Endpoint through which the storage account can be reached""],
         [""Service Principal"", ""component"", ""Principal used for limited permissions""],
         [""Azure Virtual Desktop"", ""component"", ""Platform used as a privileged jump box""],
         [""Streaming Application"", ""component"", ""Application used by researchers to access secure environment""],
         [""Azure Data Science VMs"", ""component"", ""Virtual Machines provisioned in a secure network environment for research work""],
         [""Secure Network Environment"", ""system"", ""Environment where data science VMs and Azure Machine Learning Compute are located""],
         [""Azure Machine Learning Compute"", ""component"", ""Compute used to train, deploy, automate, and manage machine learning models""],
         [""Export Path"", ""component"", ""Location on the secure storage account for saving approved de-identified data""],
         [""Logic App"", ""component"", ""App that starts the manual approval process""],
         [""Manual Reviewers"", ""person"", ""Individuals who ensure sensitive data isn't exported""],
         [""De-identified Data"", ""component"", ""Data without personal information after being approved for export""],
         [""Azure Machine Learning Capabilities"", ""component"", ""Capabilities provided by Azure Machine Learning Compute""],
         [""Models"", ""component"", ""Outputs created from the de-identified data""],
         [""Public Storage Account"", ""system"", ""Storage account where approved models and data are stored for external access""],
         [""Lower Security Environment"", ""system"", ""Alternate storage account with lower security level""]
      ],
      ""relationships"": [
         [""Data Owners"", ""calls"", ""Blob Storage Account""],
         [""Blob Storage Account"", ""contains"", ""Datasets""],
         [""Blob Storage Account"", ""part-of"", ""Microsoft-managed keys""],
         [""Azure Data Factory"", ""part-of"", ""Trigger""],
         [""Trigger"", ""calls"", ""Import Path""],
         [""Import Path"", ""part-of"", ""Another storage account""],
         [""Another storage account"", ""reached through"", ""Private Endpoint""],
         [""Another storage account"", ""has permissions for"", ""Service Principal""],
         [""Azure Data Factory"", ""deletes"", ""Datasets""],
         [""Streaming Application"", ""accesses"", ""Azure Virtual Desktop""],
         [""Azure Virtual Desktop"", ""provides access to"", ""Secure Network Environment""],
         [""Secure Network Environment"", ""contains"", ""Azure Data Science VMs""],
         [""Secure Network Environment"", ""contains"", ""Azure Machine Learning Compute""],
         [""Export Path"", ""contained in"", ""Secure Storage Account""],
         [""Logic App"", ""requests review from"", ""Manual Reviewers""],
         [""Manual Reviewers"", ""approves or denies"", ""De-identified Data""],
         [""De-identified Data"", ""saved to"", ""Export Path""],
         [""De-identified Data"", ""called-from"", ""Azure Machine Learning Capabilities""],
         [""Approved De-identified Data"", ""called-from"", ""Data Factory instance""],
         [""Data Factory instance"", ""moves data to"", ""Public Storage Account""],
         [""Public Storage Account"", ""contains"", ""Models or de-identified data""]
      ]
   }"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Individuals who upload datasets""],
         [""Datasets"", ""component"", ""Data being uploaded by Data Owners""],
         [""Blob Storage Account"", ""system"", ""Public storage account used for data uploads""],
         [""Microsoft-managed keys"", ""component"", ""Keys used for encrypting the datasets""],
         [""Azure Data Factory"", ""component"", ""Tool that starts copying of the dataset""],
         [""Trigger"", ""component"", ""Mechanism that initiates the action in Azure Data Factory""],
         [""Import Path"", ""component"", ""Specific location on another storage account where data is copied to""],
         [""Private Endpoint"", ""component"", ""Endpoint through which the storage account can be reached""],
         [""Service Principal"", ""component"", ""Principal used for limited permissions""],
         [""Azure Virtual Desktop"", ""component"", ""Platform used as a privileged jump box""],
         [""Streaming Application"", ""component"", ""Application used by researchers to access secure environment""],
         [""Azure Data Science VMs"", ""component"", ""Virtual Machines provisioned in a secure network environment for research work""],
         [""Secure Network Environment"", ""system"", ""Environment where data science VMs and Azure Machine Learning Compute are located""],
         [""Azure Machine Learning Compute"", ""component"", ""Compute used to train, deploy, automate, and manage machine learning models""],
         [""Export Path"", ""component"", ""Location on the secure storage account for saving approved de-identified data""],
         [""Logic App"", ""component"", ""App that starts the manual approval process""],
         [""Manual Reviewers"", ""person"", ""Individuals who ensure sensitive data isn't exported""],
         [""De-identified Data"", ""component"", ""Data without personal information after being approved for export""],
         [""Azure Machine Learning Capabilities"", ""component"", ""Capabilities provided by Azure Machine Learning Compute""],
         [""Models"", ""component"", ""Outputs created from the de-identified data""],
         [""Public Storage Account"", ""system"", ""Storage account where approved models and data are stored for external access""],
         [""Lower Security Environment"", ""system"", ""Alternate storage account with lower security level""]
      ],
      ""relationships"": [
         [""Data Owners"", ""calls"", ""Blob Storage Account""],
         [""Blob Storage Account"", ""contains"", ""Datasets""],
         [""Blob Storage Account"", ""part-of"", ""Microsoft-managed keys""],
         [""Azure Data Factory"", ""part-of"", ""Trigger""],
         [""Trigger"", ""calls"", ""Import Path""],
         [""Import Path"", ""part-of"", ""Another storage account""],
         [""Another storage account"", ""reached through"", ""Private Endpoint""],
         [""Another storage account"", ""has permissions for"", ""Service Principal""],
         [""Azure Data Factory"", ""deletes"", ""Datasets""],
         [""Streaming Application"", ""accesses"", ""Azure Virtual Desktop""],
         [""Azure Virtual Desktop"", ""provides access to"", ""Secure Network Environment""],
         [""Secure Network Environment"", ""contains"", ""Azure Data Science VMs""],
         [""Secure Network Environment"", ""contains"", ""Azure Machine Learning Compute""],
         [""Export Path"", ""contained in"", ""Secure Storage Account""],
         [""Logic App"", ""requests review from"", ""Manual Reviewers""],
         [""Manual Reviewers"", ""approves or denies"", ""De-identified Data""],
         [""De-identified Data"", ""saved to"", ""Export Path""],
         [""De-identified Data"", ""called-from"", ""Azure Machine Learning Capabilities""],
         [""Approved De-identified Data"", ""called-from"", ""Data Factory instance""],
         [""Data Factory instance"", ""moves data to"", ""Public Storage Account""],
         [""Public Storage Account"", ""contains"", ""Models or de-identified data""]
      ]
   }",9.582761399971787
66,mistral:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""A reverse proxy service that handles incoming client requests and forwards them to another service.""],
         [""Azure AD B2C"", ""component"", ""An identity provider (IdP) service that authenticates users by validating their credentials.""],
         [""Custom Domain"", ""component"", ""A domain name used for sign-in in Azure AD B2C.""],
         [""API Management"", ""system"", ""A service provided by Microsoft Azure that provides a centralized management and security layer for APIs.""],
         [""App Service Environment"", ""component"", ""An Azure App Service environment optimized for performance, isolation, and network connectivity.""],
         [""Function Apps"", ""component"", ""A serverless compute solution in Azure that runs functions in response to events.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""A managed container orchestration service in Azure based on Kubernetes.""],
         [""Backend API"", ""component"", ""An application programming interface (API) that provides access to functionality offered by a software application.""],
         [""JSON Web Token (JWT)"", ""component"", ""A compact, self-contained data structure used to pass claims securely between parties as a JSON object.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management"", ""The Azure Front Door service is part of the API Management system.""],
         [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""The Azure Front Door component initiates a connection to Azure AD B2C for authentication.""],
         [""Azure AD B2C"", ""called-from"", ""API Management"", ""The API Management component calls Azure AD B2C to authenticate users.""],
         [""API Management"", ""part-of"", ""Backend API"", ""API Management intercepts and forwards requests from the client to the appropriate Backend API.""],
         [""Backend API"", ""part-of"", ""App Service Environment"", ""[If applicable] The Backend API component is running in an App Service Environment.""],
         [""Backend API"", ""part-of"", ""Function Apps"", ""[If applicable] The Backend API component is running as a Function App.""],
         [""Backend API"", ""part-of"", ""Azure Kubernetes Services (AKS)"", ""[If applicable] The Backend API component is running in an Azure Kubernetes Service.""]
      ]
   }"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""A reverse proxy service that handles incoming client requests and forwards them to another service.""],
         [""Azure AD B2C"", ""component"", ""An identity provider (IdP) service that authenticates users by validating their credentials.""],
         [""Custom Domain"", ""component"", ""A domain name used for sign-in in Azure AD B2C.""],
         [""API Management"", ""system"", ""A service provided by Microsoft Azure that provides a centralized management and security layer for APIs.""],
         [""App Service Environment"", ""component"", ""An Azure App Service environment optimized for performance, isolation, and network connectivity.""],
         [""Function Apps"", ""component"", ""A serverless compute solution in Azure that runs functions in response to events.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""A managed container orchestration service in Azure based on Kubernetes.""],
         [""Backend API"", ""component"", ""An application programming interface (API) that provides access to functionality offered by a software application.""],
         [""JSON Web Token (JWT)"", ""component"", ""A compact, self-contained data structure used to pass claims securely between parties as a JSON object.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management"", ""The Azure Front Door service is part of the API Management system.""],
         [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""The Azure Front Door component initiates a connection to Azure AD B2C for authentication.""],
         [""Azure AD B2C"", ""called-from"", ""API Management"", ""The API Management component calls Azure AD B2C to authenticate users.""],
         [""API Management"", ""part-of"", ""Backend API"", ""API Management intercepts and forwards requests from the client to the appropriate Backend API.""],
         [""Backend API"", ""part-of"", ""App Service Environment"", ""[If applicable] The Backend API component is running in an App Service Environment.""],
         [""Backend API"", ""part-of"", ""Function Apps"", ""[If applicable] The Backend API component is running as a Function App.""],
         [""Backend API"", ""part-of"", ""Azure Kubernetes Services (AKS)"", ""[If applicable] The Backend API component is running in an Azure Kubernetes Service.""]
      ]
   }",5.993419767008163
67,mistral:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}"," {
      ""entities"": [
         [""Connect Agent Component"", ""component"", ""Captures change logs using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""Publisher Component (Mainframe)"", ""component"", ""Manages data migration for mainframe systems.""],
         [""Listener Component (Midrange)"", ""component"", ""Manages data migration for midrange systems. It's located on either a Windows or Linux machine.""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target.""],
         [""Database (Target)"", ""system"", ""Receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""system"", ""Consumes the changed data and applies it to Azure data platform services.""]
      ],
      ""relationships"": [
         [""Connect Agent Component"", ""calls"", ""Connect Controller Daemon"", ""The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""],
         [""Publisher Component (Mainframe)"", ""part-of"", ""Azure"", ""The publisher moves the data from on-premises to Azure via an enhanced-security connection.""],
         [""Listener Component (Midrange)"", ""part-of"", ""Azure"", ""The listener moves the data from on-premises to Azure via an enhanced-security connection.""],
         [""Publisher Component (Mainframe)"", ""contains"", ""Data Migration"", ""The publisher component handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.""],
         [""Listener Component (Midrange)"", ""contains"", ""Data Migration"", ""The listener component handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.""],
         [""Connect Replicator Engine"", ""contains"", ""Parallel Processing"", ""The Connect Replicator Engine distributes data for parallel processing.""],
         [""Database (Target)"", ""part-of"", ""Azure Data Platform Services"", ""The database receives the changes via ODBC or ingests the changes via Azure Event Hubs.""]
      ]
   }"," {
      ""entities"": [
         [""Connect Agent Component"", ""component"", ""Captures change logs using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""Publisher Component (Mainframe)"", ""component"", ""Manages data migration for mainframe systems.""],
         [""Listener Component (Midrange)"", ""component"", ""Manages data migration for midrange systems. It's located on either a Windows or Linux machine.""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target.""],
         [""Database (Target)"", ""system"", ""Receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""system"", ""Consumes the changed data and applies it to Azure data platform services.""]
      ],
      ""relationships"": [
         [""Connect Agent Component"", ""calls"", ""Connect Controller Daemon"", ""The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""],
         [""Publisher Component (Mainframe)"", ""part-of"", ""Azure"", ""The publisher moves the data from on-premises to Azure via an enhanced-security connection.""],
         [""Listener Component (Midrange)"", ""part-of"", ""Azure"", ""The listener moves the data from on-premises to Azure via an enhanced-security connection.""],
         [""Publisher Component (Mainframe)"", ""contains"", ""Data Migration"", ""The publisher component handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.""],
         [""Listener Component (Midrange)"", ""contains"", ""Data Migration"", ""The listener component handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.""],
         [""Connect Replicator Engine"", ""contains"", ""Parallel Processing"", ""The Connect Replicator Engine distributes data for parallel processing.""],
         [""Database (Target)"", ""part-of"", ""Azure Data Platform Services"", ""The database receives the changes via ODBC or ingests the changes via Azure Event Hubs.""]
      ]
   }",5.5200231250491925
68,mistral:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A platform for version control and collaboration.""],
         [""CI/CD pipeline"", ""component"", ""A series of steps defined to automate the software delivery process.""],
         [""Azure VM"", ""component"", ""A virtual machine provided by Microsoft Azure.""],
         [""API application"", ""component"", ""An application that provides access to a set of functions or data over a network using the API (Application Programming Interface) protocol.""],
         [""ILB ASE"", ""component"", ""Internal Load Balancer in an Azure App Service Environment.""],
         [""Azure API Management"", ""component"", ""A cloud-based solution for managing and securing APIs in Azure.""],
         [""HOST headers"", ""component"", ""Additional request information sent by the client to the server, usually used for routing requests.""],
         [""API Management policy"", ""component"", ""The rules that define how an API behaves.""],
         [""App Service Environment's DNS name"", ""component"", ""The domain name associated with the App Service Environment.""],
         [""Application Gateway"", ""component"", ""A load balancer in Azure that distributes incoming traffic to multiple backend servers.""],
         [""Azure Private DNS"", ""component"", ""A service for managing private domain names within an Azure environment.""],
         [""ASE"", ""abbreviation"", ""Stands for App Service Environment.""],
         [""API Management's developer and API portal"", ""component"", ""The user interface for developers to interact with the APIs provided by Azure API Management.""],
         [""Application Gateway's public IP"", ""component"", ""The public IP address associated with the Application Gateway.""],
         [""external users"", ""person"", ""Users that are not part of the internal network and access the APIs via the exposed developer portal.""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline""],
         [""CI/CD pipeline"", ""part-of"", ""Azure VM""],
         [""Azure VM"", ""contains"", ""API application""],
         [""API application"", ""called-from"", ""Azure API Management""],
         [""Azure API Management"", ""uses"", ""HOST headers""],
         [""Azure API Management"", ""part-of"", ""App Service Environment's DNS name""],
         [""App Service Environment's DNS name"", ""exposed-by"", ""Application Gateway""],
         [""Application Gateway"", ""calls"", ""API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", ""traffic between ASE, API Management, and Application Gateway""],
         [""external users"", ""consumes"", ""APIs via Application Gateway's public IP""]
      ]
   }"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A platform for version control and collaboration.""],
         [""CI/CD pipeline"", ""component"", ""A series of steps defined to automate the software delivery process.""],
         [""Azure VM"", ""component"", ""A virtual machine provided by Microsoft Azure.""],
         [""API application"", ""component"", ""An application that provides access to a set of functions or data over a network using the API (Application Programming Interface) protocol.""],
         [""ILB ASE"", ""component"", ""Internal Load Balancer in an Azure App Service Environment.""],
         [""Azure API Management"", ""component"", ""A cloud-based solution for managing and securing APIs in Azure.""],
         [""HOST headers"", ""component"", ""Additional request information sent by the client to the server, usually used for routing requests.""],
         [""API Management policy"", ""component"", ""The rules that define how an API behaves.""],
         [""App Service Environment's DNS name"", ""component"", ""The domain name associated with the App Service Environment.""],
         [""Application Gateway"", ""component"", ""A load balancer in Azure that distributes incoming traffic to multiple backend servers.""],
         [""Azure Private DNS"", ""component"", ""A service for managing private domain names within an Azure environment.""],
         [""ASE"", ""abbreviation"", ""Stands for App Service Environment.""],
         [""API Management's developer and API portal"", ""component"", ""The user interface for developers to interact with the APIs provided by Azure API Management.""],
         [""Application Gateway's public IP"", ""component"", ""The public IP address associated with the Application Gateway.""],
         [""external users"", ""person"", ""Users that are not part of the internal network and access the APIs via the exposed developer portal.""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline""],
         [""CI/CD pipeline"", ""part-of"", ""Azure VM""],
         [""Azure VM"", ""contains"", ""API application""],
         [""API application"", ""called-from"", ""Azure API Management""],
         [""Azure API Management"", ""uses"", ""HOST headers""],
         [""Azure API Management"", ""part-of"", ""App Service Environment's DNS name""],
         [""App Service Environment's DNS name"", ""exposed-by"", ""Application Gateway""],
         [""Application Gateway"", ""calls"", ""API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", ""traffic between ASE, API Management, and Application Gateway""],
         [""external users"", ""consumes"", ""APIs via Application Gateway's public IP""]
      ]
   }",6.6748422760283574
69,mistral:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}"," {
      ""entities"": [
        [""Outlook Email Account"", ""person"", ""A designated Outlook email account""],
        [""Logic App"", ""component"", ""The logic app that processes emails received in the designated Outlook email account and uploads PDF files to Azure Data Lake Storage""],
        [""Azure Logic Apps"", ""system"", ""The framework used to build the logic app""],
        [""PDF Container"", ""component"", ""A container in Azure Data Lake Storage where PDF files are stored""],
        [""Function App"", ""component"", ""The function app that processes single-page PDF files, sends them for processing using Azure REST API and stores results in a data structure""],
        [""Azure Functions"", ""system"", ""The framework used to build the function app""],
        [""HTTPS POST"", ""component"", ""An HTTP method used by the function app to send the location of single-page PDF files for processing using Azure REST API""],
        [""AI Document Intelligence"", ""component"", ""Azure AI service that processes PDF files""],
        [""Data Structure"", ""component"", ""A data structure where the function app stores the processed information""],
        [""Third Container in Data Lake Storage"", ""component"", ""A container in Azure Data Lake Storage where the JSON data file containing the response data is stored""],
        [""Azure Cosmos DB"", ""system"", ""The database that saves the processed data""],
        [""Power BI"", ""component"", ""The tool that obtains data from Azure Cosmos DB and provides insights and dashboards""]
      ],
      ""relationships"": [
        [""Outlook Email Account"", ""calls"", ""Logic App"", ""The arrival of an email in the Outlook account triggers the logic app""],
        [""Logic App"", ""part-of"", ""Azure Logic Apps""],
        [""PDF Container"", ""part-of"", ""Azure Data Lake Storage""],
        [""Function App"", ""part-of"", ""Azure Functions""],
        [""PDF Container"", ""calls"", ""Second Logic App""],
        [""Function App"", ""part-of"", ""Data Structure""],
        [""Function App"", ""calls"", ""AI Document Intelligence""],
        [""Function App"", ""called-from"", ""AI Document Intelligence""],
        [""Function App"", ""part-of"", ""Third Container in Data Lake Storage""],
        [""Forms Processing Logic App"", ""called-from"", ""Function App""],
        [""Forms Processing Logic App"", ""part-of"", ""Azure Cosmos DB""],
        [""Power BI"", ""calls"", ""Azure Cosmos DB""]
      ]
    }"," {
      ""entities"": [
        [""Outlook Email Account"", ""person"", ""A designated Outlook email account""],
        [""Logic App"", ""component"", ""The logic app that processes emails received in the designated Outlook email account and uploads PDF files to Azure Data Lake Storage""],
        [""Azure Logic Apps"", ""system"", ""The framework used to build the logic app""],
        [""PDF Container"", ""component"", ""A container in Azure Data Lake Storage where PDF files are stored""],
        [""Function App"", ""component"", ""The function app that processes single-page PDF files, sends them for processing using Azure REST API and stores results in a data structure""],
        [""Azure Functions"", ""system"", ""The framework used to build the function app""],
        [""HTTPS POST"", ""component"", ""An HTTP method used by the function app to send the location of single-page PDF files for processing using Azure REST API""],
        [""AI Document Intelligence"", ""component"", ""Azure AI service that processes PDF files""],
        [""Data Structure"", ""component"", ""A data structure where the function app stores the processed information""],
        [""Third Container in Data Lake Storage"", ""component"", ""A container in Azure Data Lake Storage where the JSON data file containing the response data is stored""],
        [""Azure Cosmos DB"", ""system"", ""The database that saves the processed data""],
        [""Power BI"", ""component"", ""The tool that obtains data from Azure Cosmos DB and provides insights and dashboards""]
      ],
      ""relationships"": [
        [""Outlook Email Account"", ""calls"", ""Logic App"", ""The arrival of an email in the Outlook account triggers the logic app""],
        [""Logic App"", ""part-of"", ""Azure Logic Apps""],
        [""PDF Container"", ""part-of"", ""Azure Data Lake Storage""],
        [""Function App"", ""part-of"", ""Azure Functions""],
        [""PDF Container"", ""calls"", ""Second Logic App""],
        [""Function App"", ""part-of"", ""Data Structure""],
        [""Function App"", ""calls"", ""AI Document Intelligence""],
        [""Function App"", ""called-from"", ""AI Document Intelligence""],
        [""Function App"", ""part-of"", ""Third Container in Data Lake Storage""],
        [""Forms Processing Logic App"", ""called-from"", ""Function App""],
        [""Forms Processing Logic App"", ""part-of"", ""Azure Cosmos DB""],
        [""Power BI"", ""calls"", ""Azure Cosmos DB""]
      ]
    }",6.366901177971158
70,mistral:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}"," {
      ""entities"": [
         [""Airline System"", ""system"", ""The entity that communicates with a custom API hosted in Azure API Management.""],
         [""Custom API Hosted in Azure API Management"", ""component"", ""The component that the Airline System communicates with.""],
         [""Custom API Coordinator"", ""component"", ""The component that receives notifications and handles incoming messages from the Airline System, sends them to Power Apps.""],
         [""Power Apps"", ""system"", ""The system where flights are assigned to Microsoft Teams channels.""],
         [""Microsoft Teams Channels"", ""component"", ""The component where flights are assigned.""],
         [""Azure Storage Account queue"", ""component"", ""The component where Graph API calls are queued for further processing.""],
         [""Azure Functions"", ""component"", ""The component that runs the Graph API calls based on the incoming messages in the storage queue.""],
         [""Graph API calls"", ""component"", ""The component that Azure Functions runs.""],
         [""Azure Event Hubs"", ""system"", ""The system where all events are streamed for further analytics.""],
         [""Custom Bot Messaging Service"", ""component"", ""The component that manages the airline's notification system.""],
         [""Azure Bot Service"", ""component"", ""The service employed by the Custom Bot Messaging Service.""],
         [""Custom Bots"", ""component"", ""The components that send flight updates to users in Teams.""],
         [""Teams Notifications"", ""component"", ""The notifications sent by custom bots.""],
         [""Azure Data Lake storage"", ""system"", ""The system that offers long-term retention and micro-batch processing of events from Event Hubs.""],
         [""Power BI"", ""system"", ""The system that generates insightful reports with data from Azure Data Lake storage.""]
      ],
      ""relationships"": [
         [""Airline System"", ""communicates-with"", ""Custom API Hosted in Azure API Management""],
         [""Custom API Coordinator"", ""receives-notifications-from"", ""Airline System""],
         [""Custom API Coordinator"", ""sends-to"", ""Power Apps""],
         [""Power Apps"", ""assigns-flights-to"", ""Microsoft Teams Channels""],
         [""User"", ""selects-flight"", ""Flight""],
         [""System"", ""queues-Graph API call"", ""Azure Storage Account queue""],
         [""Azure Functions"", ""runs"", ""Graph API calls""],
         [""Azure Functions"", ""sends-notifications-to"", ""Teams""],
         [""Azure Functions"", ""streams-events-to"", ""Azure Event Hubs""],
         [""Custom Bot Messaging Service"", ""employs"", ""Azure Bot Service""],
         [""Custom Bots"", ""send-flight-updates-to"", ""Users""],
         [""Custom Bots"", ""send-flight-updates-to"", ""Teams""],
         [""Azure Data Lake storage"", ""offers-long-term-retention-and-processing"", ""Event Hubs""],
         [""Power BI"", ""generates-reports-with"", ""Data from Azure Data Lake storage.""]
      ]
   }"," {
      ""entities"": [
         [""Airline System"", ""system"", ""The entity that communicates with a custom API hosted in Azure API Management.""],
         [""Custom API Hosted in Azure API Management"", ""component"", ""The component that the Airline System communicates with.""],
         [""Custom API Coordinator"", ""component"", ""The component that receives notifications and handles incoming messages from the Airline System, sends them to Power Apps.""],
         [""Power Apps"", ""system"", ""The system where flights are assigned to Microsoft Teams channels.""],
         [""Microsoft Teams Channels"", ""component"", ""The component where flights are assigned.""],
         [""Azure Storage Account queue"", ""component"", ""The component where Graph API calls are queued for further processing.""],
         [""Azure Functions"", ""component"", ""The component that runs the Graph API calls based on the incoming messages in the storage queue.""],
         [""Graph API calls"", ""component"", ""The component that Azure Functions runs.""],
         [""Azure Event Hubs"", ""system"", ""The system where all events are streamed for further analytics.""],
         [""Custom Bot Messaging Service"", ""component"", ""The component that manages the airline's notification system.""],
         [""Azure Bot Service"", ""component"", ""The service employed by the Custom Bot Messaging Service.""],
         [""Custom Bots"", ""component"", ""The components that send flight updates to users in Teams.""],
         [""Teams Notifications"", ""component"", ""The notifications sent by custom bots.""],
         [""Azure Data Lake storage"", ""system"", ""The system that offers long-term retention and micro-batch processing of events from Event Hubs.""],
         [""Power BI"", ""system"", ""The system that generates insightful reports with data from Azure Data Lake storage.""]
      ],
      ""relationships"": [
         [""Airline System"", ""communicates-with"", ""Custom API Hosted in Azure API Management""],
         [""Custom API Coordinator"", ""receives-notifications-from"", ""Airline System""],
         [""Custom API Coordinator"", ""sends-to"", ""Power Apps""],
         [""Power Apps"", ""assigns-flights-to"", ""Microsoft Teams Channels""],
         [""User"", ""selects-flight"", ""Flight""],
         [""System"", ""queues-Graph API call"", ""Azure Storage Account queue""],
         [""Azure Functions"", ""runs"", ""Graph API calls""],
         [""Azure Functions"", ""sends-notifications-to"", ""Teams""],
         [""Azure Functions"", ""streams-events-to"", ""Azure Event Hubs""],
         [""Custom Bot Messaging Service"", ""employs"", ""Azure Bot Service""],
         [""Custom Bots"", ""send-flight-updates-to"", ""Users""],
         [""Custom Bots"", ""send-flight-updates-to"", ""Teams""],
         [""Azure Data Lake storage"", ""offers-long-term-retention-and-processing"", ""Event Hubs""],
         [""Power BI"", ""generates-reports-with"", ""Data from Azure Data Lake storage.""]
      ]
   }",7.711455253011081
71,mistral:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps.""],
         [""Logic Apps"", ""component"", ""Receives HTTP requests, uses secured template parameters to retrieve database credentials from Azure Key Vault, and uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
         [""Azure Key Vault"", ""component"", ""Stores database credentials used by Logic Apps.""],
         [""Transport Layer Security (TLS)"", ""system"", ""Used by Logic Apps to send secure communication over the network.""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database and runs the provided database statement.""],
         [""SQL Server"", ""system"", ""Stores data and makes it available to apps that users access.""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance.""]
      ],
      ""relationships"": [
         [""API Management"", ""calls"", ""Logic Apps""],
         [""Logic Apps"", ""part-of"", ""Transport Layer Security (TLS)""],
         [""Logic Apps"", ""part-of"", ""Azure Key Vault""],
         [""Logic Apps"", ""called-from"", ""on-premises data gateway""],
         [""on-premises data gateway"", ""part-of"", ""SQL Server""]
      ]
   }"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps.""],
         [""Logic Apps"", ""component"", ""Receives HTTP requests, uses secured template parameters to retrieve database credentials from Azure Key Vault, and uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
         [""Azure Key Vault"", ""component"", ""Stores database credentials used by Logic Apps.""],
         [""Transport Layer Security (TLS)"", ""system"", ""Used by Logic Apps to send secure communication over the network.""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database and runs the provided database statement.""],
         [""SQL Server"", ""system"", ""Stores data and makes it available to apps that users access.""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance.""]
      ],
      ""relationships"": [
         [""API Management"", ""calls"", ""Logic Apps""],
         [""Logic Apps"", ""part-of"", ""Transport Layer Security (TLS)""],
         [""Logic Apps"", ""part-of"", ""Azure Key Vault""],
         [""Logic Apps"", ""called-from"", ""on-premises data gateway""],
         [""on-premises data gateway"", ""part-of"", ""SQL Server""]
      ]
   }",3.913688557979185
72,mistral:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway that receives POST requests from outside services and clients""],
         [""CreatePatient function in Patient API"", ""component"", ""function responsible for creating a new patient document in Azure Cosmos DB""],
         [""Audit API CreateAuditRecord function"", ""component"", ""function responsible for creating audit records""],
         [""Azure Cosmos DB"", ""system"", ""Database service where patient documents and audit records are stored""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient function in Patient API"", ""Initiates a call to the CreatePatient function with given patient information""],
         [""CreatePatient function in Patient API"", ""part-of"", ""APIM"", ""Is called by APIM when it receives a POST request from outside services or clients""],
         [""CreatePatient function in Patient API"", ""calls"", ""Audit API CreateAuditRecord function"", ""Calls the Audit API CreateAuditRecord function to create an audit record""],
         [""Audit API CreateAuditRecord function"", ""part-of"", ""APIM"", ""Is called by the CreatePatient function within the Patient API to create an audit record""],
         [""Audit API CreateAuditRecord function"", ""contains"", ""Azure Cosmos DB"", ""Stores the created audit record in Azure Cosmos DB and returns a success response""]
      ]
   }"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway that receives POST requests from outside services and clients""],
         [""CreatePatient function in Patient API"", ""component"", ""function responsible for creating a new patient document in Azure Cosmos DB""],
         [""Audit API CreateAuditRecord function"", ""component"", ""function responsible for creating audit records""],
         [""Azure Cosmos DB"", ""system"", ""Database service where patient documents and audit records are stored""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient function in Patient API"", ""Initiates a call to the CreatePatient function with given patient information""],
         [""CreatePatient function in Patient API"", ""part-of"", ""APIM"", ""Is called by APIM when it receives a POST request from outside services or clients""],
         [""CreatePatient function in Patient API"", ""calls"", ""Audit API CreateAuditRecord function"", ""Calls the Audit API CreateAuditRecord function to create an audit record""],
         [""Audit API CreateAuditRecord function"", ""part-of"", ""APIM"", ""Is called by the CreatePatient function within the Patient API to create an audit record""],
         [""Audit API CreateAuditRecord function"", ""contains"", ""Azure Cosmos DB"", ""Stores the created audit record in Azure Cosmos DB and returns a success response""]
      ]
   }",3.7965302119846456
73,mistral:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}"," {
      ""entities"": [
         [""CDS"", ""system"", ""Cloud Data Storage""],
         [""SQL"", ""system"", ""Structured Query Language Database""],
         [""Data Factory"", ""component"", ""Data Integration Service""],
         [""ERP system"", ""system"", ""Enterprise Resource Planning System""],
         [""Azure SQL database"", ""component"", ""Azure SQL Instance""],
         [""Azure Functions"", ""component"", ""Serverless Compute Service""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Business Application Platform""],
         [""Power BI"", ""system"", ""Business Intelligence and Reporting System""]
      ],
      ""relationships"": [
         [""Data Factory"", ""part-of"", ""CDS""],
         [""CDS"", ""contains"", ""Data Factory""],
         [""ERP system"", ""calls"", ""Azure SQL database""],
         [""Azure SQL database"", ""called-from"", ""ERP system""],
         [""Azure Functions"", ""uses"", ""API""],
         [""API"", ""provides"", ""Azure Functions""],
         [""Power Apps"", ""retrieves"", ""data from Azure SQL Database""],
         [""Power Apps"", ""updates"", ""POs""],
         [""Power Apps"", ""sends"", ""CSV export""],
         [""CSV export"", ""contains"", ""PO data""],
         [""Power BI"", ""uses"", ""data from Power Apps""],
         [""Power BI"", ""reports"", ""trends in supplier status""]
      ]
   }"," {
      ""entities"": [
         [""CDS"", ""system"", ""Cloud Data Storage""],
         [""SQL"", ""system"", ""Structured Query Language Database""],
         [""Data Factory"", ""component"", ""Data Integration Service""],
         [""ERP system"", ""system"", ""Enterprise Resource Planning System""],
         [""Azure SQL database"", ""component"", ""Azure SQL Instance""],
         [""Azure Functions"", ""component"", ""Serverless Compute Service""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Business Application Platform""],
         [""Power BI"", ""system"", ""Business Intelligence and Reporting System""]
      ],
      ""relationships"": [
         [""Data Factory"", ""part-of"", ""CDS""],
         [""CDS"", ""contains"", ""Data Factory""],
         [""ERP system"", ""calls"", ""Azure SQL database""],
         [""Azure SQL database"", ""called-from"", ""ERP system""],
         [""Azure Functions"", ""uses"", ""API""],
         [""API"", ""provides"", ""Azure Functions""],
         [""Power Apps"", ""retrieves"", ""data from Azure SQL Database""],
         [""Power Apps"", ""updates"", ""POs""],
         [""Power Apps"", ""sends"", ""CSV export""],
         [""CSV export"", ""contains"", ""PO data""],
         [""Power BI"", ""uses"", ""data from Power Apps""],
         [""Power BI"", ""reports"", ""trends in supplier status""]
      ]
   }",4.07262820698088
74,mistral:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""The document file is stored here.""],
         [""Web App"", ""person"", ""The web app uploads the document file and adds a command message to a storage queue.""],
         [""Storage Queue"", ""component"", ""The web app adds a command message to this component to initiate pipeline processing.""],
         [""Durable Functions Orchestration"", ""system"", ""This system is triggered by the command message.""],
         [""Scan activity function"", ""component"", ""This function calls the Computer Vision Read API.""],
         [""Computer Vision Read API"", ""component"", ""It performs optical character recognition (OCR) on the document to be processed.""],
         [""Classify activity function"", ""component"", ""This function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""The document classifier service is hosted here.""],
         [""Document Classifier Service"", ""component"", ""It uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file.""],
         [""Azure Cosmos DB"", ""system"", ""The Metadata Store activity function saves the document type and page range information here.""],
         [""Metadata Store activity function"", ""component"", ""This function saves the document type and page range information in this system.""],
         [""Cognitive Search service"", ""system"", ""The Indexing activity function creates a new search document in this service.""],
         [""Indexing activity function"", ""component"", ""It creates a new search document for each identified document type and uses the Azure AI Search libraries for .NET to include the full OCR results and document information.""],
         [""End Users"", ""person"", ""They can search for documents by contents and metadata.""]
      ],
      ""relationships"": [
         [""Web App"", ""part-of"", ""Storage Queue""],
         [""Storage Queue"", ""calls"", ""Durable Functions Orchestration""],
         [""Durable Functions Orchestration"", ""contains"", ""Scan activity function""],
         [""Durable Functions Orchestration"", ""contains"", ""Classify activity function""],
         [""Classify activity function"", ""part-of"", ""Azure Kubernetes Service (AKS)""],
         [""Azure Blob Storage"", ""called-from"", ""Scan activity function""],
         [""Azure Blob Storage"", ""called-from"", ""Classify activity function""],
         [""Metadata Store activity function"", ""part-of"", ""Azure Cosmos DB""],
         [""Indexing activity function"", ""part-of"", ""Cognitive Search service""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""The document file is stored here.""],
         [""Web App"", ""person"", ""The web app uploads the document file and adds a command message to a storage queue.""],
         [""Storage Queue"", ""component"", ""The web app adds a command message to this component to initiate pipeline processing.""],
         [""Durable Functions Orchestration"", ""system"", ""This system is triggered by the command message.""],
         [""Scan activity function"", ""component"", ""This function calls the Computer Vision Read API.""],
         [""Computer Vision Read API"", ""component"", ""It performs optical character recognition (OCR) on the document to be processed.""],
         [""Classify activity function"", ""component"", ""This function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""The document classifier service is hosted here.""],
         [""Document Classifier Service"", ""component"", ""It uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file.""],
         [""Azure Cosmos DB"", ""system"", ""The Metadata Store activity function saves the document type and page range information here.""],
         [""Metadata Store activity function"", ""component"", ""This function saves the document type and page range information in this system.""],
         [""Cognitive Search service"", ""system"", ""The Indexing activity function creates a new search document in this service.""],
         [""Indexing activity function"", ""component"", ""It creates a new search document for each identified document type and uses the Azure AI Search libraries for .NET to include the full OCR results and document information.""],
         [""End Users"", ""person"", ""They can search for documents by contents and metadata.""]
      ],
      ""relationships"": [
         [""Web App"", ""part-of"", ""Storage Queue""],
         [""Storage Queue"", ""calls"", ""Durable Functions Orchestration""],
         [""Durable Functions Orchestration"", ""contains"", ""Scan activity function""],
         [""Durable Functions Orchestration"", ""contains"", ""Classify activity function""],
         [""Classify activity function"", ""part-of"", ""Azure Kubernetes Service (AKS)""],
         [""Azure Blob Storage"", ""called-from"", ""Scan activity function""],
         [""Azure Blob Storage"", ""called-from"", ""Classify activity function""],
         [""Metadata Store activity function"", ""part-of"", ""Azure Cosmos DB""],
         [""Indexing activity function"", ""part-of"", ""Cognitive Search service""]
      ]
   }",6.768953416030854
75,mistral:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}"," {
      ""entities"": [
         [""Custom API Coordinator"", ""component"", ""Coordinates notifications and handles incoming messages from the airline system""],
         [""Power Apps"", ""system"", ""Assigns flights to Microsoft Teams channels""],
         [""Azure Storage Account queue"", ""component"", ""Queues Graph API calls for further processing""],
         [""Azure Functions"", ""component"", ""Runs the Graph API calls based on incoming messages in the storage queue""],
         [""Azure Bot Service"", ""system"", ""Manages the airline's notification system""],
         [""Custom bot messaging service"", ""component"", ""Sends flight updates to users in Teams""],
         [""Azure Event Hubs"", ""component"", ""Streams all events for further analytics""]
      ],
      ""relationships"": [
         [""Custom API Coordinator"", ""part-of"", ""Azure API Management"", ""Hosted within Azure API Management""],
         [""Custom API Coordinator"", ""calls"", ""Power Apps"", ""Sends messages to Power Apps""],
         [""Power Apps"", ""part-of"", ""Microsoft Teams"", ""Assigns flights to Microsoft Teams channels""],
         [""Azure Storage Account queue"", ""part-of"", ""Azure Functions"", ""Functions run the Graph API calls based on incoming messages in the storage queue""],
         [""Azure Functions"", ""calls"", ""Azure Bot Service"", ""Sends notifications and updates to Teams""],
         [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""Streams all events for further analytics""]
      ]
   }"," {
      ""entities"": [
         [""Custom API Coordinator"", ""component"", ""Coordinates notifications and handles incoming messages from the airline system""],
         [""Power Apps"", ""system"", ""Assigns flights to Microsoft Teams channels""],
         [""Azure Storage Account queue"", ""component"", ""Queues Graph API calls for further processing""],
         [""Azure Functions"", ""component"", ""Runs the Graph API calls based on incoming messages in the storage queue""],
         [""Azure Bot Service"", ""system"", ""Manages the airline's notification system""],
         [""Custom bot messaging service"", ""component"", ""Sends flight updates to users in Teams""],
         [""Azure Event Hubs"", ""component"", ""Streams all events for further analytics""]
      ],
      ""relationships"": [
         [""Custom API Coordinator"", ""part-of"", ""Azure API Management"", ""Hosted within Azure API Management""],
         [""Custom API Coordinator"", ""calls"", ""Power Apps"", ""Sends messages to Power Apps""],
         [""Power Apps"", ""part-of"", ""Microsoft Teams"", ""Assigns flights to Microsoft Teams channels""],
         [""Azure Storage Account queue"", ""part-of"", ""Azure Functions"", ""Functions run the Graph API calls based on incoming messages in the storage queue""],
         [""Azure Functions"", ""calls"", ""Azure Bot Service"", ""Sends notifications and updates to Teams""],
         [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""Streams all events for further analytics""]
      ]
   }",3.8812628779560328
76,mistral:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}"," {
      ""entities"": [
         [""Connect Agent Component"", ""component"", ""Captures change logs using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""Publisher Component (Mainframe)"", ""component"", ""Manages data migration for mainframe systems.""],
         [""Listener Component (Midrange)"", ""component"", ""Manages data migration for midrange systems.""],
         [""Azure"", ""system"", ""The destination where the data is moved from on-premises.""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target.""],
         [""Database (Target)"", ""component"", ""Receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""system"", ""Consumes the changed data and applies it to Azure data platform services.""],
         [""Connect Controller Daemon"", ""component"", ""Authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]
      ],
      ""relationships"": [
         [""Publisher Component (Mainframe)"", ""part-of"", ""Azure""],
         [""Listener Component (Midrange)"", ""part-of"", ""Azure""],
         [""Publisher Component (Mainframe)"", ""calls"", ""Connect Replicator Engine""],
         [""Listener Component (Midrange)"", ""calls"", ""Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""contains"", ""Database (Target)""]
      ]
   }"," {
      ""entities"": [
         [""Connect Agent Component"", ""component"", ""Captures change logs using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""Publisher Component (Mainframe)"", ""component"", ""Manages data migration for mainframe systems.""],
         [""Listener Component (Midrange)"", ""component"", ""Manages data migration for midrange systems.""],
         [""Azure"", ""system"", ""The destination where the data is moved from on-premises.""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target.""],
         [""Database (Target)"", ""component"", ""Receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""system"", ""Consumes the changed data and applies it to Azure data platform services.""],
         [""Connect Controller Daemon"", ""component"", ""Authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]
      ],
      ""relationships"": [
         [""Publisher Component (Mainframe)"", ""part-of"", ""Azure""],
         [""Listener Component (Midrange)"", ""part-of"", ""Azure""],
         [""Publisher Component (Mainframe)"", ""calls"", ""Connect Replicator Engine""],
         [""Listener Component (Midrange)"", ""calls"", ""Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""contains"", ""Database (Target)""]
      ]
   }",4.116044788039289
77,mistral:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Individuals who upload datasets into a public blob storage account.""],
         [""Blob Storage Account"", ""system"", ""The storage account where data owners upload encrypted datasets.""],
         [""Microsoft-managed keys"", ""component"", ""Keys used to encrypt the data in the Blob Storage Account.""],
         [""Azure Data Factory"", ""component"", ""A system that triggers the copying of uploaded datasets.""],
         [""Trigger"", ""component"", ""Event that starts the copying process by Azure Data Factory.""],
         [""Import Path"", ""component"", ""The specific location in another storage account where the data is copied.""],
         [""Private Endpoint"", ""component"", ""A network security feature that allows access to the storage account.""],
         [""Service Principal"", ""component"", ""An identity with limited permissions that accesses the storage account.""],
         [""Azure Virtual Desktop"", ""component"", ""The streaming application used by researchers to access the secure environment.""],
         [""Privileged Jump Box"", ""component"", ""Used as a means for researchers to access the Azure Virtual Desktop.""],
         [""Secure Network Environment"", ""system"", ""A network where data science VMs are provisioned for research work.""],
         [""Data Science VMs"", ""component"", ""Virtual Machines used for much of the data preparation in the secure environment.""],
         [""Secure Storage Account"", ""system"", ""The storage account containing the dataset and de-identified data.""],
         [""Export Path"", ""component"", ""A location on the Secure Storage Account where approved data is saved.""],
         [""Logic App"", ""component"", ""An application that triggers a notification and starts the manual approval process.""],
         [""Manual Reviewers"", ""person"", ""Individuals who ensure sensitive data isn't exported.""],
         [""Azure Machine Learning Compute"", ""component"", ""A system providing Azure Machine Learning capabilities, such as model training.""],
         [""De-identified Data"", ""component"", ""Data with personal information removed.""]
      ],
      ""relationships"": [
         [""Blob Storage Account"", ""contains"", [""Microsoft-managed keys""]],
         [""Azure Data Factory"", ""part-of"", [""Trigger""]],
         [""Trigger"", ""calls"", [""Azure Data Factory""]],
         [""Azure Data Factory"", ""part-of"", [""Data Owners"", ""copying datasets from import path""]],
         [""Data Factory instance"", ""contains"", [""Import Path""]],
         [""Data Factory instance"", ""part-of"", [""Private Endpoint""]],
         [""Data Factory instance"", ""calls"", [""Service Principal""]],
         [""Azure Virtual Desktop"", ""part-of"", [""Researchers""]],
         [""Secure Network Environment"", ""contains"", [""Data Science VMs""]],
         [""Secure Storage Account"", ""contains"", [""Dataset in the secure storage account"", ""De-identified Data"", ""Export Path""]],
         [""Logic App"", ""calls"", [""Manual Reviewers""]],
         [""Azure Machine Learning Compute"", ""part-of"", [""Secure Environment""]],
         [""Data Factory instance"", ""parts-from"", [""Secure Storage Account""]],
         [""Data Factory instance"", ""calls"", [""Public Storage Account""]]
      ]
   }"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Individuals who upload datasets into a public blob storage account.""],
         [""Blob Storage Account"", ""system"", ""The storage account where data owners upload encrypted datasets.""],
         [""Microsoft-managed keys"", ""component"", ""Keys used to encrypt the data in the Blob Storage Account.""],
         [""Azure Data Factory"", ""component"", ""A system that triggers the copying of uploaded datasets.""],
         [""Trigger"", ""component"", ""Event that starts the copying process by Azure Data Factory.""],
         [""Import Path"", ""component"", ""The specific location in another storage account where the data is copied.""],
         [""Private Endpoint"", ""component"", ""A network security feature that allows access to the storage account.""],
         [""Service Principal"", ""component"", ""An identity with limited permissions that accesses the storage account.""],
         [""Azure Virtual Desktop"", ""component"", ""The streaming application used by researchers to access the secure environment.""],
         [""Privileged Jump Box"", ""component"", ""Used as a means for researchers to access the Azure Virtual Desktop.""],
         [""Secure Network Environment"", ""system"", ""A network where data science VMs are provisioned for research work.""],
         [""Data Science VMs"", ""component"", ""Virtual Machines used for much of the data preparation in the secure environment.""],
         [""Secure Storage Account"", ""system"", ""The storage account containing the dataset and de-identified data.""],
         [""Export Path"", ""component"", ""A location on the Secure Storage Account where approved data is saved.""],
         [""Logic App"", ""component"", ""An application that triggers a notification and starts the manual approval process.""],
         [""Manual Reviewers"", ""person"", ""Individuals who ensure sensitive data isn't exported.""],
         [""Azure Machine Learning Compute"", ""component"", ""A system providing Azure Machine Learning capabilities, such as model training.""],
         [""De-identified Data"", ""component"", ""Data with personal information removed.""]
      ],
      ""relationships"": [
         [""Blob Storage Account"", ""contains"", [""Microsoft-managed keys""]],
         [""Azure Data Factory"", ""part-of"", [""Trigger""]],
         [""Trigger"", ""calls"", [""Azure Data Factory""]],
         [""Azure Data Factory"", ""part-of"", [""Data Owners"", ""copying datasets from import path""]],
         [""Data Factory instance"", ""contains"", [""Import Path""]],
         [""Data Factory instance"", ""part-of"", [""Private Endpoint""]],
         [""Data Factory instance"", ""calls"", [""Service Principal""]],
         [""Azure Virtual Desktop"", ""part-of"", [""Researchers""]],
         [""Secure Network Environment"", ""contains"", [""Data Science VMs""]],
         [""Secure Storage Account"", ""contains"", [""Dataset in the secure storage account"", ""De-identified Data"", ""Export Path""]],
         [""Logic App"", ""calls"", [""Manual Reviewers""]],
         [""Azure Machine Learning Compute"", ""part-of"", [""Secure Environment""]],
         [""Data Factory instance"", ""parts-from"", [""Secure Storage Account""]],
         [""Data Factory instance"", ""calls"", [""Public Storage Account""]]
      ]
   }",8.382760272012092
78,mistral:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""The storage service where phone call recordings and transcripts are stored.""],
         [""Azure Storage Account"", ""component"", ""A component within Azure Blob Storage, used for storing files such as audio files and transcripts.""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool used to interact with Azure Storage Account.""],
         [""Storage SDK or API"", ""component"", ""Software development kits (SDK) or Application Programming Interfaces (API) used to interact with Azure Storage Account.""],
         [""Azure AI Speech"", ""system"", ""The system used for transcribing audio files asynchronously in batch mode, with speaker diarization enabled.""],
         [""Batch mode"", ""description"", ""A way of processing large amounts of data at once.""],
         [""Asynchronous"", ""description"", ""A method of handling tasks concurrently without blocking the main thread of execution.""],
         [""Azure AI Language"", ""system"", ""The system used for detecting and redacting personal data in transcripts.""],
         [""Ingestion Client tool"", ""component"", ""A no-code approach tool for call center transcription, using Azure AI Speech and Azure AI Language services.""],
         [""Azure OpenAI"", ""system"", ""The system used for processing transcripts, extracting entities, summarizing conversations, analyzing sentiments, and more.""],
         [""Power BI"", ""component"", ""A service or tool used for visualizing output from Azure OpenAI.""],
         [""App Service"", ""component"", ""A platform provided by Microsoft Azure for hosting web applications.""],
         [""Custom web application"", ""component"", ""A web application custom-built and hosted on App Service.""],
         [""Azure CRM"", ""system"", ""A customer relationship management system used for storing output data from the automated process, providing agents with contextual information about customer calls.""],
         [""Power BI or Custom Web Application"", ""description"", ""Options for visualizing output data, with Power BI being a service and Custom Web Application being a self-built web application.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage Account""],
         [""Azure Storage Account"", ""part-of"", ""Azure Blob Storage""],
         [""Azure AI Speech"", ""calls"", ""Azure Storage Account""],
         [""Azure AI Language"", ""calls"", ""Azure Blob Storage""],
         [""Ingestion Client tool"", ""part-of"", ""Azure AI Speech""],
         [""Ingestion Client tool"", ""part-of"", ""Azure AI Language""],
         [""Azure OpenAI"", ""calls"", ""Azure Blob Storage""],
         [""Power BI"", ""called-from"", ""Azure OpenAI""],
         [""Custom Web Application"", ""called-from"", ""Azure OpenAI""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""The storage service where phone call recordings and transcripts are stored.""],
         [""Azure Storage Account"", ""component"", ""A component within Azure Blob Storage, used for storing files such as audio files and transcripts.""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool used to interact with Azure Storage Account.""],
         [""Storage SDK or API"", ""component"", ""Software development kits (SDK) or Application Programming Interfaces (API) used to interact with Azure Storage Account.""],
         [""Azure AI Speech"", ""system"", ""The system used for transcribing audio files asynchronously in batch mode, with speaker diarization enabled.""],
         [""Batch mode"", ""description"", ""A way of processing large amounts of data at once.""],
         [""Asynchronous"", ""description"", ""A method of handling tasks concurrently without blocking the main thread of execution.""],
         [""Azure AI Language"", ""system"", ""The system used for detecting and redacting personal data in transcripts.""],
         [""Ingestion Client tool"", ""component"", ""A no-code approach tool for call center transcription, using Azure AI Speech and Azure AI Language services.""],
         [""Azure OpenAI"", ""system"", ""The system used for processing transcripts, extracting entities, summarizing conversations, analyzing sentiments, and more.""],
         [""Power BI"", ""component"", ""A service or tool used for visualizing output from Azure OpenAI.""],
         [""App Service"", ""component"", ""A platform provided by Microsoft Azure for hosting web applications.""],
         [""Custom web application"", ""component"", ""A web application custom-built and hosted on App Service.""],
         [""Azure CRM"", ""system"", ""A customer relationship management system used for storing output data from the automated process, providing agents with contextual information about customer calls.""],
         [""Power BI or Custom Web Application"", ""description"", ""Options for visualizing output data, with Power BI being a service and Custom Web Application being a self-built web application.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage Account""],
         [""Azure Storage Account"", ""part-of"", ""Azure Blob Storage""],
         [""Azure AI Speech"", ""calls"", ""Azure Storage Account""],
         [""Azure AI Language"", ""calls"", ""Azure Blob Storage""],
         [""Ingestion Client tool"", ""part-of"", ""Azure AI Speech""],
         [""Ingestion Client tool"", ""part-of"", ""Azure AI Language""],
         [""Azure OpenAI"", ""calls"", ""Azure Blob Storage""],
         [""Power BI"", ""called-from"", ""Azure OpenAI""],
         [""Custom Web Application"", ""called-from"", ""Azure OpenAI""]
      ]
   }",6.989180678036064
79,mistral:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A platform for version control and collaboration, where developers check in code.""],
         [""CI/CD pipeline agent"", ""component"", ""An Azure VM component that's installed and responsible for pushing the build to the API application.""],
         [""Azure VM"", ""component"", ""A virtual machine on which the CI/CD pipeline agent is installed.""],
         [""API application"", ""component"", ""The application that receives the build from the CI/CD pipeline agent.""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment where the API application is hosted.""],
         [""Azure API Management"", ""system"", ""A system for managing and publishing APIs via HOST headers specified in its policy.""],
         [""API Management policy"", ""component"", ""The configuration settings within Azure API Management.""],
         [""HOST headers"", ""component"", ""Headers used by Azure API Management to consume the preceding APIs.""],
         [""App Service Environment's DNS name"", ""component"", ""The DNS name of the App Service Environment used by Azure API Management for all APIs.""],
         [""Application Gateway"", ""system"", ""A system that exposes API Management's developer and API portal.""],
         [""Azure Private DNS"", ""system"", ""A system used to route the traffic internally between ASE, API Management, and Application Gateway.""],
         [""ASE"", ""component"", ""The component within the ILB ASE system that the API application is hosted on.""],
         [""API Management's developer and API portal"", ""component"", ""The part of the Application Gateway system that external users utilize to consume the APIs.""],
         [""Application Gateway's public IP"", ""component"", ""The IP address exposed by the Application Gateway through which external users access the APIs.""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
         [""Azure VM"", ""contains"", ""CI/CD pipeline agent""],
         [""API application"", ""called-from"", ""CI/CD pipeline agent""],
         [""ILB ASE"", ""contains"", ""API application""],
         [""Azure API Management"", ""uses"", ""HOST headers""],
         [""API Management policy"", ""part-of"", ""Azure API Management""],
         [""Application Gateway"", ""exposes"", ""API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", ""traffic between ASE, API Management, and Application Gateway""],
         [""ASE"", ""contains"", ""API application""],
         [""API Management"", ""consumes"", ""preceding APIs via HOST headers""],
         [""API Management's developer and API portal"", ""part-of"", ""Application Gateway""],
         [""Application Gateway's public IP"", ""utilized by"", ""external users""]
      ]
   }"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A platform for version control and collaboration, where developers check in code.""],
         [""CI/CD pipeline agent"", ""component"", ""An Azure VM component that's installed and responsible for pushing the build to the API application.""],
         [""Azure VM"", ""component"", ""A virtual machine on which the CI/CD pipeline agent is installed.""],
         [""API application"", ""component"", ""The application that receives the build from the CI/CD pipeline agent.""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment where the API application is hosted.""],
         [""Azure API Management"", ""system"", ""A system for managing and publishing APIs via HOST headers specified in its policy.""],
         [""API Management policy"", ""component"", ""The configuration settings within Azure API Management.""],
         [""HOST headers"", ""component"", ""Headers used by Azure API Management to consume the preceding APIs.""],
         [""App Service Environment's DNS name"", ""component"", ""The DNS name of the App Service Environment used by Azure API Management for all APIs.""],
         [""Application Gateway"", ""system"", ""A system that exposes API Management's developer and API portal.""],
         [""Azure Private DNS"", ""system"", ""A system used to route the traffic internally between ASE, API Management, and Application Gateway.""],
         [""ASE"", ""component"", ""The component within the ILB ASE system that the API application is hosted on.""],
         [""API Management's developer and API portal"", ""component"", ""The part of the Application Gateway system that external users utilize to consume the APIs.""],
         [""Application Gateway's public IP"", ""component"", ""The IP address exposed by the Application Gateway through which external users access the APIs.""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
         [""Azure VM"", ""contains"", ""CI/CD pipeline agent""],
         [""API application"", ""called-from"", ""CI/CD pipeline agent""],
         [""ILB ASE"", ""contains"", ""API application""],
         [""Azure API Management"", ""uses"", ""HOST headers""],
         [""API Management policy"", ""part-of"", ""Azure API Management""],
         [""Application Gateway"", ""exposes"", ""API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", ""traffic between ASE, API Management, and Application Gateway""],
         [""ASE"", ""contains"", ""API application""],
         [""API Management"", ""consumes"", ""preceding APIs via HOST headers""],
         [""API Management's developer and API portal"", ""part-of"", ""Application Gateway""],
         [""Application Gateway's public IP"", ""utilized by"", ""external users""]
      ]
   }",7.21057335898513
80,mistral:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""The point of entry for requests to an application, routing them to Azure AD B2C or another IdP.""],
         [""Azure AD B2C"", ""component"", ""Identity and Access Management service that authenticates users.""],
         [""Custom domain"", ""component"", ""A specific domain configured for user sign-in in Azure AD B2C.""],
         [""API client"", ""component"", ""The software or system that initiates a request to the application.""],
         [""Credentials"", ""component"", ""Information such as username and password provided by the API client for authentication.""],
         [""JSON Web Token (JWT)"", ""component"", ""A token returned by Azure AD B2C to authenticated users.""],
         [""API Management"", ""system"", ""A service that manages, secures, and documents APIs.""],
         [""validate-jwt policy"", ""component"", ""A policy within API Management used to validate JWT bearer tokens.""],
         [""App Service Environment"", ""component"", ""An Azure environment running backend APIs.""],
         [""Function Apps"", ""component"", ""Azure service for running serverless functions and small applications.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""An Azure managed container orchestration service that runs on a cluster of VMs.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management""],
         [""Azure AD B2C"", ""calls-from"", ""API client""],
         [""API Management"", ""part-of"", ""Azure Front Door""],
         [""API Management"", ""contains"", ""validate-jwt policy""],
         [""API Management"", ""called-from"", ""API client""],
         [""App Service Environment"", ""part-of"", ""Backend APIs""],
         [""Function Apps"", ""part-of"", ""Backend APIs""],
         [""Azure Kubernetes Services (AKS)"", ""part-of"", ""Backend APIs""]
      ]
   }"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""The point of entry for requests to an application, routing them to Azure AD B2C or another IdP.""],
         [""Azure AD B2C"", ""component"", ""Identity and Access Management service that authenticates users.""],
         [""Custom domain"", ""component"", ""A specific domain configured for user sign-in in Azure AD B2C.""],
         [""API client"", ""component"", ""The software or system that initiates a request to the application.""],
         [""Credentials"", ""component"", ""Information such as username and password provided by the API client for authentication.""],
         [""JSON Web Token (JWT)"", ""component"", ""A token returned by Azure AD B2C to authenticated users.""],
         [""API Management"", ""system"", ""A service that manages, secures, and documents APIs.""],
         [""validate-jwt policy"", ""component"", ""A policy within API Management used to validate JWT bearer tokens.""],
         [""App Service Environment"", ""component"", ""An Azure environment running backend APIs.""],
         [""Function Apps"", ""component"", ""Azure service for running serverless functions and small applications.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""An Azure managed container orchestration service that runs on a cluster of VMs.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management""],
         [""Azure AD B2C"", ""calls-from"", ""API client""],
         [""API Management"", ""part-of"", ""Azure Front Door""],
         [""API Management"", ""contains"", ""validate-jwt policy""],
         [""API Management"", ""called-from"", ""API client""],
         [""App Service Environment"", ""part-of"", ""Backend APIs""],
         [""Function Apps"", ""part-of"", ""Backend APIs""],
         [""Azure Kubernetes Services (AKS)"", ""part-of"", ""Backend APIs""]
      ]
   }",5.143719052022789
81,mistral:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}"," {
      ""entities"": [
        [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps.""],
        [""Logic Apps"", ""component"", ""Uses secured template parameters to retrieve database credentials from Azure Key Vault, uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
        [""Azure Key Vault"", ""component"", ""Stores the database credentials used by Logic Apps.""],
        [""Transport Layer Security (TLS)"", ""component"", ""Used by Logic Apps to send secure information to the on-premises data gateway.""],
        [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database and runs the provided database statement.""],
        [""SQL Server"", ""system"", ""Stores data and makes it available to apps that users access.""],
        [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance.""]
      ],
      ""relationships"": [
        [""API Management"", ""calls"", ""Logic Apps""],
        [""Logic Apps"", ""part-of"", ""API Management""],
        [""Logic Apps"", ""contains"", ""Transport Layer Security (TLS)""],
        [""Logic Apps"", ""contained-by"", ""Azure Key Vault""],
        [""on-premises data gateway"", ""part-of"", ""API Management""],
        [""SQL Server"", ""called-from"", ""Logic Apps""]
      ]
    }"," {
      ""entities"": [
        [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps.""],
        [""Logic Apps"", ""component"", ""Uses secured template parameters to retrieve database credentials from Azure Key Vault, uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
        [""Azure Key Vault"", ""component"", ""Stores the database credentials used by Logic Apps.""],
        [""Transport Layer Security (TLS)"", ""component"", ""Used by Logic Apps to send secure information to the on-premises data gateway.""],
        [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database and runs the provided database statement.""],
        [""SQL Server"", ""system"", ""Stores data and makes it available to apps that users access.""],
        [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance.""]
      ],
      ""relationships"": [
        [""API Management"", ""calls"", ""Logic Apps""],
        [""Logic Apps"", ""part-of"", ""API Management""],
        [""Logic Apps"", ""contains"", ""Transport Layer Security (TLS)""],
        [""Logic Apps"", ""contained-by"", ""Azure Key Vault""],
        [""on-premises data gateway"", ""part-of"", ""API Management""],
        [""SQL Server"", ""called-from"", ""Logic Apps""]
      ]
    }",4.198470266012009
82,mistral:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}"," {
      ""entities"": [
         [""Outlook email account"", ""person"", ""A designated Outlook email account""],
         [""Azure Logic Apps"", ""system"", ""The logic app built using the capabilities of Azure Logic Apps""],
         [""Azure Data Lake Storage"", ""system"", ""A container in Azure Data Lake Storage where PDF files are uploaded""],
         [""function app"", ""component"", ""A function app built by using the capabilities of Azure Functions that processes single-page PDF files""],
         [""Azure AI Document Intelligence"", ""component"", ""An Azure REST API used for processing single-page PDF files""],
         [""Azure Cosmos DB"", ""system"", ""A database where processed data is saved""]
      ],
      ""relationships"": [
         [""Outlook email account"", ""calls"", ""Azure Logic Apps"", ""The arrival of an email triggers the logic app to process the email""],
         [""Azure Logic Apps"", ""part-of"", ""Azure Data Lake Storage"", ""The logic app uploads PDF files to a container in Azure Data Lake Storage""],
         [""Azure Data Lake Storage"", ""contains"", ""PDF container"", ""PDF files are stored in this container""],
         [""PDF container"", ""calls"", ""another logic app"", ""The arrival of a PDF file in the PDF container triggers another logic app""],
         [""another logic app"", ""part-of"", ""function app"", ""The other logic app sends the location of the PDF file to the function app for processing""],
         [""function app"", ""part-of"", ""Azure Data Lake Storage"", ""Split files are saved to a second container in Data Lake Storage""],
         [""function app"", ""calls"", ""Azure AI Document Intelligence"", ""The function app sends the location of the single-page PDF file to Azure AI Document Intelligence for processing""],
         [""Azure AI Document Intelligence"", ""called-from"", ""function app"", ""When Azure AI Document Intelligence completes its processing, it sends a response back to the function app""],
         [""function app"", ""calls"", ""a third container in Data Lake Storage"", ""The created JSON data file is stored in this container""],
         [""Cosmos DB"", ""receives"", ""processed data"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""],
         [""Azure Cosmos DB"", ""contains"", ""data structure"", ""The processed data is placed into a data structure""],
         [""Power BI"", ""obtains"", ""data from Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB""]
      ]
   }"," {
      ""entities"": [
         [""Outlook email account"", ""person"", ""A designated Outlook email account""],
         [""Azure Logic Apps"", ""system"", ""The logic app built using the capabilities of Azure Logic Apps""],
         [""Azure Data Lake Storage"", ""system"", ""A container in Azure Data Lake Storage where PDF files are uploaded""],
         [""function app"", ""component"", ""A function app built by using the capabilities of Azure Functions that processes single-page PDF files""],
         [""Azure AI Document Intelligence"", ""component"", ""An Azure REST API used for processing single-page PDF files""],
         [""Azure Cosmos DB"", ""system"", ""A database where processed data is saved""]
      ],
      ""relationships"": [
         [""Outlook email account"", ""calls"", ""Azure Logic Apps"", ""The arrival of an email triggers the logic app to process the email""],
         [""Azure Logic Apps"", ""part-of"", ""Azure Data Lake Storage"", ""The logic app uploads PDF files to a container in Azure Data Lake Storage""],
         [""Azure Data Lake Storage"", ""contains"", ""PDF container"", ""PDF files are stored in this container""],
         [""PDF container"", ""calls"", ""another logic app"", ""The arrival of a PDF file in the PDF container triggers another logic app""],
         [""another logic app"", ""part-of"", ""function app"", ""The other logic app sends the location of the PDF file to the function app for processing""],
         [""function app"", ""part-of"", ""Azure Data Lake Storage"", ""Split files are saved to a second container in Data Lake Storage""],
         [""function app"", ""calls"", ""Azure AI Document Intelligence"", ""The function app sends the location of the single-page PDF file to Azure AI Document Intelligence for processing""],
         [""Azure AI Document Intelligence"", ""called-from"", ""function app"", ""When Azure AI Document Intelligence completes its processing, it sends a response back to the function app""],
         [""function app"", ""calls"", ""a third container in Data Lake Storage"", ""The created JSON data file is stored in this container""],
         [""Cosmos DB"", ""receives"", ""processed data"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""],
         [""Azure Cosmos DB"", ""contains"", ""data structure"", ""The processed data is placed into a data structure""],
         [""Power BI"", ""obtains"", ""data from Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB""]
      ]
   }",6.312765635026153
83,mistral:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""The system triggered by HTTP requests and used for more complex orchestration.""],
         [""Azure API Management"", ""system"", ""A system consisting of two components: API gateway and developer portal.""],
         [""API gateway"", ""component"", ""Accepts HTTP calls and routes them to the backend.""],
         [""Developer portal"", ""component"", ""Provides access to documentation, code samples for calling APIs and testing APIs.""],
         [""Azure DNS"", ""system"", ""Provides name resolution using Azure infrastructure.""],
         [""Custom domain name"", ""component"", ""A domain name used to map the custom domain name to the IP address.""],
         [""Microsoft Entra ID"", ""system"", ""Used for client authentication that calls the API gateway.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""part-of"", ""Enterprise"", ""The enterprise has deployed or relies on Azure Logic Apps.""],
         [""Azure API Management"", ""part-of"", ""Enterprise"", ""The enterprise has deployed or relies on Azure API Management.""],
         [""API gateway"", ""part-of"", ""Azure API Management"", ""The API gateway is a component of Azure API Management.""],
         [""Developer portal"", ""part-of"", ""Azure API Management"", ""The developer portal is a component of Azure API Management.""],
         [""Custom domain name"", ""called-from"", ""Azure DNS"", ""The custom domain name is mapped to the IP address using Azure DNS.""],
         [""API gateway"", ""calls"", ""Backend systems"", ""The API gateway routes HTTP calls to the backend systems.""]
         // No need to include this as it's a redundant relationship: [""Backend systems"", ""part-of"", ""Azure Logic Apps""] or [""Backend systems"", ""part-of"", ""API gateway""]
      ]
   }"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""The system triggered by HTTP requests and used for more complex orchestration.""],
         [""Azure API Management"", ""system"", ""A system consisting of two components: API gateway and developer portal.""],
         [""API gateway"", ""component"", ""Accepts HTTP calls and routes them to the backend.""],
         [""Developer portal"", ""component"", ""Provides access to documentation, code samples for calling APIs and testing APIs.""],
         [""Azure DNS"", ""system"", ""Provides name resolution using Azure infrastructure.""],
         [""Custom domain name"", ""component"", ""A domain name used to map the custom domain name to the IP address.""],
         [""Microsoft Entra ID"", ""system"", ""Used for client authentication that calls the API gateway.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""part-of"", ""Enterprise"", ""The enterprise has deployed or relies on Azure Logic Apps.""],
         [""Azure API Management"", ""part-of"", ""Enterprise"", ""The enterprise has deployed or relies on Azure API Management.""],
         [""API gateway"", ""part-of"", ""Azure API Management"", ""The API gateway is a component of Azure API Management.""],
         [""Developer portal"", ""part-of"", ""Azure API Management"", ""The developer portal is a component of Azure API Management.""],
         [""Custom domain name"", ""called-from"", ""Azure DNS"", ""The custom domain name is mapped to the IP address using Azure DNS.""],
         [""API gateway"", ""calls"", ""Backend systems"", ""The API gateway routes HTTP calls to the backend systems.""]
         // No need to include this as it's a redundant relationship: [""Backend systems"", ""part-of"", ""Azure Logic Apps""] or [""Backend systems"", ""part-of"", ""API gateway""]
      ]
   }",4.943573808996007
84,mistral:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Those who upload datasets into the public blob storage account.""],
         [""Public Blob Storage Account"", ""system"", ""Where data owners upload their encrypted datasets using Microsoft-managed keys.""],
         [""Azure Data Factory"", ""component"", ""A system that initiates copying of the dataset from a specific location to another storage account with security controls.""],
         [""Private Endpoint"", ""system"", ""A security feature allowing access to the storage account only through this endpoint.""],
         [""Service Principal"", ""person"", ""An identity representing a user or application in Azure Active Directory, used for granting permissions to the Data Factory.""],
         [""Storage Account"", ""component"", ""The storage account where the copied dataset is stored.""],
         [""Security Controls"", ""system"", ""Measures taken to secure the data in the storage account.""],
         [""Azure Virtual Desktop"", ""component"", ""A privileged jump box used by researchers to access the secure environment.""],
         [""Streaming Application"", ""component"", ""An application used by researchers to connect with Azure Virtual Desktop for accessing the secure environment.""],
         [""Secure Network Environment"", ""system"", ""The network where the dataset is presented to data science VMs and much of the data preparation is done.""],
         [""Data Science VMs"", ""component"", ""VMs provisioned in the secure network environment, where data preparation and research work are performed.""],
         [""Azure Machine Learning Compute"", ""component"", ""A system with capabilities to access the dataset through a private endpoint for machine learning tasks.""],
         [""Machine Learning Models"", ""component"", ""Models created within the secure environment that meet regulatory guidelines.""],
         [""De-identified Data"", ""component"", ""The data without personal information after removal of sensitive information.""],
         [""Export Path"", ""system"", ""A location on the secure storage account where approved de-identified data is saved.""],
         [""Logic App"", ""component"", ""An app that initiates an approval process for data queued to be exported.""],
         [""Manual Reviewers"", ""person"", ""Individuals who ensure sensitive data isn't exported in the review process.""],
         [""Secure Storage (Export Path)"", ""system"", ""The storage location where approved de-identified data is saved.""]
      ],
      ""relationships"": [
         [""Data Owners"", ""calls"", ""Public Blob Storage Account""],
         [""Public Blob Storage Account"", ""part-of"", ""Azure Data Factory""],
         [""Azure Data Factory"", ""part-of"", ""Private Endpoint""],
         [""Azure Data Factory"", ""part-of"", ""Service Principal""],
         [""Azure Data Factory"", ""calls"", ""Storage Account""],
         [""Data Factory"", ""part-of"", ""Security Controls""],
         [""Streaming Application"", ""part-of"", ""Azure Virtual Desktop""],
         [""Secure Network Environment"", ""contains"", ""Data Science VMs""],
         [""Secure Network Environment"", ""part-of"", ""Azure Machine Learning Compute""],
         [""De-identified Data"", ""part-of"", ""Export Path""],
         [""Logic App"", ""called-from"", ""Manual Reviewers""],
         [""Manual Reviewers"", ""part-of"", ""Approval Process""],
         [""Secure Storage (Export Path)"", ""calls"", ""Data Factory""]
      ]
   }"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Those who upload datasets into the public blob storage account.""],
         [""Public Blob Storage Account"", ""system"", ""Where data owners upload their encrypted datasets using Microsoft-managed keys.""],
         [""Azure Data Factory"", ""component"", ""A system that initiates copying of the dataset from a specific location to another storage account with security controls.""],
         [""Private Endpoint"", ""system"", ""A security feature allowing access to the storage account only through this endpoint.""],
         [""Service Principal"", ""person"", ""An identity representing a user or application in Azure Active Directory, used for granting permissions to the Data Factory.""],
         [""Storage Account"", ""component"", ""The storage account where the copied dataset is stored.""],
         [""Security Controls"", ""system"", ""Measures taken to secure the data in the storage account.""],
         [""Azure Virtual Desktop"", ""component"", ""A privileged jump box used by researchers to access the secure environment.""],
         [""Streaming Application"", ""component"", ""An application used by researchers to connect with Azure Virtual Desktop for accessing the secure environment.""],
         [""Secure Network Environment"", ""system"", ""The network where the dataset is presented to data science VMs and much of the data preparation is done.""],
         [""Data Science VMs"", ""component"", ""VMs provisioned in the secure network environment, where data preparation and research work are performed.""],
         [""Azure Machine Learning Compute"", ""component"", ""A system with capabilities to access the dataset through a private endpoint for machine learning tasks.""],
         [""Machine Learning Models"", ""component"", ""Models created within the secure environment that meet regulatory guidelines.""],
         [""De-identified Data"", ""component"", ""The data without personal information after removal of sensitive information.""],
         [""Export Path"", ""system"", ""A location on the secure storage account where approved de-identified data is saved.""],
         [""Logic App"", ""component"", ""An app that initiates an approval process for data queued to be exported.""],
         [""Manual Reviewers"", ""person"", ""Individuals who ensure sensitive data isn't exported in the review process.""],
         [""Secure Storage (Export Path)"", ""system"", ""The storage location where approved de-identified data is saved.""]
      ],
      ""relationships"": [
         [""Data Owners"", ""calls"", ""Public Blob Storage Account""],
         [""Public Blob Storage Account"", ""part-of"", ""Azure Data Factory""],
         [""Azure Data Factory"", ""part-of"", ""Private Endpoint""],
         [""Azure Data Factory"", ""part-of"", ""Service Principal""],
         [""Azure Data Factory"", ""calls"", ""Storage Account""],
         [""Data Factory"", ""part-of"", ""Security Controls""],
         [""Streaming Application"", ""part-of"", ""Azure Virtual Desktop""],
         [""Secure Network Environment"", ""contains"", ""Data Science VMs""],
         [""Secure Network Environment"", ""part-of"", ""Azure Machine Learning Compute""],
         [""De-identified Data"", ""part-of"", ""Export Path""],
         [""Logic App"", ""called-from"", ""Manual Reviewers""],
         [""Manual Reviewers"", ""part-of"", ""Approval Process""],
         [""Secure Storage (Export Path)"", ""calls"", ""Data Factory""]
      ]
   }",8.372905708034523
85,mistral:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}"," {
      ""entities"": [
         [""Airline System"", ""system"", ""The entity that communicates with a custom API hosted in Azure API Management""],
         [""Custom API Hosted in Azure API Management"", ""component"", ""The component that the airline system communicates with""],
         [""Custom API Coordinator"", ""component"", ""The component that receives notifications and handles incoming messages from the airline system, and sends them to Power Apps""],
         [""Power Apps"", ""system"", ""The system where flights are assigned to Microsoft Teams channels""],
         [""Microsoft Teams Channels"", ""system"", ""The system where flights are assigned""],
         [""Azure Storage Account queue"", ""component"", ""The component where the Graph API call is queued for further processing""],
         [""Azure Functions"", ""component"", ""The component that runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams and streaming all events to an Azure Event Hubs""],
         [""Azure Event Hubs"", ""system"", ""The system where all events are streamed for further analytics""],
         [""Custom Bot Messaging Service"", ""component"", ""The entity that manages the airline's notification system""],
         [""Azure Bot Service"", ""component"", ""The component employed by the Custom Bot Messaging Service""],
         [""Teams"", ""system"", ""The system where flight updates are sent to users""],
         [""Azure Data Lake storage"", ""system"", ""The system that offers long-term retention and micro-batch processing of events from Event Hubs""],
         [""Power BI"", ""system"", ""The system that generates insightful reports with data from the Azure Data Lake storage""]
      ],
      ""relationships"": [
         [""Airline System"", ""communicates-with"", ""Custom API Hosted in Azure API Management""],
         [""Custom API Coordinator"", ""receives-notifications-from"", ""Airline System""],
         [""Custom API Coordinator"", ""sends-notifications-to"", ""Power Apps""],
         [""Power Apps"", ""assigns-flights-to"", ""Microsoft Teams Channels""],
         [""System"", ""queues-Graph-API-call"", ""Azure Storage Account queue""],
         [""Azure Functions"", ""runs-based-on-messages-in"", ""Azure Storage Account queue""],
         [""Azure Functions"", ""sends-notifications-to"", ""Teams""],
         [""Azure Functions"", ""streams-events-to"", ""Azure Event Hubs""],
         [""Custom Bot Messaging Service"", ""employs"", ""Azure Bot Service""],
         [""Custom Bots"", ""send-flight-updates-to"", ""Teams""],
         [""Azure Data Lake storage"", ""offers-long-term-retention-and-processing-for"", ""Azure Event Hubs""],
         [""Power BI"", ""generates-reports-with"", ""Azure Data Lake storage""]
      ]
   }"," {
      ""entities"": [
         [""Airline System"", ""system"", ""The entity that communicates with a custom API hosted in Azure API Management""],
         [""Custom API Hosted in Azure API Management"", ""component"", ""The component that the airline system communicates with""],
         [""Custom API Coordinator"", ""component"", ""The component that receives notifications and handles incoming messages from the airline system, and sends them to Power Apps""],
         [""Power Apps"", ""system"", ""The system where flights are assigned to Microsoft Teams channels""],
         [""Microsoft Teams Channels"", ""system"", ""The system where flights are assigned""],
         [""Azure Storage Account queue"", ""component"", ""The component where the Graph API call is queued for further processing""],
         [""Azure Functions"", ""component"", ""The component that runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams and streaming all events to an Azure Event Hubs""],
         [""Azure Event Hubs"", ""system"", ""The system where all events are streamed for further analytics""],
         [""Custom Bot Messaging Service"", ""component"", ""The entity that manages the airline's notification system""],
         [""Azure Bot Service"", ""component"", ""The component employed by the Custom Bot Messaging Service""],
         [""Teams"", ""system"", ""The system where flight updates are sent to users""],
         [""Azure Data Lake storage"", ""system"", ""The system that offers long-term retention and micro-batch processing of events from Event Hubs""],
         [""Power BI"", ""system"", ""The system that generates insightful reports with data from the Azure Data Lake storage""]
      ],
      ""relationships"": [
         [""Airline System"", ""communicates-with"", ""Custom API Hosted in Azure API Management""],
         [""Custom API Coordinator"", ""receives-notifications-from"", ""Airline System""],
         [""Custom API Coordinator"", ""sends-notifications-to"", ""Power Apps""],
         [""Power Apps"", ""assigns-flights-to"", ""Microsoft Teams Channels""],
         [""System"", ""queues-Graph-API-call"", ""Azure Storage Account queue""],
         [""Azure Functions"", ""runs-based-on-messages-in"", ""Azure Storage Account queue""],
         [""Azure Functions"", ""sends-notifications-to"", ""Teams""],
         [""Azure Functions"", ""streams-events-to"", ""Azure Event Hubs""],
         [""Custom Bot Messaging Service"", ""employs"", ""Azure Bot Service""],
         [""Custom Bots"", ""send-flight-updates-to"", ""Teams""],
         [""Azure Data Lake storage"", ""offers-long-term-retention-and-processing-for"", ""Azure Event Hubs""],
         [""Power BI"", ""generates-reports-with"", ""Azure Data Lake storage""]
      ]
   }",7.131935363053344
86,mistral:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}"," {
      ""entities"": [
         [""APIM"", ""system"", ""The gateway that receives POST requests from outside services and clients.""],
         [""CreatePatient function in Patient API"", ""component"", ""A software component responsible for creating a new patient record.""],
         [""Patient API"", ""system"", ""An application that manages patient records.""],
         [""Audit API CreateAuditRecord function"", ""component"", ""A software component responsible for logging events related to the creation of an audit record.""],
         [""Audit API"", ""system"", ""An application that handles auditing events.""],
         [""Azure Cosmos DB"", ""system"", ""A NoSQL database service provided by Microsoft Azure, where patient documents and audit records are stored.""],
         [""outside services and clients"", ""person"", ""Entities that make POST requests to APIM.""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient function in Patient API"", ""APIM calls the CreatePatient function with patient information received from outside services and clients.""],
         [""CreatePatient function in Patient API"", ""part-of"", ""Patient API"", ""The CreatePatient function is a part of the functionality provided by the Patient API.""],
         [""CreatePatient function in Patient API"", ""calls"", ""Audit API CreateAuditRecord function"", ""The CreatePatient function calls the Audit API CreateAuditRecord function to create an audit record.""],
         [""Audit API CreateAuditRecord function"", ""part-of"", ""Audit API"", ""The Audit API CreateAuditRecord function is a part of the functionality provided by the Audit API.""],
         [""Audit API CreateAuditRecord function"", ""contains"", ""Azure Cosmos DB"", ""The Audit API CreateAuditRecord function creates an audit record in Azure Cosmos DB.""],
         [""Patient API CreatePatient function"", ""contains"", ""Azure Cosmos DB"", ""The Patient API CreatePatient function creates the patient document in Azure Cosmos DB.""]
      ]
   }"," {
      ""entities"": [
         [""APIM"", ""system"", ""The gateway that receives POST requests from outside services and clients.""],
         [""CreatePatient function in Patient API"", ""component"", ""A software component responsible for creating a new patient record.""],
         [""Patient API"", ""system"", ""An application that manages patient records.""],
         [""Audit API CreateAuditRecord function"", ""component"", ""A software component responsible for logging events related to the creation of an audit record.""],
         [""Audit API"", ""system"", ""An application that handles auditing events.""],
         [""Azure Cosmos DB"", ""system"", ""A NoSQL database service provided by Microsoft Azure, where patient documents and audit records are stored.""],
         [""outside services and clients"", ""person"", ""Entities that make POST requests to APIM.""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient function in Patient API"", ""APIM calls the CreatePatient function with patient information received from outside services and clients.""],
         [""CreatePatient function in Patient API"", ""part-of"", ""Patient API"", ""The CreatePatient function is a part of the functionality provided by the Patient API.""],
         [""CreatePatient function in Patient API"", ""calls"", ""Audit API CreateAuditRecord function"", ""The CreatePatient function calls the Audit API CreateAuditRecord function to create an audit record.""],
         [""Audit API CreateAuditRecord function"", ""part-of"", ""Audit API"", ""The Audit API CreateAuditRecord function is a part of the functionality provided by the Audit API.""],
         [""Audit API CreateAuditRecord function"", ""contains"", ""Azure Cosmos DB"", ""The Audit API CreateAuditRecord function creates an audit record in Azure Cosmos DB.""],
         [""Patient API CreatePatient function"", ""contains"", ""Azure Cosmos DB"", ""The Patient API CreatePatient function creates the patient document in Azure Cosmos DB.""]
      ]
   }",5.147265334031545
87,mistral:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Azure""],
         [""Azure Storage account"", ""component"", ""An account used to store data in Azure Blob Storage""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool for interacting with Azure Storage""],
         [""Storage SDK or API"", ""component"", ""Software development kits (SDK) or application programming interfaces (APIs) for interacting with Azure Storage""],
         [""Azure AI Speech"", ""system"", ""An AI service provided by Azure for speech recognition""],
         [""Batch mode"", ""attribute"", ""A way of processing audio files asynchronously in Azure AI Speech""],
         [""speaker diarization"", ""attribute"", ""A feature of Azure AI Speech that identifies different speakers in an audio file""],
         [""transcription results"", ""component"", ""The text output generated by Azure AI Speech after transcription""],
         [""Azure AI Language"", ""system"", ""An AI service provided by Azure for detecting and redacting personal data""],
         [""Ingestion Client tool"", ""component"", ""A no-code tool for using Azure AI services in a batch mode""],
         [""call center transcription"", ""process"", ""The process of transcribing phone calls in a call center""],
         [""Azure OpenAI"", ""system"", ""An AI service provided by Azure for processing and analyzing text""],
         [""entities extraction"", ""attribute"", ""A feature of Azure OpenAI that extracts entities from text""],
         [""summarize the conversation"", ""attribute"", ""A feature of Azure OpenAI that generates a summary of a conversation""],
         [""sentiment analysis"", ""attribute"", ""A feature of Azure OpenAI that analyzes sentiments in text""],
         [""Power BI"", ""system"", ""A business analytics service by Microsoft for data visualization""],
         [""custom web application"", ""component"", ""A custom-made application for visualizing data""],
         [""App Service"", ""system"", ""A platform by Microsoft for hosting web applications""],
         [""Azure CRM"", ""system"", ""A customer relationship management system""],
         [""data store"", ""system"", ""A repository for storing metadata and other information""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage account""],
         [""Azure AI Speech"", ""calls"", ""Batch mode transcription""],
         [""Batch mode transcription"", ""part-of"", ""call center transcription""],
         [""call center transcription"", ""processes"", ""audio files""],
         [""transcription results"", ""stored in"", ""Azure Blob Storage""],
         [""Azure AI Language"", ""calls"", ""personal data detection and redaction""],
         [""personal data detection and redataction"", ""part-of"", ""call center transcription""],
         [""Ingestion Client tool"", ""uses"", ""batch mode transcription""],
         [""Ingestion Client tool"", ""uses"", ""Azure AI Language""],
         [""Azure OpenAI"", ""calls"", ""entities extraction""],
         [""Azure OpenAI"", ""calls"", ""summarize the conversation""],
         [""Azure OpenAI"", ""calls"", ""sentiment analysis""],
         [""Power BI"", ""visualizes"", ""output of Azure OpenAI""],
         [""custom web application"", ""hosted by"", ""App Service""],
         [""Azure CRM"", ""stores"", ""processed output""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Azure""],
         [""Azure Storage account"", ""component"", ""An account used to store data in Azure Blob Storage""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool for interacting with Azure Storage""],
         [""Storage SDK or API"", ""component"", ""Software development kits (SDK) or application programming interfaces (APIs) for interacting with Azure Storage""],
         [""Azure AI Speech"", ""system"", ""An AI service provided by Azure for speech recognition""],
         [""Batch mode"", ""attribute"", ""A way of processing audio files asynchronously in Azure AI Speech""],
         [""speaker diarization"", ""attribute"", ""A feature of Azure AI Speech that identifies different speakers in an audio file""],
         [""transcription results"", ""component"", ""The text output generated by Azure AI Speech after transcription""],
         [""Azure AI Language"", ""system"", ""An AI service provided by Azure for detecting and redacting personal data""],
         [""Ingestion Client tool"", ""component"", ""A no-code tool for using Azure AI services in a batch mode""],
         [""call center transcription"", ""process"", ""The process of transcribing phone calls in a call center""],
         [""Azure OpenAI"", ""system"", ""An AI service provided by Azure for processing and analyzing text""],
         [""entities extraction"", ""attribute"", ""A feature of Azure OpenAI that extracts entities from text""],
         [""summarize the conversation"", ""attribute"", ""A feature of Azure OpenAI that generates a summary of a conversation""],
         [""sentiment analysis"", ""attribute"", ""A feature of Azure OpenAI that analyzes sentiments in text""],
         [""Power BI"", ""system"", ""A business analytics service by Microsoft for data visualization""],
         [""custom web application"", ""component"", ""A custom-made application for visualizing data""],
         [""App Service"", ""system"", ""A platform by Microsoft for hosting web applications""],
         [""Azure CRM"", ""system"", ""A customer relationship management system""],
         [""data store"", ""system"", ""A repository for storing metadata and other information""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage account""],
         [""Azure AI Speech"", ""calls"", ""Batch mode transcription""],
         [""Batch mode transcription"", ""part-of"", ""call center transcription""],
         [""call center transcription"", ""processes"", ""audio files""],
         [""transcription results"", ""stored in"", ""Azure Blob Storage""],
         [""Azure AI Language"", ""calls"", ""personal data detection and redaction""],
         [""personal data detection and redataction"", ""part-of"", ""call center transcription""],
         [""Ingestion Client tool"", ""uses"", ""batch mode transcription""],
         [""Ingestion Client tool"", ""uses"", ""Azure AI Language""],
         [""Azure OpenAI"", ""calls"", ""entities extraction""],
         [""Azure OpenAI"", ""calls"", ""summarize the conversation""],
         [""Azure OpenAI"", ""calls"", ""sentiment analysis""],
         [""Power BI"", ""visualizes"", ""output of Azure OpenAI""],
         [""custom web application"", ""hosted by"", ""App Service""],
         [""Azure CRM"", ""stores"", ""processed output""]
      ]
   }",8.38542081398191
88,mistral:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}"," {
      ""entities"": [
         [""Connect Agent"", ""component"", ""Captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage""],
         [""Publisher (Mainframe)"", ""component"", ""Manages data migration on mainframe systems, moves the data from on-premises to Azure via an enhanced-security connection, handles the commit and rollback of transactions for each unit of work, maintains the integrity of data, calls Connect Replicator Engine""],
         [""Listener (Midrange)"", ""component"", ""Manages data migration for midrange systems, located on either a Windows or Linux machine, moves the data from on-premises to Azure via an enhanced-security connection, handles the commit and rollback of transactions for each unit of work, maintains the integrity of data, calls Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target, distributes data for parallel processing""],
         [""Target"", ""system"", ""Receives the changes via ODBC or ingests the changes via Azure Event Hubs""],
         [""Azure Databricks"", ""component"", ""Consumes the changed data and applies it to Azure data platform services""],
         [""Connect Controller Daemon"", ""component"", ""Authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine""]
      ],
      ""relationships"": [
         [""Connect Agent"", ""part-of"", ""System"", ""It's a component of the overall system""],
         [""Publisher (Mainframe)"", ""calls"", ""Connect Replicator Engine"", ""It initiates a connection to the Connect Replicator Engine""],
         [""Listener (Midrange)"", ""calls"", ""Connect Replicator Engine"", ""It initiates a connection to the Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""part-of"", ""System"", ""It's a component of the overall system""]
      ]
   }"," {
      ""entities"": [
         [""Connect Agent"", ""component"", ""Captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage""],
         [""Publisher (Mainframe)"", ""component"", ""Manages data migration on mainframe systems, moves the data from on-premises to Azure via an enhanced-security connection, handles the commit and rollback of transactions for each unit of work, maintains the integrity of data, calls Connect Replicator Engine""],
         [""Listener (Midrange)"", ""component"", ""Manages data migration for midrange systems, located on either a Windows or Linux machine, moves the data from on-premises to Azure via an enhanced-security connection, handles the commit and rollback of transactions for each unit of work, maintains the integrity of data, calls Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target, distributes data for parallel processing""],
         [""Target"", ""system"", ""Receives the changes via ODBC or ingests the changes via Azure Event Hubs""],
         [""Azure Databricks"", ""component"", ""Consumes the changed data and applies it to Azure data platform services""],
         [""Connect Controller Daemon"", ""component"", ""Authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine""]
      ],
      ""relationships"": [
         [""Connect Agent"", ""part-of"", ""System"", ""It's a component of the overall system""],
         [""Publisher (Mainframe)"", ""calls"", ""Connect Replicator Engine"", ""It initiates a connection to the Connect Replicator Engine""],
         [""Listener (Midrange)"", ""calls"", ""Connect Replicator Engine"", ""It initiates a connection to the Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""part-of"", ""System"", ""It's a component of the overall system""]
      ]
   }",5.108044638007414
89,mistral:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A system used for orchestration and integration with various services.""],
         [""API Management"", ""system"", ""A system consisting of two components: API gateway and developer portal.""],
         [""Azure API Gateway"", ""component"", ""Component of the API Management system. Accepts HTTP calls and routes them to the backend.""],
         [""Developer Portal"", ""component"", ""Component of the API Management system. Provides access to documentation, code samples, and APIs testing for developers.""],
         [""Azure DNS"", ""system"", ""A system provided by Azure for name resolution using the Azure infrastructure.""],
         [""Custom Domain Name"", ""component"", ""Part of Azure DNS system that maps a custom domain name to an IP address.""],
         [""Microsoft Entra ID"", ""system"", ""System used for client authentication when calling API gateway, supporting the OpenID Connect protocol.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""contains"", ""API Management""],
         [""API Management"", ""part-of"", ""Azure API Gateway""],
         [""API Management"", ""part-of"", ""Developer Portal""],
         [""Azure DNS"", ""part-of"", ""Custom Domain Name""],
         [""API Management"", ""calls"", ""Microsoft Entra ID""]
      ]
   }"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A system used for orchestration and integration with various services.""],
         [""API Management"", ""system"", ""A system consisting of two components: API gateway and developer portal.""],
         [""Azure API Gateway"", ""component"", ""Component of the API Management system. Accepts HTTP calls and routes them to the backend.""],
         [""Developer Portal"", ""component"", ""Component of the API Management system. Provides access to documentation, code samples, and APIs testing for developers.""],
         [""Azure DNS"", ""system"", ""A system provided by Azure for name resolution using the Azure infrastructure.""],
         [""Custom Domain Name"", ""component"", ""Part of Azure DNS system that maps a custom domain name to an IP address.""],
         [""Microsoft Entra ID"", ""system"", ""System used for client authentication when calling API gateway, supporting the OpenID Connect protocol.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""contains"", ""API Management""],
         [""API Management"", ""part-of"", ""Azure API Gateway""],
         [""API Management"", ""part-of"", ""Developer Portal""],
         [""Azure DNS"", ""part-of"", ""Custom Domain Name""],
         [""API Management"", ""calls"", ""Microsoft Entra ID""]
      ]
   }",3.756548872042913
90,mistral:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}"," {
      ""entities"": [
         [""CDS"", ""system"", ""Cloud Data Storage""],
         [""SQL"", ""system"", ""Structured Query Language Database""],
         [""Data Factory"", ""component"", ""Data Integration Service""],
         [""ERP System"", ""component"", ""Enterprise Resource Planning System""],
         [""Azure SQL Database"", ""component"", ""Database Service on Microsoft Azure""],
         [""Azure Functions"", ""component"", ""Event-driven serverless computing service on Microsoft Azure""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Platform for creating business applications without coding""],
         [""Purchase Order (PO)"", ""component"", ""Document used to order goods or services from a supplier""],
         [""User"", ""person"", ""Human interacting with the software system""],
         [""CSV export"", ""component"", ""Comma-Separated Values File Export Function""],
         [""Power BI"", ""system"", ""Business Intelligence Service""]
      ],
      ""relationships"": [
         [""CDS"", ""part-of"", ""SQL""],
         [""Data Factory"", ""part-of"", ""CDS""],
         [""ERP System"", ""calls"", ""Azure SQL Database""],
         [""Azure Functions"", ""uses"", ""API""],
         [""Azure Functions"", ""creates"", [""task"", ""component""]],
         [""Power Apps"", ""retrieves data from"", ""Azure SQL Database""],
         [""User"", ""updates POs in"", ""Power Apps""],
         [""Power Apps"", ""sends data to suppliers through"", ""CSV export""],
         [""Power BI"", ""reports trends in"", ""supplier status""]
      ]
   }"," {
      ""entities"": [
         [""CDS"", ""system"", ""Cloud Data Storage""],
         [""SQL"", ""system"", ""Structured Query Language Database""],
         [""Data Factory"", ""component"", ""Data Integration Service""],
         [""ERP System"", ""component"", ""Enterprise Resource Planning System""],
         [""Azure SQL Database"", ""component"", ""Database Service on Microsoft Azure""],
         [""Azure Functions"", ""component"", ""Event-driven serverless computing service on Microsoft Azure""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Platform for creating business applications without coding""],
         [""Purchase Order (PO)"", ""component"", ""Document used to order goods or services from a supplier""],
         [""User"", ""person"", ""Human interacting with the software system""],
         [""CSV export"", ""component"", ""Comma-Separated Values File Export Function""],
         [""Power BI"", ""system"", ""Business Intelligence Service""]
      ],
      ""relationships"": [
         [""CDS"", ""part-of"", ""SQL""],
         [""Data Factory"", ""part-of"", ""CDS""],
         [""ERP System"", ""calls"", ""Azure SQL Database""],
         [""Azure Functions"", ""uses"", ""API""],
         [""Azure Functions"", ""creates"", [""task"", ""component""]],
         [""Power Apps"", ""retrieves data from"", ""Azure SQL Database""],
         [""User"", ""updates POs in"", ""Power Apps""],
         [""Power Apps"", ""sends data to suppliers through"", ""CSV export""],
         [""Power BI"", ""reports trends in"", ""supplier status""]
      ]
   }",4.414017747039907
91,mistral:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}"," {
      ""entities"": [
         [""GitHub Repository"", ""system"", ""A platform used by developers to check in their code""],
         [""CI/CD pipeline agent"", ""component"", ""Software component responsible for building and deploying the application""],
         [""Azure VM"", ""component"", ""Virtual machine running the CI/CD pipeline agent""],
         [""API application"", ""component"", ""Application hosting the APIs""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment""],
         [""Azure API Management"", ""system"", ""System used to manage and publish APIs""],
         [""HOST headers"", ""component"", ""Headers used in HTTP requests to identify the host server""],
         [""API Management policy"", ""component"", ""Policy applied on the APIs by Azure API Management""],
         [""App Service Environment's DNS name"", ""system"", ""DNS name of the App Service Environment hosting the APIs""],
         [""Application Gateway"", ""system"", ""System used to expose the API Management and its portal""],
         [""Azure Private DNS"", ""component"", ""Service used for internal routing of traffic""],
         [""External users"", ""person"", ""Users accessing the APIs from outside the system""]
      ],
      ""relationships"": [
         [""GitHub Repository"", ""part-of"", ""CI/CD pipeline agent""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
         [""Azure VM"", ""contains"", ""API application""],
         [""API application"", ""calls"", ""Azure API Management""],
         [""Azure API Management"", ""uses"", ""HOST headers""],
         [""Azure API Management"", ""uses"", ""App Service Environment's DNS name""],
         [""Application Gateway"", ""exposes"", ""API Management""],
         [""Azure Private DNS"", ""routes"", ""traffic between ASE, API Management, and Application Gateway""],
         [""External users"", ""utilizes"", ""the exposed developer portal to consume the APIs via Application Gateway's public IP""]
      ]
   }"," {
      ""entities"": [
         [""GitHub Repository"", ""system"", ""A platform used by developers to check in their code""],
         [""CI/CD pipeline agent"", ""component"", ""Software component responsible for building and deploying the application""],
         [""Azure VM"", ""component"", ""Virtual machine running the CI/CD pipeline agent""],
         [""API application"", ""component"", ""Application hosting the APIs""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment""],
         [""Azure API Management"", ""system"", ""System used to manage and publish APIs""],
         [""HOST headers"", ""component"", ""Headers used in HTTP requests to identify the host server""],
         [""API Management policy"", ""component"", ""Policy applied on the APIs by Azure API Management""],
         [""App Service Environment's DNS name"", ""system"", ""DNS name of the App Service Environment hosting the APIs""],
         [""Application Gateway"", ""system"", ""System used to expose the API Management and its portal""],
         [""Azure Private DNS"", ""component"", ""Service used for internal routing of traffic""],
         [""External users"", ""person"", ""Users accessing the APIs from outside the system""]
      ],
      ""relationships"": [
         [""GitHub Repository"", ""part-of"", ""CI/CD pipeline agent""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
         [""Azure VM"", ""contains"", ""API application""],
         [""API application"", ""calls"", ""Azure API Management""],
         [""Azure API Management"", ""uses"", ""HOST headers""],
         [""Azure API Management"", ""uses"", ""App Service Environment's DNS name""],
         [""Application Gateway"", ""exposes"", ""API Management""],
         [""Azure Private DNS"", ""routes"", ""traffic between ASE, API Management, and Application Gateway""],
         [""External users"", ""utilizes"", ""the exposed developer portal to consume the APIs via Application Gateway's public IP""]
      ]
   }",5.120210222026799
92,mistral:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""The file containing multiple documents of various types is stored here.""],
         [""Storage Queue"", ""component"", ""The web app adds a command message to this queue to initiate pipeline processing.""],
         [""Durable Functions orchestration"", ""system"", ""This is triggered by the command message.""],
         [""Scan activity function"", ""component"", ""This function calls the Computer Vision Read API.""],
         [""Computer Vision Read API"", ""service"", ""This API is called by the Scan activity function.""],
         [""OCR results"", ""data"", ""These are returned to the orchestration to be used by subsequent activities.""],
         [""Classify activity function"", ""component"", ""This function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""This is where the document classifier service is hosted.""],
         [""document classifier service"", ""service"", ""This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file.""],
         [""Azure Cosmos DB store"", ""system"", ""The Metadata Store activity function saves the document type and page range information here.""],
         [""Cognitive Search service"", ""service"", ""The Indexing activity function creates a new search document in this service for each identified document type.""],
         [""Azure AI Search libraries for .NET"", ""tool"", ""These are used by the Indexing activity function to include the full OCR results and document information in the search document.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""The document file""],
         [""Storage Queue"", ""part-of"", ""The pipeline processing system""],
         [""Durable Functions orchestration"", ""calls"", ""The Scan activity function""],
         [""Scan activity function"", ""calls"", ""The Computer Vision Read API""],
         [""Classify activity function"", ""calls"", ""The document classifier service""],
         [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS)""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""The file containing multiple documents of various types is stored here.""],
         [""Storage Queue"", ""component"", ""The web app adds a command message to this queue to initiate pipeline processing.""],
         [""Durable Functions orchestration"", ""system"", ""This is triggered by the command message.""],
         [""Scan activity function"", ""component"", ""This function calls the Computer Vision Read API.""],
         [""Computer Vision Read API"", ""service"", ""This API is called by the Scan activity function.""],
         [""OCR results"", ""data"", ""These are returned to the orchestration to be used by subsequent activities.""],
         [""Classify activity function"", ""component"", ""This function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""This is where the document classifier service is hosted.""],
         [""document classifier service"", ""service"", ""This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file.""],
         [""Azure Cosmos DB store"", ""system"", ""The Metadata Store activity function saves the document type and page range information here.""],
         [""Cognitive Search service"", ""service"", ""The Indexing activity function creates a new search document in this service for each identified document type.""],
         [""Azure AI Search libraries for .NET"", ""tool"", ""These are used by the Indexing activity function to include the full OCR results and document information in the search document.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""The document file""],
         [""Storage Queue"", ""part-of"", ""The pipeline processing system""],
         [""Durable Functions orchestration"", ""calls"", ""The Scan activity function""],
         [""Scan activity function"", ""calls"", ""The Computer Vision Read API""],
         [""Classify activity function"", ""calls"", ""The document classifier service""],
         [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS)""]
      ]
   }",5.651704679010436
93,mistral:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps.""],
         [""Logic Apps"", ""component"", ""Uses secured template parameters to retrieve database credentials from Azure Key Vault, uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
         [""Azure Key Vault"", ""component"", ""Stores the database credentials.""],
         [""Transport Layer Security (TLS)"", ""system"", ""Secures the connection between Logic Apps and the on-premises data gateway.""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database to run the statement.""],
         [""SQL Server"", ""system"", ""Stores the data and makes it available to apps that users access.""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance.""]
      ],
      ""relationships"": [
         [""API Management"", ""calls"", ""Logic Apps""],
         [""Logic Apps"", ""part-of"", ""Transport Layer Security (TLS)""],
         [""Logic Apps"", ""uses"", ""Azure Key Vault""],
         [""Logic Apps"", ""sends"", [""on-premises data gateway"", ""database credentials and a database statement""]],
         [""on-premises data gateway"", ""connects-to"", ""SQL Server""],
         [""SQL Server"", ""provides-data-for"", [""apps that users access""]]
      ]
   }"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps.""],
         [""Logic Apps"", ""component"", ""Uses secured template parameters to retrieve database credentials from Azure Key Vault, uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
         [""Azure Key Vault"", ""component"", ""Stores the database credentials.""],
         [""Transport Layer Security (TLS)"", ""system"", ""Secures the connection between Logic Apps and the on-premises data gateway.""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database to run the statement.""],
         [""SQL Server"", ""system"", ""Stores the data and makes it available to apps that users access.""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance.""]
      ],
      ""relationships"": [
         [""API Management"", ""calls"", ""Logic Apps""],
         [""Logic Apps"", ""part-of"", ""Transport Layer Security (TLS)""],
         [""Logic Apps"", ""uses"", ""Azure Key Vault""],
         [""Logic Apps"", ""sends"", [""on-premises data gateway"", ""database credentials and a database statement""]],
         [""on-premises data gateway"", ""connects-to"", ""SQL Server""],
         [""SQL Server"", ""provides-data-for"", [""apps that users access""]]
      ]
   }",4.265050494985189
94,mistral:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""Load balancer service that routes traffic to Azure AD B2C for authentication.""],
         [""Azure AD B2C"", ""system"", ""Identity and Access Management system used for user authentication.""],
         [""Custom domain"", ""component"", ""A specific domain configured for sign-in in Azure AD B2C.""],
         [""JSON Web Token (JWT)"", ""component"", ""A compact, self-contained data structure used to pass claims securely between parties.""],
         [""API Management"", ""system"", ""An API management service that provides developers a way to publish, manage and monitor APIs.""],
         [""validate-jwt policy"", ""component"", ""A policy in API Management used to validate the JWT token.""],
         [""Backend API"", ""component"", ""The specific API being accessed by the client.""],
         [""App Service Environment"", ""component"", ""An isolated environment for running web apps and APIs in Azure.""],
         [""Function Apps"", ""component"", ""A serverless compute service used to run small pieces of code or functions in response to an event.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""A managed container orchestration service for deploying, scaling and managing containerized applications.""],
         [""API Management backend"", ""component"", ""The back end of API Management that maps requests to the appropriate backend APIs.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""Azure AD B2C""],
         [""Azure Front Door"", ""calls"", ""API Management""],
         [""API Management"", ""part-of"", ""validate-jwt policy""],
         [""API Management"", ""part-of"", ""Backend API""],
         [""Backend API"", ""contains"", ""App Service Environment""],
         [""Backend API"", ""contains"", ""Function Apps""],
         [""Backend API"", ""contains"", ""Azure Kubernetes Services (AKS)""]
      ]
   }"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""Load balancer service that routes traffic to Azure AD B2C for authentication.""],
         [""Azure AD B2C"", ""system"", ""Identity and Access Management system used for user authentication.""],
         [""Custom domain"", ""component"", ""A specific domain configured for sign-in in Azure AD B2C.""],
         [""JSON Web Token (JWT)"", ""component"", ""A compact, self-contained data structure used to pass claims securely between parties.""],
         [""API Management"", ""system"", ""An API management service that provides developers a way to publish, manage and monitor APIs.""],
         [""validate-jwt policy"", ""component"", ""A policy in API Management used to validate the JWT token.""],
         [""Backend API"", ""component"", ""The specific API being accessed by the client.""],
         [""App Service Environment"", ""component"", ""An isolated environment for running web apps and APIs in Azure.""],
         [""Function Apps"", ""component"", ""A serverless compute service used to run small pieces of code or functions in response to an event.""],
         [""Azure Kubernetes Services (AKS)"", ""component"", ""A managed container orchestration service for deploying, scaling and managing containerized applications.""],
         [""API Management backend"", ""component"", ""The back end of API Management that maps requests to the appropriate backend APIs.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""Azure AD B2C""],
         [""Azure Front Door"", ""calls"", ""API Management""],
         [""API Management"", ""part-of"", ""validate-jwt policy""],
         [""API Management"", ""part-of"", ""Backend API""],
         [""Backend API"", ""contains"", ""App Service Environment""],
         [""Backend API"", ""contains"", ""Function Apps""],
         [""Backend API"", ""contains"", ""Azure Kubernetes Services (AKS)""]
      ]
   }",4.902906251954846
95,mistral:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}"," {
      ""entities"": [
         [""Outlook email account"", ""person"", ""A designated Outlook email account that receives PDF files as attachments""],
         [""Logic app"", ""component"", ""A logic app built using the capabilities of Azure Logic Apps that processes emails and uploads PDF files to a container in Azure Data Lake Storage""],
         [""Azure Data Lake Storage"", ""system"", ""A system for storing data in Azure that contains three containers, one for PDF files, another for split pages, and another for JSON data files""],
         [""Function app"", ""component"", ""A function app built using the capabilities of Azure Functions that processes single-page PDF files by sending them to AI Document Intelligence for processing""],
         [""AI Document Intelligence"", ""system"", ""An Azure REST API that processes PDF files and sends responses back to the function app""],
         [""Azure Cosmos DB"", ""system"", ""A system in Azure that saves data from the processed forms in a database and collections""]
      ],
      ""relationships"": [
         [""Outlook email account"", ""calls"", ""Logic app"", ""The arrival of an email triggers the logic app to process the email""],
         [""Logic app"", ""part-of"", ""Azure Data Lake Storage"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""],
         [""PDF file"", ""part-of"", ""Azure Data Lake Storage (PDF container)"", ""PDF files are manually or programmatically uploaded to this container""],
         [""PDF file"", ""calls"", ""Logic app (forms processing)"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""],
         [""Function app"", ""part-of"", ""Azure Data Lake Storage (second container)"", ""Split files are saved to this second container in Data Lake Storage""],
         [""Function app"", ""calls"", ""AI Document Intelligence"", ""The function app sends the location of the single-page PDF file to AI Document Intelligence for processing""],
         [""Function app"", ""part-of"", ""Azure Data Lake Storage (third container)"", ""The created JSON data file is stored in this third container in Data Lake Storage""],
         [""forms processing logic app"", ""receives"", ""processed response data"", ""The forms processing logic app receives the processed response data""],
         [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""],
         [""Azure Cosmos DB"", ""contains"", ""data structure"", ""The data is saved in a database and in collections in Azure Cosmos DB""]
      ]
   }"," {
      ""entities"": [
         [""Outlook email account"", ""person"", ""A designated Outlook email account that receives PDF files as attachments""],
         [""Logic app"", ""component"", ""A logic app built using the capabilities of Azure Logic Apps that processes emails and uploads PDF files to a container in Azure Data Lake Storage""],
         [""Azure Data Lake Storage"", ""system"", ""A system for storing data in Azure that contains three containers, one for PDF files, another for split pages, and another for JSON data files""],
         [""Function app"", ""component"", ""A function app built using the capabilities of Azure Functions that processes single-page PDF files by sending them to AI Document Intelligence for processing""],
         [""AI Document Intelligence"", ""system"", ""An Azure REST API that processes PDF files and sends responses back to the function app""],
         [""Azure Cosmos DB"", ""system"", ""A system in Azure that saves data from the processed forms in a database and collections""]
      ],
      ""relationships"": [
         [""Outlook email account"", ""calls"", ""Logic app"", ""The arrival of an email triggers the logic app to process the email""],
         [""Logic app"", ""part-of"", ""Azure Data Lake Storage"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""],
         [""PDF file"", ""part-of"", ""Azure Data Lake Storage (PDF container)"", ""PDF files are manually or programmatically uploaded to this container""],
         [""PDF file"", ""calls"", ""Logic app (forms processing)"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""],
         [""Function app"", ""part-of"", ""Azure Data Lake Storage (second container)"", ""Split files are saved to this second container in Data Lake Storage""],
         [""Function app"", ""calls"", ""AI Document Intelligence"", ""The function app sends the location of the single-page PDF file to AI Document Intelligence for processing""],
         [""Function app"", ""part-of"", ""Azure Data Lake Storage (third container)"", ""The created JSON data file is stored in this third container in Data Lake Storage""],
         [""forms processing logic app"", ""receives"", ""processed response data"", ""The forms processing logic app receives the processed response data""],
         [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""],
         [""Azure Cosmos DB"", ""contains"", ""data structure"", ""The data is saved in a database and in collections in Azure Cosmos DB""]
      ]
   }",6.268638189008925
96,mistral:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}"," {
      ""entities"": [
         [""Connect Agent Component"", ""component"", ""Captures change logs using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""Publisher Component (Mainframe)"", ""component"", ""Manages data migration for mainframe systems.""],
         [""Listener Component (Midrange)"", ""component"", ""Manages data migration for midrange systems. It's located on either a Windows or Linux machine.""],
         [""Enhanced-Security Connection"", ""system"", ""The connection used by the publisher or listener to move data from on-premises to Azure.""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.""],
         [""Target Database"", ""system"", ""Receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""system"", ""Consumes the changed data and applies it to Azure data platform services.""]
      ],
      ""relationships"": [
         [""Connect Agent Component"", ""part-of"", ""Publisher Component (Mainframe)""],
         [""Connect Agent Component"", ""part-of"", ""Listener Component (Midrange)""],
         [""Publisher Component (Mainframe)"", ""contains"", ""Enhanced-Security Connection""],
         [""Listener Component (Midrange)"", ""contains"", ""Enhanced-Security Connection""],
         [""Publisher Component (Mainframe)"", ""calls"", ""Connect Replicator Engine""],
         [""Listener Component (Midrange)"", ""calls"", ""Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""part-of"", ""Target Database""],
         [""Target Database"", ""called-from"", ""Azure Databricks""]
      ]
   }"," {
      ""entities"": [
         [""Connect Agent Component"", ""component"", ""Captures change logs using mainframe or midrange native utilities and caches the logs in temporary storage.""],
         [""Publisher Component (Mainframe)"", ""component"", ""Manages data migration for mainframe systems.""],
         [""Listener Component (Midrange)"", ""component"", ""Manages data migration for midrange systems. It's located on either a Windows or Linux machine.""],
         [""Enhanced-Security Connection"", ""system"", ""The connection used by the publisher or listener to move data from on-premises to Azure.""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.""],
         [""Target Database"", ""system"", ""Receives the changes via ODBC or ingests the changes via Azure Event Hubs.""],
         [""Azure Databricks"", ""system"", ""Consumes the changed data and applies it to Azure data platform services.""]
      ],
      ""relationships"": [
         [""Connect Agent Component"", ""part-of"", ""Publisher Component (Mainframe)""],
         [""Connect Agent Component"", ""part-of"", ""Listener Component (Midrange)""],
         [""Publisher Component (Mainframe)"", ""contains"", ""Enhanced-Security Connection""],
         [""Listener Component (Midrange)"", ""contains"", ""Enhanced-Security Connection""],
         [""Publisher Component (Mainframe)"", ""calls"", ""Connect Replicator Engine""],
         [""Listener Component (Midrange)"", ""calls"", ""Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""part-of"", ""Target Database""],
         [""Target Database"", ""called-from"", ""Azure Databricks""]
      ]
   }",4.570363790960982
97,mistral:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""The document file is stored here.""],
         [""Command Message"", ""component"", ""The web app adds this to a storage queue to initiate pipeline processing.""],
         [""Storage Queue"", ""system"", ""The command message is added to this for processing.""],
         [""Durable Functions Orchestration"", ""system"", ""This is triggered by the command message.""],
         [""Document File"", ""component"", ""This is uploaded by the user and contains multiple documents of various types.""],
         [""PDF or multipage TIFF file"", ""description"", ""The document file can be of these types.""],
         [""Scan activity function"", ""component"", ""This calls the Computer Vision Read API.""],
         [""Computer Vision Read API"", ""service"", ""This is called by the Scan activity function for OCR processing.""],
         [""Optical character recognition (OCR) results"", ""result"", ""These are returned to the orchestration by the Computer Vision Read API.""],
         [""Classify activity function"", ""component"", ""This calls the document classifier service hosted in Azure Kubernetes Service (AKS).""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""The document classifier service is hosted here.""],
         [""Document Classifier Service"", ""service"", ""This identifies the starting page of each known document and calculates how many document types are contained in the document file.""],
         [""Metadata Store activity function"", ""component"", ""This saves the document type and page range information in an Azure Cosmos DB store.""],
         [""Azure Cosmos DB"", ""system"", ""The metadata is stored here.""],
         [""Indexing activity function"", ""component"", ""This creates a new search document in Cognitive Search service for each identified document type.""],
         [""Cognitive Search service"", ""system"", ""This is used to index the documents.""],
         [""Azure AI Search libraries for .NET"", ""tool"", ""These are used to include the full OCR results and document information in the search document.""],
         [""Correlation ID"", ""identifier"", ""This is added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.""],
         [""End users"", ""user"", ""They can search for documents by contents and metadata.""]
      ],
      ""relationships"": [
         [""Command Message"", ""part-of"", ""Storage Queue""],
         [""Storage Queue"", ""contains"", ""Command Message""],
         [""Durable Functions Orchestration"", ""part-of"", ""Each Durable Functions instance""],
         [""Document File"", ""part-of"", ""Azure Blob Storage""],
         [""Scan activity function"", ""calls"", ""Computer Vision Read API""],
         [""Classify activity function"", ""called-from"", ""Document Classifier Service""],
         [""Metadata Store activity function"", ""stores"", ""Information in Azure Cosmos DB""],
         [""Indexing activity function"", ""creates"", ""Search document in Cognitive Search service""],
         [""Cognitive Search service"", ""includes"", ""Full OCR results and document information from Azure AI Search libraries for .NET""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""The document file is stored here.""],
         [""Command Message"", ""component"", ""The web app adds this to a storage queue to initiate pipeline processing.""],
         [""Storage Queue"", ""system"", ""The command message is added to this for processing.""],
         [""Durable Functions Orchestration"", ""system"", ""This is triggered by the command message.""],
         [""Document File"", ""component"", ""This is uploaded by the user and contains multiple documents of various types.""],
         [""PDF or multipage TIFF file"", ""description"", ""The document file can be of these types.""],
         [""Scan activity function"", ""component"", ""This calls the Computer Vision Read API.""],
         [""Computer Vision Read API"", ""service"", ""This is called by the Scan activity function for OCR processing.""],
         [""Optical character recognition (OCR) results"", ""result"", ""These are returned to the orchestration by the Computer Vision Read API.""],
         [""Classify activity function"", ""component"", ""This calls the document classifier service hosted in Azure Kubernetes Service (AKS).""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""The document classifier service is hosted here.""],
         [""Document Classifier Service"", ""service"", ""This identifies the starting page of each known document and calculates how many document types are contained in the document file.""],
         [""Metadata Store activity function"", ""component"", ""This saves the document type and page range information in an Azure Cosmos DB store.""],
         [""Azure Cosmos DB"", ""system"", ""The metadata is stored here.""],
         [""Indexing activity function"", ""component"", ""This creates a new search document in Cognitive Search service for each identified document type.""],
         [""Cognitive Search service"", ""system"", ""This is used to index the documents.""],
         [""Azure AI Search libraries for .NET"", ""tool"", ""These are used to include the full OCR results and document information in the search document.""],
         [""Correlation ID"", ""identifier"", ""This is added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.""],
         [""End users"", ""user"", ""They can search for documents by contents and metadata.""]
      ],
      ""relationships"": [
         [""Command Message"", ""part-of"", ""Storage Queue""],
         [""Storage Queue"", ""contains"", ""Command Message""],
         [""Durable Functions Orchestration"", ""part-of"", ""Each Durable Functions instance""],
         [""Document File"", ""part-of"", ""Azure Blob Storage""],
         [""Scan activity function"", ""calls"", ""Computer Vision Read API""],
         [""Classify activity function"", ""called-from"", ""Document Classifier Service""],
         [""Metadata Store activity function"", ""stores"", ""Information in Azure Cosmos DB""],
         [""Indexing activity function"", ""creates"", ""Search document in Cognitive Search service""],
         [""Cognitive Search service"", ""includes"", ""Full OCR results and document information from Azure AI Search libraries for .NET""]
      ]
   }",7.868242902040947
98,mistral:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps.""],
         [""Logic Apps"", ""component"", ""Uses secured template parameters to retrieve database credentials from Azure Key Vault and uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
         [""Azure Key Vault"", ""component"", ""Stores the database credentials.""],
         [""Transport Layer Security (TLS)"", ""component"", ""Used for secure communication between Logic Apps and the on-premises data gateway.""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database to run the statement.""],
         [""SQL Server"", ""system"", ""Stores the data and makes it available to apps that users access.""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance.""]
      ],
      ""relationships"": [
         [""API Management"", ""calls"", ""Logic Apps""],
         [""Logic Apps"", ""part-of"", ""Azure Monitor""],
         [""Logic Apps"", ""contains"", ""Transport Layer Security (TLS)""],
         [""Logic Apps"", ""part-of"", ""Azure Key Vault""],
         [""on-premises data gateway"", ""part-of"", ""SQL Server""]
      ]
   }"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests and securely routes them to Logic Apps.""],
         [""Logic Apps"", ""component"", ""Uses secured template parameters to retrieve database credentials from Azure Key Vault and uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.""],
         [""Azure Key Vault"", ""component"", ""Stores the database credentials.""],
         [""Transport Layer Security (TLS)"", ""component"", ""Used for secure communication between Logic Apps and the on-premises data gateway.""],
         [""on-premises data gateway"", ""component"", ""Connects to a SQL Server database to run the statement.""],
         [""SQL Server"", ""system"", ""Stores the data and makes it available to apps that users access.""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance.""]
      ],
      ""relationships"": [
         [""API Management"", ""calls"", ""Logic Apps""],
         [""Logic Apps"", ""part-of"", ""Azure Monitor""],
         [""Logic Apps"", ""contains"", ""Transport Layer Security (TLS)""],
         [""Logic Apps"", ""part-of"", ""Azure Key Vault""],
         [""on-premises data gateway"", ""part-of"", ""SQL Server""]
      ]
   }",3.9154330299934372
99,mistral:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Azure where audio files and transcriptions are stored.""],
         [""Azure Storage Account"", ""component"", ""An account within the Azure Blob Storage system.""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool for managing Azure Storage accounts.""],
         [""Storage SDK or API"", ""component"", ""Software Development Kits or APIs for uploading audio files to Azure Storage.""],
         [""Azure AI Speech"", ""system"", ""An Azure service used for speech recognition and transcription.""],
         [""Batch mode"", ""descriptor"", ""A mode in which a large amount of data is processed asynchronously by the system.""],
         [""Speaker diarization"", ""descriptor"", ""The process of identifying speakers in a conversation based on their voice patterns.""],
         [""Azure AI Language"", ""system"", ""An Azure service used for detecting and redacting personal data in text.""],
         [""Ingestion Client tool"", ""component"", ""A tool for ingesting audio files into the Azure AI services for transcription and data redaction.""],
         [""AI services"", ""system"", ""The collection of Azure AI services used in this process, including Speech, Language, and OpenAI.""],
         [""Azure OpenAI"", ""system"", ""An Azure service used for processing and analyzing the transcribed conversations.""],
         [""Transcript"", ""component"", ""The text output of speech recognition.""],
         [""Personal data"", ""descriptor"", ""Data that is sensitive and related to individuals, such as names, addresses, or phone numbers.""],
         [""Redaction"", ""descriptor"", ""The process of removing sensitive information from a text.""],
         [""Power BI"", ""system"", ""A business analytics service for creating visualizations and reports.""],
         [""App Service"", ""system"", ""Azure's platform for hosting web applications.""],
         [""Custom web application"", ""component"", ""A web application tailored to specific needs.""],
         [""CRM"", ""system"", ""Customer relationship management software used for storing customer data and interaction history.""],
         [""Power BI or custom web application"", ""descriptor"", ""Either Power BI or a custom web application is used to visualize the output.""],
         [""Near real-time insights"", ""descriptor"", ""Insights generated in near real-time from the processed data.""],
         [""Agents"", ""person"", ""Customer service representatives who use the system.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage Account""],
         [""Azure AI Speech"", ""calls"", ""Ingestion Client tool""],
         [""Ingestion Client tool"", ""part-of"", ""AI services""],
         [""Azure AI Language"", ""called-from"", ""Transcript""],
         [""Azure OpenAI"", ""part-of"", ""AI services""],
         [""Transcript"", ""called-from"", ""Power BI or custom web application""],
         [""Power BI or custom web application"", ""part-of"", ""Near real-time insights""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Azure where audio files and transcriptions are stored.""],
         [""Azure Storage Account"", ""component"", ""An account within the Azure Blob Storage system.""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool for managing Azure Storage accounts.""],
         [""Storage SDK or API"", ""component"", ""Software Development Kits or APIs for uploading audio files to Azure Storage.""],
         [""Azure AI Speech"", ""system"", ""An Azure service used for speech recognition and transcription.""],
         [""Batch mode"", ""descriptor"", ""A mode in which a large amount of data is processed asynchronously by the system.""],
         [""Speaker diarization"", ""descriptor"", ""The process of identifying speakers in a conversation based on their voice patterns.""],
         [""Azure AI Language"", ""system"", ""An Azure service used for detecting and redacting personal data in text.""],
         [""Ingestion Client tool"", ""component"", ""A tool for ingesting audio files into the Azure AI services for transcription and data redaction.""],
         [""AI services"", ""system"", ""The collection of Azure AI services used in this process, including Speech, Language, and OpenAI.""],
         [""Azure OpenAI"", ""system"", ""An Azure service used for processing and analyzing the transcribed conversations.""],
         [""Transcript"", ""component"", ""The text output of speech recognition.""],
         [""Personal data"", ""descriptor"", ""Data that is sensitive and related to individuals, such as names, addresses, or phone numbers.""],
         [""Redaction"", ""descriptor"", ""The process of removing sensitive information from a text.""],
         [""Power BI"", ""system"", ""A business analytics service for creating visualizations and reports.""],
         [""App Service"", ""system"", ""Azure's platform for hosting web applications.""],
         [""Custom web application"", ""component"", ""A web application tailored to specific needs.""],
         [""CRM"", ""system"", ""Customer relationship management software used for storing customer data and interaction history.""],
         [""Power BI or custom web application"", ""descriptor"", ""Either Power BI or a custom web application is used to visualize the output.""],
         [""Near real-time insights"", ""descriptor"", ""Insights generated in near real-time from the processed data.""],
         [""Agents"", ""person"", ""Customer service representatives who use the system.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage Account""],
         [""Azure AI Speech"", ""calls"", ""Ingestion Client tool""],
         [""Ingestion Client tool"", ""part-of"", ""AI services""],
         [""Azure AI Language"", ""called-from"", ""Transcript""],
         [""Azure OpenAI"", ""part-of"", ""AI services""],
         [""Transcript"", ""called-from"", ""Power BI or custom web application""],
         [""Power BI or custom web application"", ""part-of"", ""Near real-time insights""]
      ]
   }",7.634045790007804
100,mistral:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}"," {
      ""entities"": [
         [""APIM"", ""system"", ""The system that receives POST requests from outside services and clients""],
         [""Patient API"", ""component"", ""The software component responsible for handling the CreatePatient function""],
         [""CreatePatient function"", ""component"", ""The specific functionality in Patient API that is called with given patient information""],
         [""Audit API"", ""component"", ""The software component that contains the CreateAuditRecord function""],
         [""CreateAuditRecord function"", ""component"", ""The specific functionality in Audit API that creates an audit record""],
         [""Azure Cosmos DB"", ""system"", ""The database system where patient documents and audit records are stored""],
         [""outside services and clients"", ""person"", ""The entities making POST requests to APIM""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""Patient API"", ""APIM calls the CreatePatient function in Patient API with given patient information""],
         [""Patient API"", ""contains"", ""CreatePatient function""],
         [""Patient API"", ""calls"", ""Audit API"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in Audit API""],
         [""Audit API"", ""contains"", ""CreateAuditRecord function""],
         [""CreateAuditRecord function"", ""creates"", ""audit record""],
         [""CreateAuditRecord function"", ""uses"", ""Azure Cosmos DB"", ""The CreateAuditRecord function in Audit API creates the audit record in Azure Cosmos DB""],
         [""Patient API"", ""writes-to"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""]
      ]
   }"," {
      ""entities"": [
         [""APIM"", ""system"", ""The system that receives POST requests from outside services and clients""],
         [""Patient API"", ""component"", ""The software component responsible for handling the CreatePatient function""],
         [""CreatePatient function"", ""component"", ""The specific functionality in Patient API that is called with given patient information""],
         [""Audit API"", ""component"", ""The software component that contains the CreateAuditRecord function""],
         [""CreateAuditRecord function"", ""component"", ""The specific functionality in Audit API that creates an audit record""],
         [""Azure Cosmos DB"", ""system"", ""The database system where patient documents and audit records are stored""],
         [""outside services and clients"", ""person"", ""The entities making POST requests to APIM""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""Patient API"", ""APIM calls the CreatePatient function in Patient API with given patient information""],
         [""Patient API"", ""contains"", ""CreatePatient function""],
         [""Patient API"", ""calls"", ""Audit API"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in Audit API""],
         [""Audit API"", ""contains"", ""CreateAuditRecord function""],
         [""CreateAuditRecord function"", ""creates"", ""audit record""],
         [""CreateAuditRecord function"", ""uses"", ""Azure Cosmos DB"", ""The CreateAuditRecord function in Audit API creates the audit record in Azure Cosmos DB""],
         [""Patient API"", ""writes-to"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""]
      ]
   }",4.406114448967855
101,mistral:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}"," {
      ""entities"": [
         [""CDS"", ""component"", ""Cloud Data Storage""],
         [""Data Factory"", ""component"", ""Azure Data Integration Service""],
         [""SQL"", ""component"", ""Database Management System""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning System""],
         [""Azure SQL database"", ""component"", ""Microsoft's Cloud-based SQL Database Service""],
         [""Azure Functions"", ""component"", ""Serverless Function as a Service Platform""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Microsoft's Low-Code Application Platform""],
         [""CSV export"", ""component"", ""Comma Separated Values File Export Feature""],
         [""Suppliers"", ""entity"", ""Third-Party Vendors""],
         [""PO data"", ""entity"", ""Purchase Order Data""]
      ],
      ""relationships"": [
         [""Data Factory"", ""part-of"", ""CDS""],
         [""CDS"", ""contains"", ""SQL""],
         [""ERP system"", ""calls"", ""Azure SQL database""],
         [""Azure SQL database"", ""called-from"", ""Azure Functions""],
         [""Azure Functions"", ""uses"", ""API""],
         [""Azure Functions"", ""creates"", ""task""],
         [""Power Apps"", ""retrieves-data-from"", ""Azure SQL Database""],
         [""Power Apps"", ""updates"", ""PO data""],
         [""Power Apps"", ""calls"", ""CSV export""],
         [""User"", ""reviews"", ""PO data""],
         [""Power Apps"", ""sends"", ""data to suppliers through"", ""CSV export""],
         [""Power BI"", ""reports-trends-in"", ""supplier status""]
      ]
   }"," {
      ""entities"": [
         [""CDS"", ""component"", ""Cloud Data Storage""],
         [""Data Factory"", ""component"", ""Azure Data Integration Service""],
         [""SQL"", ""component"", ""Database Management System""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning System""],
         [""Azure SQL database"", ""component"", ""Microsoft's Cloud-based SQL Database Service""],
         [""Azure Functions"", ""component"", ""Serverless Function as a Service Platform""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Microsoft's Low-Code Application Platform""],
         [""CSV export"", ""component"", ""Comma Separated Values File Export Feature""],
         [""Suppliers"", ""entity"", ""Third-Party Vendors""],
         [""PO data"", ""entity"", ""Purchase Order Data""]
      ],
      ""relationships"": [
         [""Data Factory"", ""part-of"", ""CDS""],
         [""CDS"", ""contains"", ""SQL""],
         [""ERP system"", ""calls"", ""Azure SQL database""],
         [""Azure SQL database"", ""called-from"", ""Azure Functions""],
         [""Azure Functions"", ""uses"", ""API""],
         [""Azure Functions"", ""creates"", ""task""],
         [""Power Apps"", ""retrieves-data-from"", ""Azure SQL Database""],
         [""Power Apps"", ""updates"", ""PO data""],
         [""Power Apps"", ""calls"", ""CSV export""],
         [""User"", ""reviews"", ""PO data""],
         [""Power Apps"", ""sends"", ""data to suppliers through"", ""CSV export""],
         [""Power BI"", ""reports-trends-in"", ""supplier status""]
      ]
   }",4.639180601981934
102,mistral:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A web-based hosting service for version control using Git""],
         [""CI/CD pipeline agent"", ""component"", ""An Azure VM-installed tool that automates the build and deployment process of code checks-in""],
         [""Azure VM"", ""component"", ""A virtual machine on Microsoft's cloud service, Azure""],
         [""API application"", ""component"", ""The application that receives the built code from the CI/CD pipeline agent""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment""],
         [""Azure API Management"", ""system"", ""A product of Microsoft Azure for managing APIs""],
         [""HOST headers"", ""component"", ""Used to specify domain names for websites or subdomains in an HTTP request""],
         [""API Management policy"", ""component"", ""Configurations for controlling and securing the APIs managed by Azure API Management""],
         [""App Service Environment's DNS name"", ""component"", ""The Domain Name System (DNS) name of the App Service Environment""],
         [""Application Gateway"", ""system"", ""A Microsoft Azure service that acts as a reverse proxy and load balancer for web traffic""],
         [""Azure Private DNS"", ""system"", ""A service that routes traffic internally between ASE, API Management, and Application Gateway""],
         [""External users"", ""person"", ""Users outside the organization who consume the APIs via the exposed developer portal""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent"", ""Code checks-in trigger the CI/CD pipeline agent""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM"", ""The CI/CD pipeline agent is installed on an Azure VM""],
         [""API application"", ""part-of"", ""ILB ASE"", ""The API application is hosted on ILB ASE""],
         [""Azure API Management"", ""uses"", ""HOST headers"", ""Uses HOST headers in the API policy""],
         [""Azure API Management"", ""uses"", ""App Service Environment's DNS name"", ""Routes traffic to APIs using its DNS name""],
         [""Application Gateway"", ""exposes"", ""API Management"", ""Exposes API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", ""traffic"", ""Routes the traffic internally between ASE, API Management, and Application Gateway""],
         [""External users"", ""utilizes"", ""Application Gateway"", ""Consume APIs via the exposed developer portal through Application Gateway's public IP""]
      ]
   }"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A web-based hosting service for version control using Git""],
         [""CI/CD pipeline agent"", ""component"", ""An Azure VM-installed tool that automates the build and deployment process of code checks-in""],
         [""Azure VM"", ""component"", ""A virtual machine on Microsoft's cloud service, Azure""],
         [""API application"", ""component"", ""The application that receives the built code from the CI/CD pipeline agent""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment""],
         [""Azure API Management"", ""system"", ""A product of Microsoft Azure for managing APIs""],
         [""HOST headers"", ""component"", ""Used to specify domain names for websites or subdomains in an HTTP request""],
         [""API Management policy"", ""component"", ""Configurations for controlling and securing the APIs managed by Azure API Management""],
         [""App Service Environment's DNS name"", ""component"", ""The Domain Name System (DNS) name of the App Service Environment""],
         [""Application Gateway"", ""system"", ""A Microsoft Azure service that acts as a reverse proxy and load balancer for web traffic""],
         [""Azure Private DNS"", ""system"", ""A service that routes traffic internally between ASE, API Management, and Application Gateway""],
         [""External users"", ""person"", ""Users outside the organization who consume the APIs via the exposed developer portal""]
      ],
      ""relationships"": [
         [""GitHub"", ""calls"", ""CI/CD pipeline agent"", ""Code checks-in trigger the CI/CD pipeline agent""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM"", ""The CI/CD pipeline agent is installed on an Azure VM""],
         [""API application"", ""part-of"", ""ILB ASE"", ""The API application is hosted on ILB ASE""],
         [""Azure API Management"", ""uses"", ""HOST headers"", ""Uses HOST headers in the API policy""],
         [""Azure API Management"", ""uses"", ""App Service Environment's DNS name"", ""Routes traffic to APIs using its DNS name""],
         [""Application Gateway"", ""exposes"", ""API Management"", ""Exposes API Management's developer and API portal""],
         [""Azure Private DNS"", ""routes"", ""traffic"", ""Routes the traffic internally between ASE, API Management, and Application Gateway""],
         [""External users"", ""utilizes"", ""Application Gateway"", ""Consume APIs via the exposed developer portal through Application Gateway's public IP""]
      ]
   }",6.441605825035367
103,mistral:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""The service that receives requests from clients and routes them to their respective destinations.""],
         [""API Management"", ""component"", ""A service that manages, secures, and documents APIs.""],
         [""Azure AD B2C"", ""component"", ""An identity provider used for authentication in this solution.""],
         [""Custom Domain"", ""component"", ""A domain configured specifically for sign-in within Azure AD B2C.""],
         [""JSON Web Token (JWT)"", ""component"", ""The token returned by Azure AD B2C after successful user authentication.""],
         [""App Service Environment"", ""system"", ""An isolated and dedicated environment for running web applications in Azure App Service.""],
         [""Function Apps"", ""system"", ""A serverless compute service in Azure that lets you run small pieces of code called functions.""],
         [""Azure Kubernetes Services (AKS)"", ""system"", ""A managed container orchestration service provided by Microsoft Azure for deploying and managing containerized applications.""],
         [""Backend API"", ""component"", ""The actual application interface accessed through the API.""],
         [""Button"", ""component"", ""A user interface element that triggers an event, such as a click, leading to a request to the backend API.""],
         [""Mobile Device"", ""component"", ""A device used by the user, capable of sending requests to the backend API.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management"", ""Azure Front Door is part of the API Management service.""],
         [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""The Azure Front Door service forwards the authentication request to Azure AD B2C.""],
         [""API Management"", ""part-of"", ""Azure AD B2C"", ""Through its validate-jwt policy, API Management validates the bearer token returned by Azure AD B2C.""],
         [""API Management"", ""calls"", ""Backend API"", ""If the JWT is valid, API Management forwards the request to the appropriate backend API.""],
         [""Button"", ""calls"", ""Backend API"", ""A click on a button or a direct call to the endpoint of the backend API triggers a request to it.""],
         [""Mobile Device"", ""calls"", ""Backend API"", ""Requests are sent to the backend API from mobile devices, either through user interactions like clicking buttons or direct calls.""]
      ]
   }"," {
      ""entities"": [
         [""Azure Front Door"", ""component"", ""The service that receives requests from clients and routes them to their respective destinations.""],
         [""API Management"", ""component"", ""A service that manages, secures, and documents APIs.""],
         [""Azure AD B2C"", ""component"", ""An identity provider used for authentication in this solution.""],
         [""Custom Domain"", ""component"", ""A domain configured specifically for sign-in within Azure AD B2C.""],
         [""JSON Web Token (JWT)"", ""component"", ""The token returned by Azure AD B2C after successful user authentication.""],
         [""App Service Environment"", ""system"", ""An isolated and dedicated environment for running web applications in Azure App Service.""],
         [""Function Apps"", ""system"", ""A serverless compute service in Azure that lets you run small pieces of code called functions.""],
         [""Azure Kubernetes Services (AKS)"", ""system"", ""A managed container orchestration service provided by Microsoft Azure for deploying and managing containerized applications.""],
         [""Backend API"", ""component"", ""The actual application interface accessed through the API.""],
         [""Button"", ""component"", ""A user interface element that triggers an event, such as a click, leading to a request to the backend API.""],
         [""Mobile Device"", ""component"", ""A device used by the user, capable of sending requests to the backend API.""]
      ],
      ""relationships"": [
         [""Azure Front Door"", ""part-of"", ""API Management"", ""Azure Front Door is part of the API Management service.""],
         [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""The Azure Front Door service forwards the authentication request to Azure AD B2C.""],
         [""API Management"", ""part-of"", ""Azure AD B2C"", ""Through its validate-jwt policy, API Management validates the bearer token returned by Azure AD B2C.""],
         [""API Management"", ""calls"", ""Backend API"", ""If the JWT is valid, API Management forwards the request to the appropriate backend API.""],
         [""Button"", ""calls"", ""Backend API"", ""A click on a button or a direct call to the endpoint of the backend API triggers a request to it.""],
         [""Mobile Device"", ""calls"", ""Backend API"", ""Requests are sent to the backend API from mobile devices, either through user interactions like clicking buttons or direct calls.""]
      ]
   }",6.154912891972344
104,mistral:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}"," {
      ""entities"": [
         [""Airline System"", ""System"", ""The data source that communicates with a custom API""],
         [""Custom API Hosted in Azure API Management"", ""Component"", ""API hosted on Azure API Management that is used by the Airline System""],
         [""Custom API Coordinator"", ""Component"", ""The component that receives notifications and handles incoming messages from the Airline System""],
         [""Power Apps"", ""System"", ""The system where flights are assigned to Microsoft Teams channels""],
         [""Azure Storage Account queue"", ""Component"", ""Queue in Azure Storage Account where Graph API calls are queued for further processing""],
         [""Azure Functions"", ""System"", ""System that runs the Graph API calls based on messages in the storage queue""],
         [""Graph API"", ""Component"", ""API called by Azure Functions to send notifications to Teams and stream events to Event Hubs""],
         [""Azure Event Hubs"", ""System"", ""Streaming platform for further analytics of events from Event Hubs""],
         [""Custom Bot Messaging Service"", ""System"", ""The service that manages the airline's notification system""],
         [""Azure Bot Service"", ""Component"", ""Service employed by the Custom Bot Messaging Service""],
         [""Microsoft Teams channels"", ""System"", ""Channels within Microsoft Teams where flights are assigned""],
         [""Users"", ""Person"", ""The entities that receive flight updates in Teams""],
         [""Airline's notification system"", ""Component"", ""The system responsible for sending flight updates to users""],
         [""Azure Data Lake storage"", ""System"", ""Storage offering long-term retention and micro-batch processing of events from Event Hubs""],
         [""Power BI"", ""System"", ""Tool used for generating insightful reports""]
      ],
      ""relationships"": [
         [""Airline System"", ""calls"", ""Custom API Hosted in Azure API Management"", ""Sends notifications and handles incoming messages through the custom API""],
         [""Custom API Coordinator"", ""part-of"", ""Power Apps"", ""Coordinates notifications from Airline System and sends them to Power Apps""],
         [""Airline System"", ""queues"", ""Graph API call"", ""Queues Graph API calls in Azure Storage Account queue for further processing""],
         [""Azure Functions"", ""calls"", ""Graph API"", ""Runs the Graph API calls based on messages in the storage queue""],
         [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""Sends notifications to Teams""],
         [""Azure Functions"", ""streams"", ""Azure Event Hubs"", ""Streams all events for further analytics""],
         [""Custom Bot Messaging Service"", ""employs"", ""Azure Bot Service"", ""Uses Azure Bot Service for its functionality""],
         [""Custom Bots"", ""calls"", ""Microsoft Teams channels"", ""Sends flight updates to users in Teams""],
         [""Airline's notification system"", ""manages"", ""Custom Bot Messaging Service"", ""Is responsible for managing the custom bot messaging service""]
      ]
   }"," {
      ""entities"": [
         [""Airline System"", ""System"", ""The data source that communicates with a custom API""],
         [""Custom API Hosted in Azure API Management"", ""Component"", ""API hosted on Azure API Management that is used by the Airline System""],
         [""Custom API Coordinator"", ""Component"", ""The component that receives notifications and handles incoming messages from the Airline System""],
         [""Power Apps"", ""System"", ""The system where flights are assigned to Microsoft Teams channels""],
         [""Azure Storage Account queue"", ""Component"", ""Queue in Azure Storage Account where Graph API calls are queued for further processing""],
         [""Azure Functions"", ""System"", ""System that runs the Graph API calls based on messages in the storage queue""],
         [""Graph API"", ""Component"", ""API called by Azure Functions to send notifications to Teams and stream events to Event Hubs""],
         [""Azure Event Hubs"", ""System"", ""Streaming platform for further analytics of events from Event Hubs""],
         [""Custom Bot Messaging Service"", ""System"", ""The service that manages the airline's notification system""],
         [""Azure Bot Service"", ""Component"", ""Service employed by the Custom Bot Messaging Service""],
         [""Microsoft Teams channels"", ""System"", ""Channels within Microsoft Teams where flights are assigned""],
         [""Users"", ""Person"", ""The entities that receive flight updates in Teams""],
         [""Airline's notification system"", ""Component"", ""The system responsible for sending flight updates to users""],
         [""Azure Data Lake storage"", ""System"", ""Storage offering long-term retention and micro-batch processing of events from Event Hubs""],
         [""Power BI"", ""System"", ""Tool used for generating insightful reports""]
      ],
      ""relationships"": [
         [""Airline System"", ""calls"", ""Custom API Hosted in Azure API Management"", ""Sends notifications and handles incoming messages through the custom API""],
         [""Custom API Coordinator"", ""part-of"", ""Power Apps"", ""Coordinates notifications from Airline System and sends them to Power Apps""],
         [""Airline System"", ""queues"", ""Graph API call"", ""Queues Graph API calls in Azure Storage Account queue for further processing""],
         [""Azure Functions"", ""calls"", ""Graph API"", ""Runs the Graph API calls based on messages in the storage queue""],
         [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""Sends notifications to Teams""],
         [""Azure Functions"", ""streams"", ""Azure Event Hubs"", ""Streams all events for further analytics""],
         [""Custom Bot Messaging Service"", ""employs"", ""Azure Bot Service"", ""Uses Azure Bot Service for its functionality""],
         [""Custom Bots"", ""calls"", ""Microsoft Teams channels"", ""Sends flight updates to users in Teams""],
         [""Airline's notification system"", ""manages"", ""Custom Bot Messaging Service"", ""Is responsible for managing the custom bot messaging service""]
      ]
   }",7.303587763977703
105,mistral:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A system that handles workflows, triggered by HTTP requests and uses connectors to integrate with commonly used services. Offers hundreds of connectors and allows creating custom connectors.""],
         [""Azure API Management"", ""system"", ""A system consisting of two components: the API gateway and the developer portal.""],
         [""API gateway"", ""component"", ""A component of Azure API Management that accepts HTTP calls and routes them to the backend.""],
         [""Developer portal"", ""component"", ""A component of Azure API Management providing access to documentation, code samples for calling APIs, and testing APIs.""],
         [""Azure DNS"", ""system"", ""A system provided by Azure that handles name resolution using Azure infrastructure. Allows managing DNS records using the same credentials, APIs, tools, and billing as other Azure services.""],
         [""Custom domain name"", ""component"", ""A domain name used to map a custom domain name to an IP address for more information see Configure a custom domain name in API Management.""],
         [""Microsoft Entra ID"", ""system"", ""A system that authenticates clients that call the API gateway using OpenID Connect (OIDC) protocol. Provides access control and token validation for authorized requests.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""calls-from"", ""API Management""],
         [""API Management"", ""part-of"", ""Azure API Management""],
         [""API gateway"", ""part-of"", ""Azure API Management""],
         [""Developer portal"", ""part-of"", ""Azure API Management""],
         [""Azure DNS"", ""contains"", ""Custom domain name""],
         [""Microsoft Entra ID"", ""called-from"", ""API Gateway""]
      ]
   }"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A system that handles workflows, triggered by HTTP requests and uses connectors to integrate with commonly used services. Offers hundreds of connectors and allows creating custom connectors.""],
         [""Azure API Management"", ""system"", ""A system consisting of two components: the API gateway and the developer portal.""],
         [""API gateway"", ""component"", ""A component of Azure API Management that accepts HTTP calls and routes them to the backend.""],
         [""Developer portal"", ""component"", ""A component of Azure API Management providing access to documentation, code samples for calling APIs, and testing APIs.""],
         [""Azure DNS"", ""system"", ""A system provided by Azure that handles name resolution using Azure infrastructure. Allows managing DNS records using the same credentials, APIs, tools, and billing as other Azure services.""],
         [""Custom domain name"", ""component"", ""A domain name used to map a custom domain name to an IP address for more information see Configure a custom domain name in API Management.""],
         [""Microsoft Entra ID"", ""system"", ""A system that authenticates clients that call the API gateway using OpenID Connect (OIDC) protocol. Provides access control and token validation for authorized requests.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""calls-from"", ""API Management""],
         [""API Management"", ""part-of"", ""Azure API Management""],
         [""API gateway"", ""part-of"", ""Azure API Management""],
         [""Developer portal"", ""part-of"", ""Azure API Management""],
         [""Azure DNS"", ""contains"", ""Custom domain name""],
         [""Microsoft Entra ID"", ""called-from"", ""API Gateway""]
      ]
   }",4.6969362529926
106,mistral:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}"," {
      ""entities"": [
         [""Data owners"", ""person"", ""Individuals who upload datasets into a public blob storage account""],
         [""Public blob storage account"", ""system"", ""A storage system where data is initially uploaded and encrypted by Microsoft-managed keys""],
         [""Microsoft-managed keys"", ""component"", ""Encryption keys used for the data stored in the public blob storage account""],
         [""Azure Data Factory"", ""component"", ""A component that triggers the copying of the uploaded dataset to a specific location on another storage account""],
         [""Trigger"", ""component"", ""An event or condition that starts the copying process initiated by Azure Data Factory""],
         [""Specific location (import path)"", ""system"", ""The destination where the copied data is stored in the second storage account""],
         [""Second storage account with security controls"", ""system"", ""The storage system where the copied data is stored after it's been encrypted and moved by Azure Data Factory""],
         [""Private endpoint"", ""component"", ""A networking feature that allows secure access to the second storage account""],
         [""Service principal with limited permissions"", ""person"", ""A security identity used to manage access to the second storage account""],
         [""Streaming application"", ""component"", ""An application through which researchers access the secure environment""],
         [""Azure Virtual Desktop"", ""component"", ""The privileged jump box for accessing the streaming application""],
         [""Secure network environment"", ""system"", ""A network used to provision data science VMs and the Azure Machine Learning compute""],
         [""Data science VMs"", ""component"", ""Virtual Machines used for research work on the secure network environment""],
         [""Azure Machine Learning compute"", ""component"", ""A system that can access the dataset through a private endpoint for machine learning capabilities""],
         [""De-identified data"", ""component"", ""The model or data that is saved to a separate location after removing personal information""],
         [""Export path"", ""system"", ""The destination where de-identified data is stored on the secure storage account""],
         [""Logic app"", ""component"", ""A component that starts the manual approval process when new data is added to the export path""],
         [""Manual reviewers"", ""person"", ""Individuals who ensure sensitive data isn't exported""],
         [""Regulatory guidelines"", ""component"", ""Standards or rules that models must meet for compliance purposes""],
         [""Data Factory instance"", ""component"", ""A component used to move the approved de-identified data to the public storage account""],
         [""Public storage account"", ""system"", ""The storage system where approved de-identified data is stored for external access""],
         [""External researchers"", ""person"", ""Researchers who have access to the exported data and models in the public storage account""]
      ],
      ""relationships"": [
         [""Data owners"", ""uploads"", ""Public blob storage account""],
         [""Public blob storage account"", ""contains"", ""Microsoft-managed keys""],
         [""Azure Data Factory"", ""uses"", ""Trigger""],
         [""Azure Data Factory"", ""copies"", ""Specific location (import path)""],
         [""Second storage account with security controls"", ""is accessed through"", ""Private endpoint""],
         [""Second storage account with security controls"", ""has access permissions granted to"", ""Service principal with limited permissions""],
         [""Streaming application"", ""uses"", ""Azure Virtual Desktop""],
         [""Secure network environment"", ""contains"", ""Data science VMs""],
         [""Secure network environment"", ""contains"", ""Azure Machine Learning compute""],
         [""De-identified data"", ""is saved to"", ""Export path""],
         [""Logic app"", ""starts"", ""Manual approval process""],
         [""Data Factory instance"", ""moves"", ""Public storage account""]
      ]
   }"," {
      ""entities"": [
         [""Data owners"", ""person"", ""Individuals who upload datasets into a public blob storage account""],
         [""Public blob storage account"", ""system"", ""A storage system where data is initially uploaded and encrypted by Microsoft-managed keys""],
         [""Microsoft-managed keys"", ""component"", ""Encryption keys used for the data stored in the public blob storage account""],
         [""Azure Data Factory"", ""component"", ""A component that triggers the copying of the uploaded dataset to a specific location on another storage account""],
         [""Trigger"", ""component"", ""An event or condition that starts the copying process initiated by Azure Data Factory""],
         [""Specific location (import path)"", ""system"", ""The destination where the copied data is stored in the second storage account""],
         [""Second storage account with security controls"", ""system"", ""The storage system where the copied data is stored after it's been encrypted and moved by Azure Data Factory""],
         [""Private endpoint"", ""component"", ""A networking feature that allows secure access to the second storage account""],
         [""Service principal with limited permissions"", ""person"", ""A security identity used to manage access to the second storage account""],
         [""Streaming application"", ""component"", ""An application through which researchers access the secure environment""],
         [""Azure Virtual Desktop"", ""component"", ""The privileged jump box for accessing the streaming application""],
         [""Secure network environment"", ""system"", ""A network used to provision data science VMs and the Azure Machine Learning compute""],
         [""Data science VMs"", ""component"", ""Virtual Machines used for research work on the secure network environment""],
         [""Azure Machine Learning compute"", ""component"", ""A system that can access the dataset through a private endpoint for machine learning capabilities""],
         [""De-identified data"", ""component"", ""The model or data that is saved to a separate location after removing personal information""],
         [""Export path"", ""system"", ""The destination where de-identified data is stored on the secure storage account""],
         [""Logic app"", ""component"", ""A component that starts the manual approval process when new data is added to the export path""],
         [""Manual reviewers"", ""person"", ""Individuals who ensure sensitive data isn't exported""],
         [""Regulatory guidelines"", ""component"", ""Standards or rules that models must meet for compliance purposes""],
         [""Data Factory instance"", ""component"", ""A component used to move the approved de-identified data to the public storage account""],
         [""Public storage account"", ""system"", ""The storage system where approved de-identified data is stored for external access""],
         [""External researchers"", ""person"", ""Researchers who have access to the exported data and models in the public storage account""]
      ],
      ""relationships"": [
         [""Data owners"", ""uploads"", ""Public blob storage account""],
         [""Public blob storage account"", ""contains"", ""Microsoft-managed keys""],
         [""Azure Data Factory"", ""uses"", ""Trigger""],
         [""Azure Data Factory"", ""copies"", ""Specific location (import path)""],
         [""Second storage account with security controls"", ""is accessed through"", ""Private endpoint""],
         [""Second storage account with security controls"", ""has access permissions granted to"", ""Service principal with limited permissions""],
         [""Streaming application"", ""uses"", ""Azure Virtual Desktop""],
         [""Secure network environment"", ""contains"", ""Data science VMs""],
         [""Secure network environment"", ""contains"", ""Azure Machine Learning compute""],
         [""De-identified data"", ""is saved to"", ""Export path""],
         [""Logic app"", ""starts"", ""Manual approval process""],
         [""Data Factory instance"", ""moves"", ""Public storage account""]
      ]
   }",9.101613162027206
107,mistral:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}"," {
      ""entities"": [
         [""Outlook email account"", ""person"", ""A designated email account that receives PDF files as attachments.""],
         [""Logic app (1)"", ""component"", ""A program built using Azure Logic Apps, which processes emails and uploads PDF files to Azure Data Lake Storage.""],
         [""Azure Data Lake Storage"", ""system"", ""A cloud-based storage system used for storing the PDF files.""],
         [""PDF container"", ""component"", ""A specific container in Azure Data Lake Storage that stores the PDF files.""],
         [""Logic app (2)"", ""component"", ""A program built using Azure Logic Apps, which processes the PDF forms in PDF files when they arrive in the PDF container.""],
         [""Function app"", ""component"", ""A program built using Azure Functions, which processes the single-page PDF files after they're split by the function app.""],
         [""Azure REST API"", ""component"", ""An application programming interface used by the function app to send the location of the single-page PDF file for processing.""],
         [""AI Document Intelligence"", ""component"", ""An Azure service used for processing the PDF files.""],
         [""JSON data file"", ""component"", ""A file containing the response data from AI Document Intelligence.""],
         [""third container in Data Lake Storage"", ""component"", ""Another container in Azure Data Lake Storage where the JSON data files are stored.""],
         [""Azure Cosmos DB"", ""system"", ""A cloud-based, NoSQL database service used for storing and managing the processed data.""],
         [""database"", ""component"", ""The data storage component within Azure Cosmos DB.""]
      ],
      ""relationships"": [
         [""Outlook email account"", ""part-of"", ""Logic app (1)"", ""The Outlook email account is part of the trigger for the Logic app (1).""],
         [""PDF container"", ""contains"", ""Logic app (2)"", ""The PDF container contains the files that trigger Logic app (2).""],
         [""Function app"", ""part-of"", ""third container in Data Lake Storage"", ""The Function app saves the split and processed PDF pages to a second container in Data Lake Storage.""],
         [""Function app"", ""calls"", ""AI Document Intelligence"", ""The Function app sends the location of the single-page PDF file for processing to AI Document Intelligence.""],
         [""AI Document Intelligence"", ""called-from"", ""Function app"", ""After processing, AI Document Intelligence returns a response back to the Function app.""],
         [""Function app"", ""calls"", ""Azure Cosmos DB"", ""The Function app sends the processed data to Azure Cosmos DB for storage.""],
         [""Power BI"", ""obtains-data-from"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB.""]
      ]
   }"," {
      ""entities"": [
         [""Outlook email account"", ""person"", ""A designated email account that receives PDF files as attachments.""],
         [""Logic app (1)"", ""component"", ""A program built using Azure Logic Apps, which processes emails and uploads PDF files to Azure Data Lake Storage.""],
         [""Azure Data Lake Storage"", ""system"", ""A cloud-based storage system used for storing the PDF files.""],
         [""PDF container"", ""component"", ""A specific container in Azure Data Lake Storage that stores the PDF files.""],
         [""Logic app (2)"", ""component"", ""A program built using Azure Logic Apps, which processes the PDF forms in PDF files when they arrive in the PDF container.""],
         [""Function app"", ""component"", ""A program built using Azure Functions, which processes the single-page PDF files after they're split by the function app.""],
         [""Azure REST API"", ""component"", ""An application programming interface used by the function app to send the location of the single-page PDF file for processing.""],
         [""AI Document Intelligence"", ""component"", ""An Azure service used for processing the PDF files.""],
         [""JSON data file"", ""component"", ""A file containing the response data from AI Document Intelligence.""],
         [""third container in Data Lake Storage"", ""component"", ""Another container in Azure Data Lake Storage where the JSON data files are stored.""],
         [""Azure Cosmos DB"", ""system"", ""A cloud-based, NoSQL database service used for storing and managing the processed data.""],
         [""database"", ""component"", ""The data storage component within Azure Cosmos DB.""]
      ],
      ""relationships"": [
         [""Outlook email account"", ""part-of"", ""Logic app (1)"", ""The Outlook email account is part of the trigger for the Logic app (1).""],
         [""PDF container"", ""contains"", ""Logic app (2)"", ""The PDF container contains the files that trigger Logic app (2).""],
         [""Function app"", ""part-of"", ""third container in Data Lake Storage"", ""The Function app saves the split and processed PDF pages to a second container in Data Lake Storage.""],
         [""Function app"", ""calls"", ""AI Document Intelligence"", ""The Function app sends the location of the single-page PDF file for processing to AI Document Intelligence.""],
         [""AI Document Intelligence"", ""called-from"", ""Function app"", ""After processing, AI Document Intelligence returns a response back to the Function app.""],
         [""Function app"", ""calls"", ""Azure Cosmos DB"", ""The Function app sends the processed data to Azure Cosmos DB for storage.""],
         [""Power BI"", ""obtains-data-from"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB.""]
      ]
   }",6.830644773028325
108,mistral:7b,../IMAGES NEW/Azure/07 Azure internal APIs original.txt,"
The data flows as follows:

    Developers check in code to a GitHub repository that's connected to a CI/CD pipeline agent that's installed on an Azure VM.
    The agent pushes the build to the API application that's hosted on ILB ASE.
    Azure API Management consumes the preceding APIs via HOST headers that are specified in API Management policy.
    API Management uses the App Service Environment's DNS name for all the APIs.
    Application Gateway exposes API Management's developer and API portal.
    Azure Private DNS is used to route the traffic internally between ASE, API Management, and Application Gateway.
    External users utilize the exposed developer portal to consume the APIs via Application Gateway's public IP.

","{""entities"": [[""GitHub repository"", ""system"", """"], [""Developers"", ""person"", """"], [""External users"", ""person"", """"], [""CI/CD pipeline agent"", ""system"", """"], [""Azure VM"", ""system"", """"], [""ILB ASE"", ""system"", """"], [""API application"", ""component"", """"], [""Azure API Management"", ""system"", ""consumes the preceding APIs via HOST headers""], [""App Service Environment"", ""component"", """"], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Developer portal"", ""component"", ""API Management's developer and API portal""], [""API portal"", ""component"", ""API Management's developer and API portal""], [""Application Gateway"", ""component"", ""exposes API Management's developer and API portal""], [""Azure Private DNS"", ""component"", ""used to route the traffic internally between ASE, API Management, and Application Gateway""]], ""relationships"": [[""Azure VM"", ""contains"", ""CI/CD pipeline agent"", ""installed on an Azure VM""], [""API application"", ""part-of"", ""ILB ASE"", ""API application that's hosted on ILB ASE""], [""GitHub repository"", ""calls"", ""CI/CD pipeline agent"", ""GitHub repository that's connected to a CI/CD pipeline agent""], [""CI/CD pipeline agent"", ""calls"", ""API application"", ""agent pushes the build to the API application""], [""Azure API Management"", ""calls"", ""App Service Environment"", ""uses the App Service Environment's DNS name for all the APIs""], [""External users"", ""calls"", ""Application Gateway"", """"], [""Application Gateway"", ""calls"", ""API application"", ""to consume the APIs via Application Gateway's public IP""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management's developer and API portal""], [""Azure API Management"", ""contains"", ""API portal"", ""API Management's developer and API portal""], [""Application Gateway"", ""calls"", ""API portal"", ""exposes API Management's developer and API portal.""], [""Application Gateway"", ""calls"", ""Developer portal"", ""exposes API Management's developer and API portal.""], [""Azure Private DNS"", ""calls"", ""Azure API Management"", ""route the traffic internally between ASE, API Management, and Application Gateway""], [""Azure Private DNS"", ""calls"", ""Application Gateway"", ""route the traffic internally between ASE, API Management, and Application Gateway""]]}"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A web-based hosting service for version control using Git""],
         [""CI/CD pipeline agent"", ""component"", ""A tool that automates the continuous integration and delivery process""],
         [""Azure VM"", ""component"", ""A virtual machine provided by Microsoft Azure""],
         [""API application"", ""component"", ""An application that provides APIs for other applications to interact with it""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment""],
         [""Azure API Management"", ""component"", ""A cloud-based service for managing, securing, and publishing APIs""],
         [""HOST headers"", ""entity"", ""Information sent by a client to a server indicating the Internet site the user wants to view""],
         [""API Management policy"", ""entity"", ""Rules applied to API Management services""],
         [""App Service Environment's DNS name"", ""entity"", ""The Domain Name System (DNS) name of an App Service Environment""],
         [""Application Gateway"", ""component"", ""A Microsoft Azure load balancer that routes traffic to backend pools""],
         [""Azure Private DNS"", ""system"", ""A private managed DNS service in Azure""],
         [""External users"", ""person"", ""Users who are outside of the internal network""]
      ],
      ""relationships"": [
         [""GitHub"", ""part-of"", ""CI/CD pipeline agent""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
         [""Azure VM"", ""part-of"", ""API application""],
         [""API application"", ""calls"", ""Azure API Management""],
         [""Azure API Management"", ""uses"", ""HOST headers""],
         [""Azure API Management"", ""uses"", ""App Service Environment's DNS name""],
         [""Application Gateway"", ""exposes"", ""API Management""],
         [""Azure Private DNS"", ""routes"", ""traffic between ASE, API Management, and Application Gateway""]
      ]
   }"," {
      ""entities"": [
         [""GitHub"", ""system"", ""A web-based hosting service for version control using Git""],
         [""CI/CD pipeline agent"", ""component"", ""A tool that automates the continuous integration and delivery process""],
         [""Azure VM"", ""component"", ""A virtual machine provided by Microsoft Azure""],
         [""API application"", ""component"", ""An application that provides APIs for other applications to interact with it""],
         [""ILB ASE"", ""system"", ""Internal Load Balancer Application Service Environment""],
         [""Azure API Management"", ""component"", ""A cloud-based service for managing, securing, and publishing APIs""],
         [""HOST headers"", ""entity"", ""Information sent by a client to a server indicating the Internet site the user wants to view""],
         [""API Management policy"", ""entity"", ""Rules applied to API Management services""],
         [""App Service Environment's DNS name"", ""entity"", ""The Domain Name System (DNS) name of an App Service Environment""],
         [""Application Gateway"", ""component"", ""A Microsoft Azure load balancer that routes traffic to backend pools""],
         [""Azure Private DNS"", ""system"", ""A private managed DNS service in Azure""],
         [""External users"", ""person"", ""Users who are outside of the internal network""]
      ],
      ""relationships"": [
         [""GitHub"", ""part-of"", ""CI/CD pipeline agent""],
         [""CI/CD pipeline agent"", ""part-of"", ""Azure VM""],
         [""Azure VM"", ""part-of"", ""API application""],
         [""API application"", ""calls"", ""Azure API Management""],
         [""Azure API Management"", ""uses"", ""HOST headers""],
         [""Azure API Management"", ""uses"", ""App Service Environment's DNS name""],
         [""Application Gateway"", ""exposes"", ""API Management""],
         [""Azure Private DNS"", ""routes"", ""traffic between ASE, API Management, and Application Gateway""]
      ]
   }",5.018823774007615
109,mistral:7b,../IMAGES NEW/Azure/12 Azure Network Microservices original.txt,"
    Outside services and clients make a POST request to APIM, with a data body that includes patient information.
    APIM calls the CreatePatient function in the Patient API with the given patient information.
    The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app to create an audit record.
    The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function.
    The CreatePatient function creates the patient document in Azure Cosmos DB, and returns a success response to APIM.
    The outside services and clients receive the success response from APIM.

","{""entities"": [[""Outside services"", ""system"", ""Outside services and clients make a POST request to APIM""], [""clients"", ""system"", ""Outside services and clients make a POST request to APIM""], [""APIM"", ""system"", ""Outside services and clients make a POST request to APIM""], [""Patient API"", ""system"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreatePatient function"", ""component"", "" The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""Audit API function app"", ""system"", ""The CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""component"", ""CreatePatient function in Patient API calls the CreateAuditRecord function""], [""Azure Cosmos DB"", ""system"", ""The Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB, and returns a success response to the Patient API CreatePatient function""]], ""relationships"": [[""CreatePatient function"", ""part-of"", ""Patient API"", ""Reason""], [""Outside services"", ""calls"", ""APIM"", ""utside services and clients make a POST request to APIM""], [""clients"", ""calls"", ""APIM"", ""outside services and clients make a POST request to APIM""], [""APIM"", ""calls"", ""CreatePatient function"", ""APIM calls the CreatePatient function in the Patient API""], [""CreateAuditRecord function"", ""part-of"", ""Audit API function app"", ""Reason""], [""CreatePatient function"", ""calls"", ""CreateAuditRecord function"", ""CreatePatient function in Patient API calls the CreateAuditRecord function in the Audit API function app""], [""CreateAuditRecord function"", ""calls"", ""Azure Cosmos DB"", ""Audit API CreateAuditRecord function creates the audit record in Azure Cosmos DB""], [""CreateAuditRecord function"", ""calls"", ""CreatePatient function"", ""returns a success response to the Patient API CreatePatient function.""], [""CreatePatient function"", ""calls"", ""Azure Cosmos DB"", ""The CreatePatient function creates the patient document in Azure Cosmos DB""], [""CreatePatient function"", ""is-called-from"", ""APIM"", ""The CreatePatient function returns a success response to APIM""]]}"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway that receives POST requests from outside services and clients.""],
         [""CreatePatient"", ""component"", ""Function in Patient API called by APIM with patient information.""],
         [""Patient API"", ""system"", ""Software system containing the CreatePatient function.""],
         [""Audit API"", ""system"", ""Software system containing the CreateAuditRecord function.""],
         [""CreateAuditRecord"", ""component"", ""Function in Audit API called by Patient API to create an audit record.""],
         [""Azure Cosmos DB"", ""system"", ""Database where patient document and audit records are stored.""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient"", ""Initiates a connection to the CreatePatient function in Patient API with patient information.""],
         [""CreatePatient"", ""calls"", ""CreateAuditRecord"", ""Calls the CreateAuditRecord function in Audit API to create an audit record.""],
         [""CreateAuditRecord"", ""called-from"", ""CreatePatient"", ""Is called or invoked from the CreatePatient function in Patient API.""],
         [""CreatePatient"", ""part-of"", ""Azure Cosmos DB"", ""Creates a patient document within Azure Cosmos DB.""],
         [""Audit API CreateAuditRecord function"", ""part-of"", ""Azure Cosmos DB"", ""Creates an audit record within Azure Cosmos DB.""]
      ]
   }"," {
      ""entities"": [
         [""APIM"", ""system"", ""API Gateway that receives POST requests from outside services and clients.""],
         [""CreatePatient"", ""component"", ""Function in Patient API called by APIM with patient information.""],
         [""Patient API"", ""system"", ""Software system containing the CreatePatient function.""],
         [""Audit API"", ""system"", ""Software system containing the CreateAuditRecord function.""],
         [""CreateAuditRecord"", ""component"", ""Function in Audit API called by Patient API to create an audit record.""],
         [""Azure Cosmos DB"", ""system"", ""Database where patient document and audit records are stored.""]
      ],
      ""relationships"": [
         [""APIM"", ""calls"", ""CreatePatient"", ""Initiates a connection to the CreatePatient function in Patient API with patient information.""],
         [""CreatePatient"", ""calls"", ""CreateAuditRecord"", ""Calls the CreateAuditRecord function in Audit API to create an audit record.""],
         [""CreateAuditRecord"", ""called-from"", ""CreatePatient"", ""Is called or invoked from the CreatePatient function in Patient API.""],
         [""CreatePatient"", ""part-of"", ""Azure Cosmos DB"", ""Creates a patient document within Azure Cosmos DB.""],
         [""Audit API CreateAuditRecord function"", ""part-of"", ""Azure Cosmos DB"", ""Creates an audit record within Azure Cosmos DB.""]
      ]
   }",3.9747307799989358
110,mistral:7b,../IMAGES NEW/Azure/05. azure Original.txt,"
    Backend systems. The right-hand side of the diagram shows the various backend systems that the enterprise has deployed or relies on. These systems might include SaaS systems, other Azure services, or web services that expose REST or SOAP endpoints.

    Azure Logic Apps. In this architecture, logic apps are triggered by HTTP requests. You can also nest workflows for more complex orchestration. Logic Apps uses connectors to integrate with commonly used services. Logic Apps offers hundreds of connectors, and you can create custom connectors.

    Azure API Management. API Management consists of two related components:

        API gateway. The API gateway accepts HTTP calls and routes them to the backend.

        Developer portal. Each instance of Azure API Management provides access to a developer portal. This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal.

    Azure DNS. Azure DNS provides name resolution by using the Azure infrastructure. By hosting your domains in Azure, you can manage your DNS records by using the same credentials, APIs, tools, and billing that you use for your other Azure services. To use a custom domain name, such as contoso.com, create DNS records that map the custom domain name to the IP address. For more information, see Configure a custom domain name in API Management.

    Microsoft Entra ID. Use Microsoft Entra ID to authenticate clients that call the API gateway. Microsoft Entra ID supports the OpenID Connect (OIDC) protocol. Clients obtain an access token from Microsoft Entra ID, and API Gateway validates the token to authorize the request. If you use the Standard or Premium tier of API Management, Microsoft Entra ID can also help secure access to the developer portal.

","{""entities"": [[""Backend systems"", ""system"", ""various backend systems that the enterprise has deployed or relies on""], [""SaaS systems"", ""component"", ""SaaS systems""], [""other Azure services"", ""component"", ""other Azure services""], [""web services"", ""component"", ""expose REST or SOAP endpoints""], [""Azure Logic Apps"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""Azure API Management"", ""system"", ""Logic Apps uses connectors to integrate with commonly used services""], [""API gateway"", ""component"", ""The API gateway accepts HTTP calls and routes them to the backend""], [""Developer portal"", ""component"", ""This portal gives your developers access to documentation and code samples for calling the APIs""], [""Azure DNS"", ""system"", ""Azure DNS provides name resolution by using the Azure infrastructure""], [""Microsoft Entra ID"", ""system"", ""Use Microsoft Entra ID to authenticate clients that call the API gateway""], [""clients"", ""system"", ""clients that call the API gateway""], [""developers"", ""person"", ""accesses documentation and code samples""]], ""relationships"": [[""Backend systems"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Backend systems"", ""contains"", ""web services"", ""These systems might include ""], [""Backend systems"", ""contains"", ""other Azure services"", ""These systems might include ""], [""web services"", ""contains"", ""SaaS systems"", ""These systems might include ""], [""Azure API Management"", ""contains"", ""API gateway"", ""API Management consists of two related components""], [""Azure API Management"", ""contains"", ""Developer portal"", ""API Management consists of two related components""], [""clients"", ""calls"", ""API gateway"", ""clients that call the API gateway""], [""clients"", ""calls"", ""Microsoft Entra ID"", ""Clients obtain an access token from Microsoft Entra ID""], [""developers"", ""calls"", ""Developer portal"", ""This portal gives your developers access to documentation and code samples for calling the APIs. You can also test APIs in the developer portal""]]}"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A system that handles orchestration of tasks and workflows in Azure, triggered by HTTP requests, and uses connectors to integrate with commonly used services""],
         [""API Management"", ""system"", ""An API management system provided by Azure, consisting of two components: API gateway and developer portal.""],
         [""API Gateway"", ""component"", ""A component within the API Management system that accepts HTTP calls and routes them to the backend.""],
         [""Developer Portal"", ""component"", ""A component within the API Management system that provides access to documentation, code samples for calling APIs, testing of APIs, and other resources for developers.""],
         [""Azure DNS"", ""system"", ""An Azure service that offers name resolution by using the Azure infrastructure.""],
         [""Custom Domain Name"", ""entity"", ""A custom domain name used for mapping to an IP address in API Management.""],
         [""Microsoft Entra ID"", ""person"", ""A system used for authenticating clients that call the API gateway, supporting the OpenID Connect (OIDC) protocol.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""contains"", ""API Management""],
         [""API Management"", ""part-of"", ""Azure""],
         [""API Gateway"", ""part-of"", ""API Management""],
         [""Developer Portal"", ""part-of"", ""API Management""],
         [""Azure DNS"", ""part-of"", ""Azure""]
      ]
   }"," {
      ""entities"": [
         [""Azure Logic Apps"", ""system"", ""A system that handles orchestration of tasks and workflows in Azure, triggered by HTTP requests, and uses connectors to integrate with commonly used services""],
         [""API Management"", ""system"", ""An API management system provided by Azure, consisting of two components: API gateway and developer portal.""],
         [""API Gateway"", ""component"", ""A component within the API Management system that accepts HTTP calls and routes them to the backend.""],
         [""Developer Portal"", ""component"", ""A component within the API Management system that provides access to documentation, code samples for calling APIs, testing of APIs, and other resources for developers.""],
         [""Azure DNS"", ""system"", ""An Azure service that offers name resolution by using the Azure infrastructure.""],
         [""Custom Domain Name"", ""entity"", ""A custom domain name used for mapping to an IP address in API Management.""],
         [""Microsoft Entra ID"", ""person"", ""A system used for authenticating clients that call the API gateway, supporting the OpenID Connect (OIDC) protocol.""]
      ],
      ""relationships"": [
         [""Azure Logic Apps"", ""contains"", ""API Management""],
         [""API Management"", ""part-of"", ""Azure""],
         [""API Gateway"", ""part-of"", ""API Management""],
         [""Developer Portal"", ""part-of"", ""API Management""],
         [""Azure DNS"", ""part-of"", ""Azure""]
      ]
   }",4.058768125018105
111,mistral:7b,../IMAGES NEW/Azure/15 Azure automate-document-classification.txt,"
    The user provides a document file that the web app uploads. The file contains multiple documents of various types. It can, for instance, be a PDF or multipage TIFF file.
        The document file is stored in Azure Blob Storage.
        The web app adds a command message to a storage queue to initiate pipeline processing.

    Durable Functions orchestration is triggered by the command message. The message contains metadata that identifies the location in Blob Storage of the document file to be processed. Each Durable Functions instance processes only one document file.

    The Scan activity function calls the Computer Vision Read API, passing in the location in storage of the document to be processed. Optical character recognition (OCR) results are returned to the orchestration to be used by subsequent activities.

    The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster. This service uses regular expression pattern matching to identify the starting page of each known document and to calculate how many document types are contained in the document file. The types and page ranges of the documents are calculated and returned to the orchestration.



Note

Azure doesn't offer a service that can classify multiple document types in a single file. This solution uses a non-Azure service that's hosted in AKS.

The Metadata Store activity function saves the document type and page range information in an Azure Cosmos DB store.

The Indexing activity function creates a new search document in the Cognitive Search service for each identified document type and uses the Azure AI Search libraries for .NET to include in the search document the full OCR results and document information. A correlation ID is also added to the search document so that the search results can be matched with the corresponding document metadata from Azure Cosmos DB.

End users can search for documents by contents and metadata. Correlation IDs in the search result set can be used to look up document records that are in Azure Cosmos DB. The records include links to the original document file in Blob Storage.


","{""entities"": [[""web app"", ""system"", ""The user provides a document file that the web app uploads""], [""Azure Blob Storage"", ""system"", ""The document file is stored in Azure Blob Storage""], [""storage queue"", ""system"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""Durable Functions orchestration"", ""system"", ""Durable Functions orchestration is triggered by the command message.""], [""Scan activity function"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Computer Vision Read API"", ""component"", ""The Scan activity function calls the Computer Vision Read API.""], [""Classify activity function"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""document classifier service"", ""component"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Azure Kubernetes Service (AKS) cluster"", ""system"", ""The Classify activity function calls the document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""User"", ""person"", ""provides a document file that the web app uploads""]], ""relationships"": [[""User"", ""calls"", ""web app"", ""The user provides a document file that the web app uploads""], [""web app"", ""calls"", ""Azure Blob Storage"", ""The user provides a document file that the web app uploads.The document file is stored in Azure Blob Storage""], [""web app"", ""calls"", ""storage queue"", ""the web app adds a command message to a storage queue to initiate pipeline processing""], [""storage queue"", ""calls"", ""Durable Functions orchestration"", ""Durable Functions orchestration is triggered by the command message.""], [""Durable Functions orchestration"", ""calls"", ""Azure Blob Storage"", ""Each Durable Functions instance processes only one document file""], [""Scan activity function"", ""calls"", ""Computer Vision Read API"", ""the Scan activity function calls the Computer Vision Read API""], [""document classifier service"", ""part-of"", ""Azure Kubernetes Service (AKS) cluster"", ""document classifier service that's hosted in an Azure Kubernetes Service (AKS) cluster""], [""Classify activity function"", ""calls"", ""Scan activity function"", ""The Classify activity function calls the document classifier service""], [""Classify activity function"", ""calls"", ""Durable Functions orchestration"", ""The types and page ranges of the documents are calculated and returned to the orchestration""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""Cloud storage service where the user provides a document file is stored.""],
         [""storage queue"", ""component"", ""A message queue to initiate pipeline processing.""],
         [""Durable Functions orchestration"", ""component"", ""A series of coordinated functions that process one document file at a time.""],
         [""Computer Vision Read API"", ""component"", ""An optical character recognition (OCR) service called by the Scan activity function.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""A managed container orchestration service in Azure.""],
         [""document classifier service"", ""component"", ""A regular expression-based document classifier hosted within an Azure Kubernetes Service (AKS) cluster.""],
         [""Azure Cosmos DB store"", ""system"", ""A NoSQL database used to save the document type and page range information.""],
         [""Cognitive Search service"", ""component"", ""An Azure search service that indexes documents based on their contents and metadata.""],
         [""Azure AI Search libraries for .NET"", ""component"", ""Libraries used by the Indexing activity function for indexing documents in Cognitive Search.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""document file""],
         [""storage queue"", ""part-of"", ""Durable Functions orchestration""],
         [""Durable Functions orchestration"", ""calls"", ""Scan activity function""],
         [""Scan activity function"", ""calls"", ""Computer Vision Read API""],
         [""Classify activity function"", ""part-of"", ""Durable Functions orchestration""],
         [""Classify activity function"", ""calls"", ""document classifier service""],
         [""Metadata Store activity function"", ""part-of"", ""Durable Functions orchestration""],
         [""Metadata Store activity function"", ""saves"", [""Azure Cosmos DB store""]],
         [""Indexing activity function"", ""part-of"", ""Durable Functions orchestration""],
         [""Indexing activity function"", ""creates"", ""search document""],
         [""Indexing activity function"", ""uses"", [""Azure AI Search libraries for .NET""]]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""Cloud storage service where the user provides a document file is stored.""],
         [""storage queue"", ""component"", ""A message queue to initiate pipeline processing.""],
         [""Durable Functions orchestration"", ""component"", ""A series of coordinated functions that process one document file at a time.""],
         [""Computer Vision Read API"", ""component"", ""An optical character recognition (OCR) service called by the Scan activity function.""],
         [""Azure Kubernetes Service (AKS)"", ""system"", ""A managed container orchestration service in Azure.""],
         [""document classifier service"", ""component"", ""A regular expression-based document classifier hosted within an Azure Kubernetes Service (AKS) cluster.""],
         [""Azure Cosmos DB store"", ""system"", ""A NoSQL database used to save the document type and page range information.""],
         [""Cognitive Search service"", ""component"", ""An Azure search service that indexes documents based on their contents and metadata.""],
         [""Azure AI Search libraries for .NET"", ""component"", ""Libraries used by the Indexing activity function for indexing documents in Cognitive Search.""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""document file""],
         [""storage queue"", ""part-of"", ""Durable Functions orchestration""],
         [""Durable Functions orchestration"", ""calls"", ""Scan activity function""],
         [""Scan activity function"", ""calls"", ""Computer Vision Read API""],
         [""Classify activity function"", ""part-of"", ""Durable Functions orchestration""],
         [""Classify activity function"", ""calls"", ""document classifier service""],
         [""Metadata Store activity function"", ""part-of"", ""Durable Functions orchestration""],
         [""Metadata Store activity function"", ""saves"", [""Azure Cosmos DB store""]],
         [""Indexing activity function"", ""part-of"", ""Durable Functions orchestration""],
         [""Indexing activity function"", ""creates"", ""search document""],
         [""Indexing activity function"", ""uses"", [""Azure AI Search libraries for .NET""]]
      ]
   }",5.626868651015684
112,mistral:7b,../IMAGES NEW/Azure/13 Azure Secure Research.txt,"
    Data owners upload datasets into a public blob storage account. The data is encrypted by using Microsoft-managed keys.

    Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls. The storage account can only be reached through a private endpoint. Also, it's accessed by a service principal with limited permissions. Data Factory deletes the original copy making the dataset immutable.

    Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box.

    The dataset in the secure storage account is presented to the data science VMs provisioned in a secure network environment for research work. Much of the data preparation is done on those VMs.

    The secure environment has Azure Machine Learning compute that can access the dataset through a private endpoint for users for Azure Machine Learning capabilities, such as to train, deploy, automate, and manage machine learning models. At this point, models are created that meet regulatory guidelines. All model data is de-identified by removing personal information.

    Models or de-identified data is saved to a separate location on the secure storage (export path). When new data is added to the export path, a logic app is triggered. In this architecture, the logic app is outside the secure environment because no data is sent to the logic app. Its only function is to send notification and start the manual approval process.

    The app starts an approval process requesting a review of data that is queued to be exported. The manual reviewers ensure that sensitive data isn't exported. After the review process, the data is either approved or denied.

    Note

    If an approval step is not required on exfiltration, the logic app step could be omitted.

    If the de-identified data is approved, it's sent to the Data Factory instance.

    Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models. Alternately, you can provision another storage account in a lower security environment.

","{""entities"": [[""datasets"", ""component"", ""Data owners upload datasets into a public blob storage account""], [""public blob storage account"", ""system"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""storage account"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""copied dataset"", ""system"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""private endpoint"", ""component"", ""The storage account can only be reached through a private endpoint.""], [""service principal"", ""component"", ""Also, it's accessed by a service principal with limited permissions.""], [""streaming application"", ""component"", ""access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""component"", ""Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""component"", ""data science VMs provisioned in a secure network environment""], [""Azure Machine Learning compute"", ""component"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""secure network environment"", ""system"", ""data science VMs provisioned in a secure network environment""], [""machine learning models"", ""component"", ""At this point, models are created that meet regulatory guidelines""], [""de-identified data"", ""component"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""logic app"", ""component"", ""When new data is added to the export path, a logic app is triggered""], [""separate container"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""moved data"", ""component"", ""Data Factory moves the data to the public storage account in a separate container""], [""Data owners"", ""person"", ""Data owners upload datasets into a public blob storage account""], [""Researchers"", ""person"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""manual reviewers"", ""person"", ""The manual reviewers ensure that sensitive data isn't exported.""], [""Users"", ""person"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]], ""relationships"": [[""datasets"", ""part-of"", ""public blob storage account"", ""Data owners upload datasets into a public blob storage account""], [""Data owners"", ""calls"", ""datasets"", ""Data owners upload datasets into a public blob storage account""], [""Azure Data Factory"", ""calls"", ""datasets"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset""], [""storage account"", ""contains"", ""copied dataset"", ""opying of the uploaded dataset to a specific location (import path) on another storage account with security controls""], [""Azure Data Factory"", ""calls"", ""copied dataset"", ""Azure Data Factory uses a trigger that starts copying of the uploaded dataset to a specific location""], [""private endpoint"", ""calls"", ""storage account"", ""The storage account can only be reached through a private endpoint""], [""storage account"", ""is-called-from"", ""service principal"", ""Also, it's accessed by a service principal with limited permissions""], [""Researchers"", ""calls"", ""Azure Virtual Desktop"", "" Researchers access the secure environment through a streaming application""], [""Azure Virtual Desktop"", ""calls"", ""streaming application"", ""through a streaming application using Azure Virtual Desktop""], [""streaming application"", ""calls"", ""storage account"", ""Researchers access the secure environment through a streaming application using Azure Virtual Desktop as a privileged jump box""], [""data science VMs"", ""part-of"", ""secure network environment"", ""The dataset in the secure storage account is presented to the data science VMs""], [""data science VMs"", ""calls"", ""copied dataset"", "" The dataset in the secure storage account is presented to the data science VMs""], [""secure network environment"", ""contains"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""Azure Machine Learning compute"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""], [""machine learning models"", ""part-of"", ""storage account"", ""Models or de-identified data is saved to a separate location on the secure storage (export path)""], [""machine learning models"", ""calls"", ""logic app"", ""When new data is added to the export path, a logic app is triggered""], [""logic app"", ""calls"", ""manual reviewers"", ""The app starts an approval process requesting a review of data that is queued to be exported.""], [""public blob storage account"", ""contains"", ""separate container"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""de-identified data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""separate container"", ""contains"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container""], [""Azure Data Factory"", ""calls"", ""moved data"", ""Data Factory moves the data to the public storage account in a separate container to allow external researchers to have access to their exported data and models""], [""Users"", ""calls"", ""Azure Machine Learning compute"", ""The secure environment has Azure Machine Learning compute that can access the dataset""]]}"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Individuals who upload datasets""],
         [""Blob Storage Account"", ""system"", ""Public storage account where datasets are uploaded by data owners""],
         [""Microsoft-managed keys"", ""component"", ""Keys used for encrypting the datasets""],
         [""Azure Data Factory"", ""component"", ""Service that triggers copying of datasets to another location""],
         [""Trigger"", ""component"", ""Event that initiates the copying process""],
         [""Import Path"", ""component"", ""Specific location on another storage account where datasets are copied""],
         [""Private Endpoint"", ""component"", ""Endpoint used for secure access to the other storage account""],
         [""Service Principal"", ""component"", ""Entity with limited permissions that accesses the other storage account""],
         [""Streaming Application"", ""component"", ""Application through which researchers access the secure environment""],
         [""Azure Virtual Desktop"", ""component"", ""Privileged jump box used by researchers to access the secure environment""],
         [""Data Science VMs"", ""component"", ""Virtual machines provisioned in a secure network environment for research work""],
         [""Secure Network Environment"", ""system"", ""Environment where data science VMs are provisioned and research is conducted""],
         [""Azure Machine Learning Compute"", ""component"", ""Service that can access the dataset through a private endpoint""],
         [""De-identified Data"", ""component"", ""Data from which personal information has been removed""],
         [""Export Path"", ""component"", ""Location on the secure storage account where de-identified data is saved""],
         [""Logic App"", ""component"", ""Application that starts the manual approval process""],
         [""Manual Reviewers"", ""person"", ""Individuals who ensure sensitive data isn't exported""],
         [""Data Factory Instance"", ""component"", ""Instance of Data Factory that moves approved data to public storage account""],
         [""Public Storage Account"", ""system"", ""Storage account where de-identified data and models are saved for external access""]
      ],
      ""relationships"": [
         [""Blob Storage Account"", ""contains"", ""Microsoft-managed keys""],
         [""Data Owners"", ""part-of"", ""Blob Storage Account""],
         [""Azure Data Factory"", ""part-of"", ""Secure Network Environment""],
         [""Trigger"", ""part-of"", ""Azure Data Factory""],
         [""Azure Data Factory"", ""calls"", ""Import Path""],
         [""Import Path"", ""contains"", ""Dataset""],
         [""Service Principal"", ""part-of"", ""Private Endpoint""],
         [""Streaming Application"", ""part-of"", ""Azure Virtual Desktop""],
         [""Data Science VMs"", ""part-of"", ""Secure Network Environment""],
         [""Azure Machine Learning Compute"", ""part-of"", ""Secure Network Environment""],
         [""De-identified Data"", ""part-of"", ""Export Path""],
         [""Logic App"", ""called-from"", ""Manual Reviewers""],
         [""Data Factory Instance"", ""calls"", ""Public Storage Account""],
         [""Public Storage Account"", ""contains"", ""De-identified Data""],
         [""Public Storage Account"", ""contains"", ""Models""]
      ]
   }"," {
      ""entities"": [
         [""Data Owners"", ""person"", ""Individuals who upload datasets""],
         [""Blob Storage Account"", ""system"", ""Public storage account where datasets are uploaded by data owners""],
         [""Microsoft-managed keys"", ""component"", ""Keys used for encrypting the datasets""],
         [""Azure Data Factory"", ""component"", ""Service that triggers copying of datasets to another location""],
         [""Trigger"", ""component"", ""Event that initiates the copying process""],
         [""Import Path"", ""component"", ""Specific location on another storage account where datasets are copied""],
         [""Private Endpoint"", ""component"", ""Endpoint used for secure access to the other storage account""],
         [""Service Principal"", ""component"", ""Entity with limited permissions that accesses the other storage account""],
         [""Streaming Application"", ""component"", ""Application through which researchers access the secure environment""],
         [""Azure Virtual Desktop"", ""component"", ""Privileged jump box used by researchers to access the secure environment""],
         [""Data Science VMs"", ""component"", ""Virtual machines provisioned in a secure network environment for research work""],
         [""Secure Network Environment"", ""system"", ""Environment where data science VMs are provisioned and research is conducted""],
         [""Azure Machine Learning Compute"", ""component"", ""Service that can access the dataset through a private endpoint""],
         [""De-identified Data"", ""component"", ""Data from which personal information has been removed""],
         [""Export Path"", ""component"", ""Location on the secure storage account where de-identified data is saved""],
         [""Logic App"", ""component"", ""Application that starts the manual approval process""],
         [""Manual Reviewers"", ""person"", ""Individuals who ensure sensitive data isn't exported""],
         [""Data Factory Instance"", ""component"", ""Instance of Data Factory that moves approved data to public storage account""],
         [""Public Storage Account"", ""system"", ""Storage account where de-identified data and models are saved for external access""]
      ],
      ""relationships"": [
         [""Blob Storage Account"", ""contains"", ""Microsoft-managed keys""],
         [""Data Owners"", ""part-of"", ""Blob Storage Account""],
         [""Azure Data Factory"", ""part-of"", ""Secure Network Environment""],
         [""Trigger"", ""part-of"", ""Azure Data Factory""],
         [""Azure Data Factory"", ""calls"", ""Import Path""],
         [""Import Path"", ""contains"", ""Dataset""],
         [""Service Principal"", ""part-of"", ""Private Endpoint""],
         [""Streaming Application"", ""part-of"", ""Azure Virtual Desktop""],
         [""Data Science VMs"", ""part-of"", ""Secure Network Environment""],
         [""Azure Machine Learning Compute"", ""part-of"", ""Secure Network Environment""],
         [""De-identified Data"", ""part-of"", ""Export Path""],
         [""Logic App"", ""called-from"", ""Manual Reviewers""],
         [""Data Factory Instance"", ""calls"", ""Public Storage Account""],
         [""Public Storage Account"", ""contains"", ""De-identified Data""],
         [""Public Storage Account"", ""contains"", ""Models""]
      ]
   }",7.774916179012507
113,mistral:7b,../IMAGES NEW/Azure/16 Azure automate-pdf-forms.txt,"
    A designated Outlook email account receives PDF files as attachments. The arrival of an email triggers a logic app to process the email. The logic app is built by using the capabilities of Azure Logic Apps.
    The logic app uploads the PDF files to a container in Azure Data Lake Storage.
    You can also manually or programmatically upload PDF files to the same PDF container.
    The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms that are in the PDF file.
    The logic app sends the location of the PDF file to a function app for processing. The function app is built by using the capabilities of Azure Functions.
    The function app receives the location of the file and takes these actions:
        It splits the file into single pages if the file has multiple pages. Each page contains one independent form. Split files are saved to a second container in Data Lake Storage.
        It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing. When Azure AI Document Intelligence completes its processing, it sends a response back to the function app, which places the information into a data structure.
        It creates a JSON data file that contains the response data and stores the file to a third container in Data Lake Storage.
    The forms processing logic app receives the processed response data.
    The forms processing logic app sends the processed data to Azure Cosmos DB, which saves the data in a database and in collections.
    Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards.
    You can implement further processing as needed on the data that's in Azure Cosmos DB.

","{""entities"": [[""designated Outlook email account"", ""component"", ""A designated Outlook email account receives PDF files as attachments""], [""logic app"", ""component"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""system"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""Container"", ""component"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Azure Data Lake Storage"", ""system"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""logic app 2"", ""system"", ""The arrival of a PDF file in the PDF container triggers another logic app to process the PDF forms""], [""function app"", ""component"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""Azure Functions"", ""system"", ""The function app is built by using the capabilities of Azure Functions""], [""Azure REST API"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file""], [""Container 2"", ""component"", ""Split files are saved to a second container in Data Lake Storage""], [""AI Document Intelligence"", ""component"", ""It uses HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence for processing""], [""Container 3"", ""component"", ""JSON data file that contains the response data and stores the file to a third container in Data Lake Storage""], [""forms processing logic app"", ""component"", ""The forms processing logic app receives the processed response data""], [""Azure Cosmos DB"", ""component"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""database"", ""component"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""component"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]], ""relationships"": [[""designated Outlook email account"", ""calls"", ""logic app"", ""The arrival of an email triggers a logic app to process the email.""], [""Azure Logic Apps"", ""contains"", ""logic app"", ""The logic app is built by using the capabilities of Azure Logic Apps.""], [""logic app"", ""calls"", ""Container"", ""The logic app uploads the PDF files to a container in Azure Data Lake Storage""], [""Container"", ""part-of"", ""Azure Data Lake Storage"", ""container in Azure Data Lake Storage""], [""Container"", ""calls"", ""logic app 2"", ""The arrival of a PDF file in the PDF container triggers another logic app""], [""logic app"", ""calls"", ""function app"", ""The logic app sends the location of the PDF file to a function app for processing.""], [""function app"", ""part-of"", ""Azure Functions"", ""container in Azure Data Lake Storage""], [""function app"", ""calls"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""Azure Data Lake Storage"", ""contains"", ""Container 2"", ""Split files are saved to a second container in Data Lake Storage""], [""function app"", ""calls"", ""Azure REST API"", ""It uses HTTPS POST, an Azure REST API""], [""Azure REST API"", ""calls"", ""AI Document Intelligence"", ""HTTPS POST, an Azure REST API, to send the location of the single-page PDF file to AI Document Intelligence""], [""Azure Data Lake Storage"", ""contains"", ""Container 3"", ""third container in Data Lake Storage""], [""function app"", ""calls"", ""Container 3"", ""It creates a JSON data file that contains the response data and stores the file to a third container""], [""forms processing logic app"", ""calls"", ""Container 3"", ""The forms processing logic app receives the processed response data""], [""forms processing logic app"", ""calls"", ""Azure Cosmos DB"", ""The forms processing logic app sends the processed data to Azure Cosmos DB""], [""Azure Cosmos DB"", ""calls"", ""database"", ""Azure Cosmos DB, which saves the data in a database""], [""Power BI"", ""calls"", ""Azure Cosmos DB"", ""Power BI obtains the data from Azure Cosmos DB and provides insights and dashboards""]]}"," {
      ""entities"": [
         [""Outlook email account"", ""person"", ""A designated Outlook email account that receives PDF files as attachments""],
         [""Logic app"", ""component"", ""A logic app built using the capabilities of Azure Logic Apps that processes emails containing PDF files and triggers other logic apps.""],
         [""Azure Logic Apps"", ""system"", ""The cloud-based workflow automation service offered by Microsoft Azure, used to build the logic app""],
         [""Azure Data Lake Storage"", ""system"", ""A distributed file system for big data analytics, used to store PDF files and split pages of PDF files.""],
         [""PDF container"", ""component"", ""A container within Azure Data Lake Storage used to store PDF files""],
         [""Function app"", ""component"", ""A function app built using the capabilities of Azure Functions that processes single-page PDF files sent by the logic app and sends data to Cosmos DB.""],
         [""Azure Functions"", ""system"", ""The serverless compute service offered by Microsoft Azure, used to build the function app""],
         [""HTTPS POST"", ""component"", ""A method for sending data over the internet using a secure connection""],
         [""Azure REST API"", ""component"", ""A set of endpoints and protocols that allow developers to interact with Azure services programmatically.""],
         [""AI Document Intelligence"", ""component"", ""An AI service offered by Microsoft Azure that processes PDF forms.""],
         [""Data structure"", ""component"", ""The format used to store the processed response data.""],
         [""third container in Data Lake Storage"", ""component"", ""A third container within Azure Data Lake Storage used to store the JSON data files containing the response data.""],
         [""Azure Cosmos DB"", ""system"", ""A globally distributed, multi-model database service offered by Microsoft Azure, used to save the processed data from the forms processing logic app.""],
         [""Power BI"", ""component"", ""A business analytics tool that obtains the data from Azure Cosmos DB and provides insights and dashboards.""]
      ],
      ""relationships"": [
         [""Outlook email account"", ""part-of"", ""Logic app""],
         [""Logic app"", ""calls"", ""Function app""],
         [""PDF container"", ""contains"", ""PDF files""],
         [""Function app"", ""part-of"", ""PDF container (split pages)""],
         [""Function app"", ""calls"", ""AI Document Intelligence""],
         [""AI Document Intelligence"", ""called-from"", ""Function app""],
         [""Function app"", ""stores to"", ""third container in Data Lake Storage""],
         [""Logic app"", ""part-of"", ""Forms processing logic app""],
         [""Forms processing logic app"", ""sends to"", ""Azure Cosmos DB""],
         [""Power BI"", ""obtains from"", ""Azure Cosmos DB""]
      ]
   }"," {
      ""entities"": [
         [""Outlook email account"", ""person"", ""A designated Outlook email account that receives PDF files as attachments""],
         [""Logic app"", ""component"", ""A logic app built using the capabilities of Azure Logic Apps that processes emails containing PDF files and triggers other logic apps.""],
         [""Azure Logic Apps"", ""system"", ""The cloud-based workflow automation service offered by Microsoft Azure, used to build the logic app""],
         [""Azure Data Lake Storage"", ""system"", ""A distributed file system for big data analytics, used to store PDF files and split pages of PDF files.""],
         [""PDF container"", ""component"", ""A container within Azure Data Lake Storage used to store PDF files""],
         [""Function app"", ""component"", ""A function app built using the capabilities of Azure Functions that processes single-page PDF files sent by the logic app and sends data to Cosmos DB.""],
         [""Azure Functions"", ""system"", ""The serverless compute service offered by Microsoft Azure, used to build the function app""],
         [""HTTPS POST"", ""component"", ""A method for sending data over the internet using a secure connection""],
         [""Azure REST API"", ""component"", ""A set of endpoints and protocols that allow developers to interact with Azure services programmatically.""],
         [""AI Document Intelligence"", ""component"", ""An AI service offered by Microsoft Azure that processes PDF forms.""],
         [""Data structure"", ""component"", ""The format used to store the processed response data.""],
         [""third container in Data Lake Storage"", ""component"", ""A third container within Azure Data Lake Storage used to store the JSON data files containing the response data.""],
         [""Azure Cosmos DB"", ""system"", ""A globally distributed, multi-model database service offered by Microsoft Azure, used to save the processed data from the forms processing logic app.""],
         [""Power BI"", ""component"", ""A business analytics tool that obtains the data from Azure Cosmos DB and provides insights and dashboards.""]
      ],
      ""relationships"": [
         [""Outlook email account"", ""part-of"", ""Logic app""],
         [""Logic app"", ""calls"", ""Function app""],
         [""PDF container"", ""contains"", ""PDF files""],
         [""Function app"", ""part-of"", ""PDF container (split pages)""],
         [""Function app"", ""calls"", ""AI Document Intelligence""],
         [""AI Document Intelligence"", ""called-from"", ""Function app""],
         [""Function app"", ""stores to"", ""third container in Data Lake Storage""],
         [""Logic app"", ""part-of"", ""Forms processing logic app""],
         [""Forms processing logic app"", ""sends to"", ""Azure Cosmos DB""],
         [""Power BI"", ""obtains from"", ""Azure Cosmos DB""]
      ]
   }",6.984060092014261
114,mistral:7b,../IMAGES NEW/Azure/14 Azure Call Center Analytics.txt,"
    A phone call between an agent and a customer is recorded and stored in Azure Blob Storage. Audio files are uploaded to an Azure Storage account via a supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API.

    Azure AI Speech is used to transcribe audio files in Batch mode asynchronously with speaker diarization enabled. The transcription results are persisted in Blob Storage.

    Azure AI Language is used to detect and redact personal data in the transcript.

    For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool. The Ingestion Client tool uses a no-code approach for call center transcription.

    Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments. The processed output is stored in Blob Storage and then analyzed and visualized by using other services. You can also store the output in a datastore for keeping track of metadata and for reporting. Use Azure OpenAI to process the stored transcription information.

    Power BI or a custom web application that's hosted by App Service is used to visualize the output. Both options provide near real-time insights. You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems. This process is fully automated, which saves the agents time and effort.

","{""entities"": [[""phone call"", ""component"", ""A phone call between an agent and a customer is recorded""], [""agent"", ""person"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""person"", ""A phone call between an agent and a customer is recorded""], [""Azure Blob Storage"", ""system"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""system"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""component"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""UI-based tool"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure Storage Explorer"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Storage SDK"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""API"", ""component"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""component"", ""transcribe audio files in Batch mode asynchronously""], [""transcription results"", ""component"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""component"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""component"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""component"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI output"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""component"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""App Service"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""system"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Azure OpenAI output 2"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""datastore"", ""component"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""customer relationship management (CRM)"", ""system"", ""You can store this output in a customer relationship management (CRM)""], [""Azure OpenAI output 3"", ""component"", ""You can store this output in a customer relationship management (CRM)""]], ""relationships"": [[""agent"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""customer"", ""calls"", ""phone call"", ""A phone call between an agent and a customer is recorded""], [""phone call"", ""part-of"", ""Azure Blob Storage"", ""is recorded and stored in Azure Blob Storage""], [""Audio files"", ""part-of"", ""Azure Blob Storage"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""calls"", ""Audio files"", ""Audio files are uploaded to an Azure Storage account via a supported method""], [""supported method"", ""contains"", ""UI-based tool"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Azure Storage Explorer"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""Storage SDK"", ""upported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""supported method"", ""contains"", ""API"", ""supported method, such as the UI-based tool, Azure Storage Explorer, or a Storage SDK or API""], [""Azure AI Speech"", ""calls"", ""Audio files"", ""transcribe audio files in Batch mode asynchronously""], [""Azure Blob Storage"", ""contains"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Speech"", ""calls"", ""transcription results"", ""The transcription results are persisted in Blob Storage.""], [""Azure AI Language"", ""calls"", ""transcription results"", ""Azure AI Language is used to detect and redact personal data in the transcript.""], [""AI services Ingestion Client tool"", ""calls"", ""transcription results"", ""For batch mode transcription and personal data detection and redaction, use the AI services Ingestion Client tool""], [""Azure OpenAI"", ""calls"", ""transcription results"", ""Azure OpenAI is used to process the transcript and extract entities, summarize the conversation, and analyze sentiments.""], [""Azure OpenAI"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Azure Blob Storage"", ""contains"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""other services"", ""calls"", ""Azure OpenAI output"", ""The processed output is stored in Blob Storage and then analyzed and visualized by using other services.""], [""Power BI"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""part-of"", ""App Service"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""custom web application"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""Power BI"", ""calls"", ""Azure OpenAI output"", ""Power BI or a custom web application that's hosted by App Service is used to visualize the output.""], [""agent"", ""calls"", ""customer relationship management (CRM)"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""], [""Azure OpenAI output 2"", ""part-of"", ""datastore"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output 3"", ""part-of"", ""customer relationship management (CRM)"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 2"", ""You can also store the output in a datastore for keeping track of metadata and for reporting""], [""Azure OpenAI output"", ""calls"", ""Azure OpenAI output 3"", ""You can store this output in a customer relationship management (CRM), so agents have contextual information about why the customer called and can quickly solve potential problems""]]}"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Microsoft Azure where audio files and transcription results are stored""],
         [""Azure Storage account"", ""component"", ""An account associated with the Azure Blob Storage system that is used for uploading and storing audio files and transcription results""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool provided by Microsoft Azure for managing storage accounts and files""],
         [""Storage SDK or API"", ""component"", ""Software development kits (SDK) or application programming interfaces (API) provided by Microsoft Azure for uploading audio files to the Azure Blob Storage system""],
         [""Azure AI Speech"", ""system"", ""An intelligent speech recognition service provided by Microsoft Azure that transcribes audio files asynchronously with speaker diarization enabled""],
         [""Batch mode"", ""description"", ""A method used by Azure AI Speech for processing multiple audio files at once""],
         [""Transcription results"", ""component"", ""The output generated by Azure AI Speech after transcribing the audio files""],
         [""Azure AI Language"", ""system"", ""An intelligent language understanding service provided by Microsoft Azure that detects and redacts personal data in the transcript""],
         [""Ingestion Client tool"", ""component"", ""A no-code approach for call center transcription using Microsoft Azure services""],
         [""AI services"", ""description"", ""Collective term for Azure AI Speech, Azure AI Language, and Ingestion Client tool""],
         [""Azure OpenAI"", ""system"", ""An intelligent processing service provided by Microsoft Azure that processes the transcribed conversations, extracts entities, summarizes the conversation, analyzes sentiments, and stores the output in Blob Storage""],
         [""Power BI"", ""component"", ""A business analytics service provided by Microsoft for visualizing data and creating interactive reports""],
         [""App Service"", ""component"", ""Microsoft Azure's platform as a service (PaaS) offering that allows developers to host web applications""],
         [""Custom web application"", ""component"", ""A web application specifically designed for the task of visualizing the processed output""],
         [""CRM"", ""system"", ""A customer relationship management system used for storing and managing customer interactions""],
         [""Power BI or a custom web application hosted by App Service"", ""description"", ""Two options for visualizing the output, either Power BI or a custom web application hosted on Microsoft Azure's App Service""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage account""],
         [""Azure Storage Explorer"", ""calls"", ""Azure Storage account""],
         [""Storage SDK or API"", ""calls"", ""Azure Storage account""],
         [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
         [""Ingestion Client tool"", ""part-of"", ""AI services""],
         [""Azure OpenAI"", ""called-from"", ""Azure Blob Storage""],
         [""Power BI or a custom web application hosted by App Service"", ""visualizes"", ""Output processed by Azure OpenAI""]
      ]
   }"," {
      ""entities"": [
         [""Azure Blob Storage"", ""system"", ""A storage service provided by Microsoft Azure where audio files and transcription results are stored""],
         [""Azure Storage account"", ""component"", ""An account associated with the Azure Blob Storage system that is used for uploading and storing audio files and transcription results""],
         [""Azure Storage Explorer"", ""component"", ""A UI-based tool provided by Microsoft Azure for managing storage accounts and files""],
         [""Storage SDK or API"", ""component"", ""Software development kits (SDK) or application programming interfaces (API) provided by Microsoft Azure for uploading audio files to the Azure Blob Storage system""],
         [""Azure AI Speech"", ""system"", ""An intelligent speech recognition service provided by Microsoft Azure that transcribes audio files asynchronously with speaker diarization enabled""],
         [""Batch mode"", ""description"", ""A method used by Azure AI Speech for processing multiple audio files at once""],
         [""Transcription results"", ""component"", ""The output generated by Azure AI Speech after transcribing the audio files""],
         [""Azure AI Language"", ""system"", ""An intelligent language understanding service provided by Microsoft Azure that detects and redacts personal data in the transcript""],
         [""Ingestion Client tool"", ""component"", ""A no-code approach for call center transcription using Microsoft Azure services""],
         [""AI services"", ""description"", ""Collective term for Azure AI Speech, Azure AI Language, and Ingestion Client tool""],
         [""Azure OpenAI"", ""system"", ""An intelligent processing service provided by Microsoft Azure that processes the transcribed conversations, extracts entities, summarizes the conversation, analyzes sentiments, and stores the output in Blob Storage""],
         [""Power BI"", ""component"", ""A business analytics service provided by Microsoft for visualizing data and creating interactive reports""],
         [""App Service"", ""component"", ""Microsoft Azure's platform as a service (PaaS) offering that allows developers to host web applications""],
         [""Custom web application"", ""component"", ""A web application specifically designed for the task of visualizing the processed output""],
         [""CRM"", ""system"", ""A customer relationship management system used for storing and managing customer interactions""],
         [""Power BI or a custom web application hosted by App Service"", ""description"", ""Two options for visualizing the output, either Power BI or a custom web application hosted on Microsoft Azure's App Service""]
      ],
      ""relationships"": [
         [""Azure Blob Storage"", ""contains"", ""Azure Storage account""],
         [""Azure Storage Explorer"", ""calls"", ""Azure Storage account""],
         [""Storage SDK or API"", ""calls"", ""Azure Storage account""],
         [""Azure AI Speech"", ""calls"", ""Azure Blob Storage""],
         [""Ingestion Client tool"", ""part-of"", ""AI services""],
         [""Azure OpenAI"", ""called-from"", ""Azure Blob Storage""],
         [""Power BI or a custom web application hosted by App Service"", ""visualizes"", ""Output processed by Azure OpenAI""]
      ]
   }",7.337084910017438
115,mistral:7b,../IMAGES NEW/Azure/08 Azure Business Processes.txt,"
The data flows through the solution as follows:

    The airline system communicates with a custom API hosted in Azure API Management.
    A custom API coordinator receives notifications and handles incoming messages from the airline system. It sends them to Power Apps, where flights are assigned to Microsoft Teams channels.
    When a user selects a flight to monitor, or when the system assigns the user to a flight, the system queues a Graph API call in an Azure Storage Account queue for further processing.
    Azure Functions runs the Graph API calls based on the incoming messages in the storage queue, sending notifications to Teams, and also streams all events to an Azure Event Hubs for further analytics.
    The airline's notification system is managed by a custom bot messaging service that employs Azure Bot Service.
    Custom bots send flight updates to users in Teams.
    An Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI.

","{""entities"": [[""airline system"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API"", ""component"", ""communicates with a custom API hosted in Azure API Management""], [""Azure API Management"", ""system"", ""communicates with a custom API hosted in Azure API Management""], [""custom API coordinator"", ""system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Microsoft Teams channels"", ""system"", ""where flights are assigned to Microsoft Teams channels""], [""Storage Account queue"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Graph API call"", ""system"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing.""], [""Azure Functions"", ""system"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Event Hubs"", ""system"", ""Azure Event Hubs for further analytics""], [""notification system"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Azure Bot Service"", ""system"", ""managed by a custom bot messaging service that employs Azure Bot Service""], [""Users"", ""person"", ""selects a flight to monitor, or when the system assigns the user to a flight""], [""Azure Data Lake storage"", ""system"", ""offers long-term retention and micro-batch processing of events from Event Hubs""], [""Power BI"", ""system"", ""generating insightful reports""]], ""relationships"": [[""Azure API Management"", ""contains"", ""custom API"", ""custom API hosted in Azure API Management""], [""airline system"", ""calls"", ""custom API"", ""communicates with a custom API""], [""custom API coordinator"", ""is-called-from"", ""airline system"", ""receives notifications and handles incoming messages from the airline system""], [""Power Apps"", ""calls"", ""Microsoft Teams channels"", ""Power Apps, where flights are assigned to Microsoft Teams channels""], [""airline system"", ""calls"", ""Graph API call"", "" the system queues a Graph API call in an Azure Storage Account queue for further processing""], [""Azure Functions"", ""calls"", ""Graph API call"", ""runs the Graph API calls based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Storage Account queue"", ""based on the incoming messages in the storage queue""], [""Azure Functions"", ""calls"", ""Microsoft Teams channels"", ""sending notifications to Teams""], [""Azure Functions"", ""calls"", ""Azure Event Hubs"", ""also streams all events to an Azure Event Hubs for further analytics""], [""notification system"", ""is-called-from"", ""custom bot messaging service"", ""managed by a custom bot messaging service""], [""custom bot messaging service"", ""calls"", ""Azure Bot Service"", ""custom bot messaging service that employs Azure Bot Service""], [""custom bot messaging service"", ""calls"", ""Microsoft Teams channels"", ""Custom bots send flight updates to users in Teams.""], [""Microsoft Teams channels"", ""calls"", ""Users"", ""Custom bots send flight updates to users in Teams.""], [""Azure Data Lake storage"", ""is-called-from"", ""Azure Event Hubs"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs""], [""Azure Data Lake storage"", ""calls"", ""Power BI"", ""Azure Data Lake storage offers long-term retention and micro-batch processing of events from Event Hubs, ultimately generating insightful reports with Power BI""]]}"," {
      ""entities"": [
         [""Airline System"", ""System"", ""Communicates with a custom API hosted in Azure API Management.""],
         [""Custom API Hosted in Azure API Management"", ""Component"", ""Receives notifications and handles incoming messages from the airline system.""],
         [""Custom API Coordinator"", ""Component"", ""Sends messages to Power Apps and queues Graph API calls for further processing.""],
         [""Power Apps"", ""System"", ""Assigns flights to Microsoft Teams channels.""],
         [""Azure Storage Account Queue"", ""Component"", ""Queues Graph API calls based on incoming messages.""],
         [""Azure Functions"", ""Component"", ""Runs the Graph API calls and sends notifications to Teams, streams events to Azure Event Hubs.""],
         [""Azure Event Hubs"", ""System"", ""Collects events for further analytics.""],
         [""Custom Bot Messaging Service"", ""Component"", ""Sends flight updates to users in Teams.""],
         [""Azure Bot Service"", ""Component"", ""Manages the custom bot messaging service.""],
         [""Microsoft Teams"", ""System"", ""Receives notifications from the system.""],
         [""Graph API"", ""Component"", ""Called by Azure Functions for processing flight data.""],
         [""Azure Data Lake Storage"", ""System"", ""Offers long-term retention and micro-batch processing of events from Event Hubs.""],
         [""Power BI"", ""System"", ""Generates insightful reports from processed data.""]
      ],
      ""relationships"": [
         [""Airline System"", ""calls"", ""Graph API""],
         [""Custom API Hosted in Azure API Management"", ""part-of"", ""Azure API Management""],
         [""Custom API Coordinator"", ""part-of"", ""Airline System""],
         [""Power Apps"", ""part-of"", ""Microsoft Teams""],
         [""Azure Storage Account Queue"", ""part-of"", ""Azure Storage Account""],
         [""Azure Functions"", ""part-of"", ""Azure Function App""],
         [""Custom Bot Messaging Service"", ""part-of"", ""Azure Bot Service""],
         [""Graph API"", ""called-from"", ""Azure Functions""],
         [""Event Hubs"", ""part-of"", ""Azure Stream Analytics""],
         [""Azure Data Lake Storage"", ""part-of"", ""Azure Data Lake""]
      ]
   }"," {
      ""entities"": [
         [""Airline System"", ""System"", ""Communicates with a custom API hosted in Azure API Management.""],
         [""Custom API Hosted in Azure API Management"", ""Component"", ""Receives notifications and handles incoming messages from the airline system.""],
         [""Custom API Coordinator"", ""Component"", ""Sends messages to Power Apps and queues Graph API calls for further processing.""],
         [""Power Apps"", ""System"", ""Assigns flights to Microsoft Teams channels.""],
         [""Azure Storage Account Queue"", ""Component"", ""Queues Graph API calls based on incoming messages.""],
         [""Azure Functions"", ""Component"", ""Runs the Graph API calls and sends notifications to Teams, streams events to Azure Event Hubs.""],
         [""Azure Event Hubs"", ""System"", ""Collects events for further analytics.""],
         [""Custom Bot Messaging Service"", ""Component"", ""Sends flight updates to users in Teams.""],
         [""Azure Bot Service"", ""Component"", ""Manages the custom bot messaging service.""],
         [""Microsoft Teams"", ""System"", ""Receives notifications from the system.""],
         [""Graph API"", ""Component"", ""Called by Azure Functions for processing flight data.""],
         [""Azure Data Lake Storage"", ""System"", ""Offers long-term retention and micro-batch processing of events from Event Hubs.""],
         [""Power BI"", ""System"", ""Generates insightful reports from processed data.""]
      ],
      ""relationships"": [
         [""Airline System"", ""calls"", ""Graph API""],
         [""Custom API Hosted in Azure API Management"", ""part-of"", ""Azure API Management""],
         [""Custom API Coordinator"", ""part-of"", ""Airline System""],
         [""Power Apps"", ""part-of"", ""Microsoft Teams""],
         [""Azure Storage Account Queue"", ""part-of"", ""Azure Storage Account""],
         [""Azure Functions"", ""part-of"", ""Azure Function App""],
         [""Custom Bot Messaging Service"", ""part-of"", ""Azure Bot Service""],
         [""Graph API"", ""called-from"", ""Azure Functions""],
         [""Event Hubs"", ""part-of"", ""Azure Stream Analytics""],
         [""Azure Data Lake Storage"", ""part-of"", ""Azure Data Lake""]
      ]
   }",5.8522201759624295
116,mistral:7b,../IMAGES NEW/Azure/09 Azure protect APIs original.txt,"
    To gain access to an application, an API client authenticates by providing credentials such as username and password. The IdP is Azure AD B2C in this solution, but you can use a different one.

    The authentication request goes via Azure Front Door to Azure AD B2C, which is configured with a custom domain for sign-in. Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user.

    The client triggers an event that accesses a backend API. This event could be a click of a button on a web application or on a mobile device, or a direct call to the endpoint of the backend API.

    The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management. API Management intercepts the request and validates the bearer token against Azure AD B2C by using its validate-jwt policy. If the token isn't valid, API Management rejects the request by responding with a 401 code.

    If the token is valid, API Management forwards the request to the appropriate backend API.

    The diagram shows backend APIs running in three environments:
        App Service Environment
        Function Apps
        Azure Kubernetes Services (AKS)

    APIs running in on-premises and hybrid cloud environments can also be integrated with API Management if network connectivity is established between the APIs and API Management.

","{""entities"": [[""API client"", ""system"", ""wants to gain access to an application""], [""application"", ""system"", ""API client wants to gain access to an application""], [""Azure AD B2C"", ""system"", ""The IdP is Azure AD B2C""], [""Azure Front Door"", ""system"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""backend API"", ""system"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""system"", ""The request goes through Azure Front Door, whose back end is mapped to the public endpoint of API Management.""], [""backend"", ""component"", ""Azure Front Door, whose back end""], [""API Management"", ""system"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C ""], [""public endpoint"", ""system"", ""whose back end is mapped to the public endpoint of API Management""], [""User"", ""person"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""]], ""relationships"": [[""API client"", ""calls"", ""application"", """"], [""Azure Front Door"", ""calls"", ""Azure AD B2C"", ""authentication request goes via Azure Front Door to Azure AD B2C""], [""Azure AD B2C"", ""is-called-from"", ""User"", ""Azure AD B2C authenticates the user and returns a JSON Web Token (JWT) bearer token back to the user""], [""API client"", ""calls"", ""backend API"", ""The client triggers an event that accesses a backend API""], [""Azure Front Door"", ""contains"", ""backend"", ""Azure Front Door, whose back end""], [""public endpoint"", ""part-of"", ""API Management"", ""Azure Front Door, whose back end""], [""backend"", ""calls"", ""public endpoint"", ""whose back end is mapped to the public endpoint of API Management""], [""API Management"", ""calls"", ""Azure AD B2C"", ""API Management intercepts the request and validates the bearer token against Azure AD B2C""]]}"," {
      ""entities"": [
         [""Azure Front Door"", ""system"", ""A reverse proxy service that routes client requests to specified services.""],
         [""API Management"", ""system"", ""A management service for APIs, providing features like authentication and rate limiting.""],
         [""Azure AD B2C"", ""system"", ""An identity provider for consumer applications.""],
         [""custom domain"", ""component"", ""A specific domain configured in Azure AD B2C for sign-in.""],
         [""JSON Web Token (JWT)"", ""component"", ""A compact, self-contained data structure used to pass information between parties as a claim.""],
         [""API client"", ""person"", ""The software or system that initiates the request to access an application through an API.""],
         [""backend API"", ""component"", ""The main application service with specific functionality, which can be accessed via an API.""]
      ],
      ""relationships"": [
         [""API client"", ""calls"", [""Azure Front Door""]],
         [""Azure Front Door"", ""part-of"", [""API Management""]],
         [""API Management"", ""part-of"", [""custom domain""]],
         [""Azure AD B2C"", ""called-from"", [""API Management""]],
         [""API Management"", ""calls"", [""backend API""]]
      ]
   }"," {
      ""entities"": [
         [""Azure Front Door"", ""system"", ""A reverse proxy service that routes client requests to specified services.""],
         [""API Management"", ""system"", ""A management service for APIs, providing features like authentication and rate limiting.""],
         [""Azure AD B2C"", ""system"", ""An identity provider for consumer applications.""],
         [""custom domain"", ""component"", ""A specific domain configured in Azure AD B2C for sign-in.""],
         [""JSON Web Token (JWT)"", ""component"", ""A compact, self-contained data structure used to pass information between parties as a claim.""],
         [""API client"", ""person"", ""The software or system that initiates the request to access an application through an API.""],
         [""backend API"", ""component"", ""The main application service with specific functionality, which can be accessed via an API.""]
      ],
      ""relationships"": [
         [""API client"", ""calls"", [""Azure Front Door""]],
         [""Azure Front Door"", ""part-of"", [""API Management""]],
         [""API Management"", ""part-of"", [""custom domain""]],
         [""Azure AD B2C"", ""called-from"", [""API Management""]],
         [""API Management"", ""calls"", [""backend API""]]
      ]
   }",3.78289043600671
117,mistral:7b,../IMAGES NEW/Azure/10 Azure LoB original.txt,"
The data flows through the solution as follows:

    Supplier data stored in CDS is moved to SQL via Data Factory.
    Purchase order (PO) data stored in ERP system is sent to Azure SQL database.
    Azure Functions uses API to surface PO data monthly and creates a task for users to review.
    Power Apps retrieves data from Azure SQL Database through API.
    User reviews and updates POs in Power Apps and sends this data to suppliers through CSV export.
    Power BI reports trends in supplier status.

","{""entities"": [[""Supplier data"", ""component"", ""Supplier data stored in CDS""], [""CDS"", ""system"", "" Supplier data stored in CDS""], [""Azure SQL database"", ""system"", ""moved to SQL via Data Factory""], [""Data Factory"", ""system"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""system"", ""moved to SQL via Data Factory""], [""ERP system"", ""system"", ""Purchase order (PO) data stored in ERP system""], [""Azure Functions"", ""system"", ""uses API to surface PO data monthly and creates a task for users to review""], [""Power Apps"", ""system"", ""retrieves data from Azure SQL Database through API""], [""CSV export"", ""system"", ""ends this data to suppliers through CSV export""], [""User"", ""person"", "" reviews and updates POs in Power Apps""], [""suppliers"", ""person"", "" reviews and updates POs in Power Apps""]], ""relationships"": [[""Supplier data"", ""part-of"", ""CDS"", ""Supplier data stored in CDS""], [""Supplier data"", ""is-called-from"", ""Data Factory"", ""moved to SQL via Data Factory""], [""Data Factory"", ""calls"", ""Azure SQL database"", ""moved to SQL via Data Factory""], [""Purchase order (PO)"", ""part-of"", ""ERP system"", ""Purchase order (PO) data stored in ERP system""], [""Purchase order (PO)"", ""calls"", ""Azure SQL database"", ""Purchase order (PO) data stored in ERP system is sent to Azure SQL database""], [""Azure Functions"", ""calls"", ""Purchase order (PO)"", ""uses API to surface PO data monthly""], [""Azure Functions"", ""calls"", ""User"", ""creates a task for users to review""], [""Power Apps"", ""calls"", ""Azure SQL database"", ""retrieves data from Azure SQL Database through API""], [""User"", ""calls"", ""Power Apps"", ""Reviews and updates POs in Power Apps""], [""User"", ""calls"", ""CSV export"", ""sends this data to suppliers through CSV export""], [""CSV export"", ""calls"", ""suppliers"", ""sends this data to suppliers through CSV export""]]}"," {
      ""entities"": [
         [""CDS"", ""system"", ""Cloud Data Storage (CDS)""],
         [""SQL"", ""system"", ""SQL Database""],
         [""Data Factory"", ""component"", ""Azure Data Factory""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning System""],
         [""Azure SQL database"", ""component"", ""Database in Azure""],
         [""Azure Functions"", ""component"", ""Azure Function used for automation""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Microsoft Power Apps""],
         [""Power BI"", ""component"", ""Microsoft Power Business Intelligence""]
      ],
      ""relationships"": [
         [""CDS"", ""contains"", ""Data Factory""],
         [""SQL"", ""part-of"", ""Azure SQL database""],
         [""ERP system"", ""calls"", ""Data Factory""],
         [""Azure Functions"", ""calls"", ""API""],
         [""Power Apps"", ""calls"", ""Azure SQL database""],
         [""Power Apps"", ""called-from"", ""User""],
         [""Power Apps"", ""part-of"", ""Supplier data update process""],
         [""Power BI"", ""calls"", ""Power Apps""],
         [""Supplier data update process"", ""contains"", ""Power Apps""]
      ]
   }"," {
      ""entities"": [
         [""CDS"", ""system"", ""Cloud Data Storage (CDS)""],
         [""SQL"", ""system"", ""SQL Database""],
         [""Data Factory"", ""component"", ""Azure Data Factory""],
         [""ERP system"", ""component"", ""Enterprise Resource Planning System""],
         [""Azure SQL database"", ""component"", ""Database in Azure""],
         [""Azure Functions"", ""component"", ""Azure Function used for automation""],
         [""API"", ""component"", ""Application Programming Interface""],
         [""Power Apps"", ""component"", ""Microsoft Power Apps""],
         [""Power BI"", ""component"", ""Microsoft Power Business Intelligence""]
      ],
      ""relationships"": [
         [""CDS"", ""contains"", ""Data Factory""],
         [""SQL"", ""part-of"", ""Azure SQL database""],
         [""ERP system"", ""calls"", ""Data Factory""],
         [""Azure Functions"", ""calls"", ""API""],
         [""Power Apps"", ""calls"", ""Azure SQL database""],
         [""Power Apps"", ""called-from"", ""User""],
         [""Power Apps"", ""part-of"", ""Supplier data update process""],
         [""Power BI"", ""calls"", ""Power Apps""],
         [""Supplier data update process"", ""contains"", ""Power Apps""]
      ]
   }",3.550124044995755
118,mistral:7b,../IMAGES NEW/Azure/11 Mainframe original.txt,"
    A Connect agent component captures change logs by using mainframe or midrange native utilities and caches the logs in temporary storage.
    For mainframe systems, a publisher component on the mainframe manages data migration.
    For midrange systems, in place of the publisher, a listener component manages data migration. It's located on either a Windows or Linux machine.
    The publisher or listener moves the data from on-premises to Azure via an enhanced-security connection. The publisher or listener handles the commit and rollback of transactions for each unit of work, maintaining the integrity of data.
    The Connect Replicator Engine captures the data from the publisher or listener and applies it to the target. It distributes data for parallel processing.
    The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs.
    The changed data is consumed by Azure Databricks and applied to Azure data platform services.
    The Connect Controller Daemon authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.

","{""entities"": [[""Connect agent"", ""component"", ""captures change logs""], [""temporary storage"", ""component"", ""caches the logs in temporary storage""], [""mainframe native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""midrange native utilities"", ""system"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""system"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""system"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""component"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""listener"", ""component"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""Connect Replicator Engine"", ""system"", ""captures the data from the publisher or listener and applies it to the target.""], [""on-premises"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Azure"", ""system"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""system"", ""The Connect Replicator Engine captures the data from the publisher or listener""], [""target"", ""system"", ""applies it to the target""], [""database"", ""system"", ""target is a database""], [""ODBC"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Event Hubs"", ""component"", ""The target is a database that receives the changes via ODBC or ingests the changes via Azure Event Hubs""], [""Azure Databricks"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Azure data platform services"", ""component"", ""the changed data is consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""component"", ""authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]], ""relationships"": [[""Connect agent"", ""calls"", ""mainframe native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""midrange native utilities"", ""captures change logs by using mainframe or midrange native utilities""], [""Connect agent"", ""calls"", ""temporary storage"", ""captures change logs by using mainframe or midrange native utilities""], [""mainframe systems"", ""contains"", ""mainframe native utilities"", ""F""], [""midrange systems"", ""contains"", ""midrange native utilities"", """"], [""mainframe systems"", ""contains"", ""publisher"", ""For mainframe systems, a publisher component on the mainframe manages data migration.""], [""midrange systems"", ""contains"", ""listener"", ""For midrange systems, in place of the publisher, a listener component manages data migration.""], [""publisher"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""publisher"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""on-premises"", ""The publisher or listener moves the data from on-premises to Azure""], [""listener"", ""calls"", ""Azure"", ""The publisher or listener moves the data from on-premises to Azure""], [""Connect Replicator Engine"", ""calls"", ""publisher"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""listener"", ""captures the data from the publisher or listener and applies it to the target.""], [""Connect Replicator Engine"", ""calls"", ""target"", ""captures the data from the publisher or listener and applies it to the target.""], [""target"", ""contains"", ""database"", ""target is a database""], [""database"", ""is-called-from"", ""ODBC"", ""database that receives the changes via ODBC""], [""database"", ""is-called-from"", ""Azure Event Hubs"", ""ingests the changes via Azure Event Hubs""], [""database"", ""is-called-from"", ""Azure Databricks"", ""the changed data is consumed by Azure Databricks""], [""Azure Databricks"", ""calls"", ""Azure data platform services"", ""consumed by Azure Databricks and applied to Azure data platform services.""], [""Connect Controller Daemon"", ""is-called-from"", ""publisher"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""is-called-from"", ""listener"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""], [""Connect Controller Daemon"", ""calls"", ""Connect Replicator Engine"", ""establishes the socket connection between the publisher or listener and the Replicator Engine..""]]}"," {
      ""entities"": [
         [""Connect Agent Component"", ""component"", ""Captures change logs using mainframe or midrange native utilities and caches them in temporary storage.""],
         [""Publisher Component (Mainframe)"", ""component"", ""Manages data migration for mainframe systems. Handles commit and rollback of transactions, maintains the integrity of data, and moves data from on-premises to Azure via an enhanced-security connection.""],
         [""Listener Component (Midrange)"", ""component"", ""Manages data migration for midrange systems. Located on either a Windows or Linux machine, it handles commit and rollback of transactions, maintains the integrity of data, and moves data from on-premises to Azure via an enhanced-security connection.""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target. Distributes data for parallel processing.""],
         [""Target Database"", ""system"", ""Receives the changes via ODBC or ingests them via Azure Event Hubs.""],
         [""Azure Databricks"", ""component"", ""Consumes the changed data and applies it to Azure data platform services.""],
         [""Connect Controller Daemon"", ""component"", ""Authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]
      ],
      ""relationships"": [
         [""Publisher Component (Mainframe)"", ""part-of"", ""Azure""],
         [""Listener Component (Midrange)"", ""part-of"", ""Azure""],
         [""Connect Agent Component"", ""calls"", ""Publisher Component (Mainframe)""],
         [""Connect Agent Component"", ""calls"", ""Listener Component (Midrange)""],
         [""Publisher Component (Mainframe)"", ""contains"", ""Connect Replicator Engine""],
         [""Listener Component (Midrange)"", ""contains"", ""Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""part-of"", ""Target Database""]
      ]
   }"," {
      ""entities"": [
         [""Connect Agent Component"", ""component"", ""Captures change logs using mainframe or midrange native utilities and caches them in temporary storage.""],
         [""Publisher Component (Mainframe)"", ""component"", ""Manages data migration for mainframe systems. Handles commit and rollback of transactions, maintains the integrity of data, and moves data from on-premises to Azure via an enhanced-security connection.""],
         [""Listener Component (Midrange)"", ""component"", ""Manages data migration for midrange systems. Located on either a Windows or Linux machine, it handles commit and rollback of transactions, maintains the integrity of data, and moves data from on-premises to Azure via an enhanced-security connection.""],
         [""Connect Replicator Engine"", ""component"", ""Captures the data from the publisher or listener and applies it to the target. Distributes data for parallel processing.""],
         [""Target Database"", ""system"", ""Receives the changes via ODBC or ingests them via Azure Event Hubs.""],
         [""Azure Databricks"", ""component"", ""Consumes the changed data and applies it to Azure data platform services.""],
         [""Connect Controller Daemon"", ""component"", ""Authenticates the request and establishes the socket connection between the publisher or listener and the Replicator Engine.""]
      ],
      ""relationships"": [
         [""Publisher Component (Mainframe)"", ""part-of"", ""Azure""],
         [""Listener Component (Midrange)"", ""part-of"", ""Azure""],
         [""Connect Agent Component"", ""calls"", ""Publisher Component (Mainframe)""],
         [""Connect Agent Component"", ""calls"", ""Listener Component (Midrange)""],
         [""Publisher Component (Mainframe)"", ""contains"", ""Connect Replicator Engine""],
         [""Listener Component (Midrange)"", ""contains"", ""Connect Replicator Engine""],
         [""Connect Replicator Engine"", ""part-of"", ""Target Database""]
      ]
   }",4.9416432880098
119,mistral:7b,../IMAGES NEW/Azure/06 Azure logic-apps-data-integration.txt,"
    API Management accepts API calls in the form of HTTP requests.

    API Management securely routes the HTTP requests to Logic Apps.

    Each HTTP request triggers a run in Logic Apps:
        Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault.
        Logic Apps uses Transport Layer Security (TLS) to send the database credentials and a database statement to the on-premises data gateway.

    The on-premises data gateway connects to a SQL Server database to run the statement.

    SQL Server stores the data and makes it available to apps that users access.

    Azure Monitor collects information on Logic Apps events and performance.

","{""entities"": [[""API Management"", ""system"", ""accepts API calls in the form of HTTP requests""], [""Logic Apps"", ""system"", ""uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""on-premises data gateway"", ""system"", """"], [""SQL Server"", ""system"", ""stores the data and makes it available to apps that users access""], [""Azure Monitor"", ""system"", ""collects information on Logic Apps events and performance""], [""Azure Key Vault"", ""system"", ""database credentials""]], ""relationships"": [[""API Management"", ""calls"", ""Logic Apps"", ""routes the HTTP requests to Logic Apps""], [""Logic Apps"", ""calls"", ""Azure Key Vault"", ""Logic Apps uses secured template parameters to retrieve database credentials from Azure Key Vault""], [""Logic Apps"", ""calls"", ""on-premises data gateway"", ""send the database credentials and a database statement to the on-premises data gateway""], [""on-premises data gateway"", ""calls"", ""SQL Server"", ""connects to a SQL Server database""], [""Azure Monitor"", ""calls"", ""Logic Apps"", ""collects information on Logic Apps events and performance""]]}"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests""],
         [""Logic Apps"", ""component"", ""Securely routes the HTTP requests and triggers a run""],
         [""Azure Key Vault"", ""component"", ""Stores database credentials""],
         [""Transport Layer Security (TLS)"", ""component"", ""Used to send the database credentials and a database statement""],
         [""on-premises data gateway"", ""component"", ""Connects to SQL Server database""],
         [""SQL Server"", ""system"", ""Stores the data and makes it available to apps""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance""]
      ],
      ""relationships"": [
         [""API Management"", ""part-of"", null],
         [""Logic Apps"", ""called-from"", ""API Management""],
         [""Logic Apps"", ""part-of"", ""Azure Monitor""],
         [""Logic Apps"", ""contains"", ""Azure Key Vault""],
         [""Logic Apps"", ""calls"", ""Transport Layer Security (TLS)""],
         [""Transport Layer Security (TLS)"", ""called-from"", ""Logic Apps""],
         [""on-premises data gateway"", ""part-of"", null],
         [""on-premises data gateway"", ""connected-to"", ""SQL Server""],
         [""SQL Server"", ""contains"", null]
      ]
   }"," {
      ""entities"": [
         [""API Management"", ""system"", ""Accepts API calls in the form of HTTP requests""],
         [""Logic Apps"", ""component"", ""Securely routes the HTTP requests and triggers a run""],
         [""Azure Key Vault"", ""component"", ""Stores database credentials""],
         [""Transport Layer Security (TLS)"", ""component"", ""Used to send the database credentials and a database statement""],
         [""on-premises data gateway"", ""component"", ""Connects to SQL Server database""],
         [""SQL Server"", ""system"", ""Stores the data and makes it available to apps""],
         [""Azure Monitor"", ""component"", ""Collects information on Logic Apps events and performance""]
      ],
      ""relationships"": [
         [""API Management"", ""part-of"", null],
         [""Logic Apps"", ""called-from"", ""API Management""],
         [""Logic Apps"", ""part-of"", ""Azure Monitor""],
         [""Logic Apps"", ""contains"", ""Azure Key Vault""],
         [""Logic Apps"", ""calls"", ""Transport Layer Security (TLS)""],
         [""Transport Layer Security (TLS)"", ""called-from"", ""Logic Apps""],
         [""on-premises data gateway"", ""part-of"", null],
         [""on-premises data gateway"", ""connected-to"", ""SQL Server""],
         [""SQL Server"", ""contains"", null]
      ]
   }",3.9201834769919515
